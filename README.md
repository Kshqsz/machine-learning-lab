# 机器学习实验 (Machine Learning Lab)

个人机器学习算法学习与实践项目

> 📖 **学习资料**：本项目基于李航老师的《机器学习方法（第2版）》进行学习，通过代码实现加深对算法原理的理解。

## 📚 项目简介

这是我的机器学习学习笔记和实验代码仓库。跟随《机器学习方法（第2版）》的章节顺序，从零开始实现各种经典的机器学习算法，将理论与实践相结合。

## 🎯 学习目标

- 理解机器学习算法的数学原理
- 从底层实现常见的机器学习算法
- 掌握 NumPy、Pandas、Matplotlib 等数据科学工具
- 培养算法调试和优化能力

## 📂 项目结构

```
machine-learning-lab/
├── linear_regression/          # 线性回归
│   └── gradient_descent.py    # 梯度下降法实现
├── perceptron/                 # 感知机
│   ├── perceptron_primal.py   # 感知机原始形式
│   └── perceptron_dual.py     # 感知机对偶形式
├── knn/                        # k近邻法
│   └── knn_basic.py           # k-d树实现
├── naive_bayes/                # 朴素贝叶斯
│   ├── naive_bayes_mle.py     # 极大似然估计
│   └── naive_bayes_est.py     # 贝叶斯估计(拉普拉斯平滑)
├── decision_tree/              # 决策树
│   ├── decision_tree_classifier.py  # 分类树(基尼指数)
│   └── decision_tree_regressor.py   # 回归树(MSE)
├── logistic_regression/        # 逻辑斯谛回归
│   ├── binomial_logistic_regression.py   # 二项逻辑斯谛回归
│   └── multinomial_logisitic_regression.py  # 多项逻辑斯谛回归
├── max_entropy/                # 最大熵模型
│   └── max_entropy_nlp_demo.py  # 中文词性标注Demo
├── svm/                        # 支持向量机
│   ├── svm_dual_linear.py     # 线性可分SVM-对偶形式
│   ├── svm_linear_softmargin.py  # 线性SVM-软间隔
│   ├── svm_sgd_linear.py      # 线性SVM-随机梯度下降
│   └── svm_kernel.py          # 非线性SVM-核函数方法
├── adaboost/                   # AdaBoost提升方法
│   ├── adaboost.py            # AdaBoost算法实现
│   └── forward_stagewise.py   # 前向分步算法实现
├── pytorch/                    # PyTorch深度学习
│   └── tensor_basics.py       # 张量基础操作
├── venv/                       # Python虚拟环境
├── .gitignore                  # Git忽略文件
└── README.md                   # 项目说明
```

## 🔬 已实现的算法

### 1. 线性回归 (Linear Regression)

#### 梯度下降法 (Gradient Descent)
- **文件**: `linear_regression/gradient_descent.py`
- **功能**: 使用梯度下降算法从头实现线性回归
- **特点**:
  - 生成带高斯噪声的训练数据
  - 实现完整的梯度下降优化过程
  - 可视化拟合结果和损失函数变化
  - 对比学习参数与真实参数

**运行示例**:
```bash
python linear_regression/gradient_descent.py
```

**效果展示**:
- 训练数据: 10个样本，符合 y = 0.5x + 1.5 + 噪声
- 学习率: 0.01
- 迭代次数: 1000次
- 输出: 拟合直线对比图 + 损失曲线图

---

### 2. 感知机 (Perceptron)

感知机是二分类的线性分类模型，是神经网络和支持向量机的基础。

#### 感知机原始形式 (Primal Form)
- **文件**: `perceptron/perceptron_primal.py`
- **模型**: $f(x) = \text{sign}(w \cdot x + b)$
- **算法**: 随机梯度下降
- **特点**:
  - 直接更新权重向量 $w$ 和偏置 $b$
  - 详细输出每次迭代的更新过程
  - 可视化分类结果和分离超平面
  - 适合特征维度较低的情况

**运行示例**:
```bash
python perceptron/perceptron_primal.py
```

#### 感知机对偶形式 (Dual Form)
- **文件**: `perceptron/perceptron_dual.py`
- **模型**: $f(x) = \text{sign}(\sum_{i=1}^{N} \alpha_i y_i x_i \cdot x + b)$
- **算法**: 基于样本更新次数的对偶表示
- **特点**:
  - 通过 $\alpha$ 向量记录每个样本的更新次数
  - 预先计算 Gram 矩阵 $G_{ij} = x_i \cdot x_j$ 提高效率
  - 可以恢复原始形式的参数
  - 适合样本数量较少的情况

**运行示例**:
```bash
python perceptron/perceptron_dual.py
```

**训练数据**:
- 正样本: x₁=(3,3)ᵀ, x₂=(4,3)ᵀ
- 负样本: x₃=(1,1)ᵀ
- 学习率: η = 1

---

### 3. k近邻法 (k-Nearest Neighbors)

k近邻法是一种基本的分类与回归方法，通过找到与待分类点最近的k个训练样本来进行预测。

#### k-d树实现 (k-d Tree)
- **文件**: `knn/knn_basic.py`
- **数据结构**: k-d树（k-dimensional tree）
- **功能**: 构造平衡k-d树，高效搜索最近邻
- **特点**:
  - 实现平衡k-d树的构造算法
  - 支持最近邻搜索和k近邻搜索
  - 详细输出树的结构和搜索过程
  - 可视化训练数据、查询点和搜索路径
  - 支持自定义查询点和k值

**运行示例**:
```bash
python knn/knn_basic.py
```

**使用方法**:
```python
# 使用默认查询点 (3, 4.5)
main()

# 自定义查询点
main(query_point=[7, 5])

# 搜索k个最近邻
main(query_point=[5, 5], k=3)
```

**训练数据**:
- 6个样本点: (2,3), (5,4), (9,6), (4,7), (8,1), (7,2)
- 默认查询点: (3, 4.5)
- 构造平衡k-d树，深度优先搜索最近邻

---

### 4. 朴素贝叶斯 (Naive Bayes)

朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法，简单高效且适用于多分类问题。

#### 极大似然估计 (MLE)
- **文件**: `naive_bayes/naive_bayes_mle.py`
- **方法**: Maximum Likelihood Estimation
- **特点**:
  - 使用极大似然估计计算先验概率和条件概率
  - 详细输出所有概率计算过程
  - 适用于训练数据充足的情况

**先验概率**:
$$P(Y=c_k) = \frac{N_{c_k}}{N}$$

**条件概率**:
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl}}{N_{c_k}}$$

**运行示例**:
```bash
python naive_bayes/naive_bayes_mle.py
```

#### 贝叶斯估计 / 拉普拉斯平滑 (EST)
- **文件**: `naive_bayes/naive_bayes_est.py`
- **方法**: Bayesian Estimation with Laplace Smoothing
- **特点**:
  - 使用贝叶斯估计（加一平滑）避免概率为0
  - $\lambda=1$ 时为拉普拉斯平滑
  - 解决训练数据不足导致的概率估计问题

**先验概率**:
$$P(Y=c_k) = \frac{N_{c_k} + \lambda}{N + K \cdot \lambda}$$

**条件概率**:
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl} + \lambda}{N_{c_k} + S_j \cdot \lambda}$$

**运行示例**:
```bash
python naive_bayes/naive_bayes_est.py
```

**训练数据**:
- 特征：X^(1) ∈ {1, 2, 3}, X^(2) ∈ {S, M, L}
- 类别：Y ∈ {1, -1}
- 15个训练样本
- 测试样本：(2, S)
- **预测结果**：两种方法都预测为 Y = -1

---

### 5. 决策树 (Decision Tree)

决策树是一种基本的分类与回归方法，通过树形结构进行决策。

#### 分类树 (Classification Tree)
- **文件**: `decision_tree/decision_tree_classifier.py`
- **分裂准则**: 基尼指数 (Gini Index)
- **特点**:
  - 使用基尼指数选择最优特征和切分点
  - 递归构建决策树
  - 支持类别型特征（自动编码）
  - 输出可读的决策规则
  - 适用于分类问题

**基尼指数**:
$$\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2$$

**分裂后的基尼指数**:
$$\text{Gini}(D, A) = \frac{|D_1|}{|D|}\text{Gini}(D_1) + \frac{|D_2|}{|D|}\text{Gini}(D_2)$$

**运行示例**:
```bash
python decision_tree/decision_tree_classifier.py
```

**训练数据** - 贷款审批数据集:
- 14个样本，4个特征（年龄、有工作、有房子、信贷情况）
- 2个类别（同意贷款、拒绝贷款）
- 训练集准确率：100%
- **决策规则示例**: 
  - 有房子 → 同意贷款
  - 无房子且信贷好 → 同意贷款
  - 无房子且信贷一般 → 拒绝贷款

#### 回归树 (Regression Tree)
- **文件**: `decision_tree/decision_tree_regressor.py`
- **分裂准则**: 均方误差 (MSE - Mean Squared Error)
- **特点**:
  - 使用MSE最小化选择分裂点
  - 叶节点预测值为区域内样本均值
  - 可视化拟合曲线
  - 适用于回归问题

**均方误差**:

$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \bar{y})^2$$

**分裂后的MSE**:

$$\text{MSE}_\text{split} = \frac{n_\text{left}}{n}\text{MSE}_\text{left} + \frac{n_\text{right}}{n}\text{MSE}_\text{right}$$

**运行示例**:
```bash
python decision_tree/decision_tree_regressor.py
```

**训练数据**:
- 10个样本点：(1, 4.50), (2, 4.75), ..., (10, 9.00)
- 训练集 MSE: 0.0525
- 训练集 R²: 0.9810

---

### 6. 逻辑斯谛回归 (Logistic Regression)

逻辑斯谛回归是一种广义线性模型，用于解决分类问题，特别是二分类问题。

#### 二项逻辑斯谛回归 (Binomial Logistic Regression)
- **文件**: `logistic_regression/binomial_logistic_regression.py`
- **方法**: 梯度下降法
- **特点**:
  - 使用 Sigmoid 函数将线性组合映射到 [0,1] 区间
  - 基于极大似然估计的损失函数
  - 梯度下降法优化参数
  - 可视化决策边界和训练过程
  - 适用于二分类问题

**Sigmoid 函数**:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**模型**:

$$P(Y=1|x) = \frac{1}{1 + \exp(-(w \cdot x + b))}$$

**损失函数（负对数似然）**:

$$L(w,b) = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$$

**梯度**:

$$\frac{\partial L}{\partial w} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)x_i$$

$$\frac{\partial L}{\partial b} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)$$

**运行示例**:
```bash
python logistic_regression/binomial_logistic_regression.py
```

**训练数据** - 学生考试通过预测:
- 20个样本：学习时长（0.5-5.5小时）与考试结果（通过/未通过）
- 通过人数：10人，未通过人数：10人
- 训练集准确率：80.00%
- **决策边界**: 学习时长约 2.7 小时
- **预测示例**:
  - 学习 1.0 小时 → 通过概率 7% → 未通过
  - 学习 3.0 小时 → 通过概率 61% → 通过
  - 学习 5.0 小时 → 通过概率 97% → 通过

#### 多项逻辑斯谛回归 (Multinomial Logistic Regression)
- **文件**: `logistic_regression/multinomial_logisitic_regression.py`
- **方法**: BFGS拟牛顿法
- **特点**:
  - 使用 Softmax 函数处理多分类问题
  - 基于极大似然估计
  - BFGS优化算法，收敛速度快
  - 可视化多分类决策边界
  - 适用于多分类问题（K ≥ 2）

**Softmax 函数**:

$$P(Y=k|x) = \frac{\exp(w_k \cdot x + b_k)}{\sum_{j=1}^{K}\exp(w_j \cdot x + b_j)}$$

**模型**:

对于 K 个类别，需要学习 K 组参数 $(w_1, b_1), ..., (w_K, b_K)$

**损失函数（交叉熵）**:

$$J(W,b) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K} \mathbb{1}(y_i=k) \log P(Y=k|x_i)$$

其中 $\mathbb{1}(\cdot)$ 是指示函数。

**梯度**:

$$\frac{\partial J}{\partial w_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))x_i$$

$$\frac{\partial J}{\partial b_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))$$

**运行示例**:
```bash
python logistic_regression/multinomial_logisitic_regression.py
```

**训练数据** - 鸢尾花分类:
- 48个样本：花瓣长度、花瓣宽度 → 3种鸢尾花（Setosa、Versicolor、Virginica）
- 类别分布：Setosa 16个，Versicolor 16个，Virginica 16个
- 训练集准确率：77.08%
- **特点**: 使用BFGS优化，自动计算梯度，收敛快且稳定
- **决策边界**: 可视化展示三个类别的非线性分离超平面

---

### 7. 最大熵模型 (Maximum Entropy Model)

最大熵模型是一种基于最大熵原理的分类模型，属于对数线性模型。在满足已知约束条件的前提下，选择熵最大的模型。

#### 最大熵NLP演示 - 中文词性标注 (POS Tagging)
- **文件**: `max_entropy/max_entropy_nlp_demo.py`
- **方法**: 梯度下降法
- **应用场景**: 中文词性标注（Part-of-Speech Tagging）
- **特点**:
  - 丰富的特征工程（7种特征类型）
  - 手工标注的中文训练数据
  - 支持8种常见词性标签
  - 梯度下降优化，过程可视化
  - 完整的训练、测试和预测流程

**最大熵模型**:

条件概率分布：

$$P(y|x) = \frac{1}{Z(x)}\exp\left(\sum_{i=1}^{n}w_i f_i(x,y)\right)$$

归一化因子：

$$Z(x) = \sum_{y}\exp\left(\sum_{i=1}^{n}w_i f_i(x,y)\right)$$

**与多项逻辑斯谛回归的关系**:

最大熵模型在形式上等价于多项逻辑斯谛回归：
- 都使用 Softmax 进行归一化
- 都是对数线性模型
- 区别在于特征函数的构造方式

**特征工程** - 7种特征类型:

1. **当前词特征**: `word=我`, `word=喜欢`
2. **前一个词**: `prev_word=我`, `prev_word=喜欢`
3. **后一个词**: `next_word=喜欢`, `next_word=中国`
4. **词长度**: `word_len=1`, `word_len=2`
5. **包含数字**: `has_digit=True`
6. **前缀**: `prefix_1=学`, `prefix_2=学习`
7. **后缀**: `suffix_1=习`, `suffix_2=学习`
8. **偏置**: `bias=1`（所有样本）

**词性标签** (8种):

| 标签 | 词性 | 示例 |
|:---:|------|------|
| n | 名词 (Noun) | 中国、音乐、书 |
| v | 动词 (Verb) | 爱、喜欢、学习 |
| a | 形容词 (Adjective) | 好、冷、干净 |
| d | 副词 (Adverb) | 很、非常、都 |
| p | 介词 (Preposition) | 在、从、对 |
| m | 数词 (Numeral) | 一、五、十 |
| q | 量词 (Quantifier) | 本、个、条 |
| r | 代词 (Pronoun) | 我、他、这 |

**损失函数**:

负对数似然 + L2正则化：

$$L(w) = -\sum_{(x,y)}\log P(y|x) + \lambda \|w\|^2$$

**梯度**:

$$\frac{\partial L}{\partial w_i} = \sum_{(x,y)}[P(y|x)f_i(x,y) - f_i(x,y_{\text{true}})] + 2\lambda w_i$$

**优化算法**:

使用梯度下降法：
- 学习率: 0.1
- 最大迭代: 50次
- 收敛阈值: 1e-4
- 监控权重变化和损失变化

**运行示例**:
```bash
python max_entropy/max_entropy_nlp_demo.py
```

**训练数据** - 中文句子标注:
- 训练集：21个句子，涵盖日常用语
- 测试集：5个句子
- 特征总数：252个
- 训练集准确率：100.00%
- 测试集准确率：94.44%

**标注示例**:
```
句子: 我 喜欢 音乐
标注: 我(r) 喜欢(v) 音乐(n)

句子: 这 是 一 本 书
标注: 这(r) 是(v) 一(m) 本(q) 书(n)

句子: 天气 很 冷
标注: 天气(n) 很(d) 冷(a)
```

**预测功能**:
- 对新句子进行词性标注
- 输出每个词的Top-3概率分布
- 可视化预测结果

**优势**:
- 特征灵活，易于添加新特征
- 概率输出，具有可解释性
- 适用于序列标注任务
- 训练过程透明，可监控优化进展

---

### 8. 支持向量机 (Support Vector Machine)

支持向量机是一种二分类模型，其基本思想是在特征空间中找到间隔最大的分离超平面。

#### 线性可分支持向量机 - 对偶形式 (Linear SVM - Dual Form)
- **文件**: `svm/svm_dual_linear.py`
- **方法**: 拉格朗日对偶 + 二次规划（SLSQP）
- **特点**:
  - 将原始问题转化为对偶问题求解
  - 使用二次规划求解最优拉格朗日乘子
  - 自动识别支持向量（α > 0 的样本）
  - 计算最大间隔分离超平面
  - 可视化决策边界、间隔边界和支持向量

**原始问题**:

$$\min_{w,b} \frac{1}{2}\|w\|^2$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1, \quad i=1,2,...,N$$

**对偶问题**:

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad \alpha_i \geq 0$$

**参数恢复**:

权重向量：

$$w^* = \sum_{i=1}^{N}\alpha_i^* y_i x_i$$

偏置（选择任一支持向量 $x_j$，满足 $\alpha_j^* > 0$）：

$$b^* = y_j - \sum_{i=1}^{N}\alpha_i^* y_i (x_i \cdot x_j)$$

**分离超平面**:

$$w^* \cdot x + b^* = 0$$

**决策函数**:

$$f(x) = \text{sign}(w^* \cdot x + b^*)$$

**分类间隔**:

$$\text{margin} = \frac{2}{\|w^*\|}$$

**运行示例**:
```bash
python svm/svm_dual_linear.py
```

**训练数据**:
- 正例：x₁=(3,3), x₂=(4,3)  标签 y=+1
- 负例：x₃=(1,1)            标签 y=-1
- 样本数：3个

**训练结果**:
- **拉格朗日乘子**：α₁=0.25, α₂=0, α₃=0.25
- **支持向量**：x₁=(3,3) 和 x₃=(1,1)（2个）
- **权重向量**：w=(0.5, 0.5)
- **偏置**：b=-2.0
- **分离超平面**：x₁ + x₂ = 4
- **分类间隔**：≈ 2.828
- **训练准确率**：100%

**关键观察**:
- x₂=(4,3) 的 α₂=0，不是支持向量（在间隔边界之外）
- 只有位于间隔边界上的样本成为支持向量（f(x)=±1）
- 支持向量决定了分离超平面，其他样本可以移除而不影响结果

**可视化**:
- 展示决策边界（黑色实线）
- 展示间隔边界（灰色虚线，f(x)=±1）
- 标记支持向量（绿色圆圈）
- 显示决策区域（彩色背景）

**优势**:
- 理论基础扎实（结构风险最小化）
- 最大间隔准则，泛化能力强
- 只依赖支持向量，模型稀疏
- 对偶形式便于引入核函数

---

#### 线性支持向量机 - 软间隔 (Linear SVM - Soft Margin)
- **文件**: `svm/svm_linear_softmargin.py`
- **方法**: 拉格朗日对偶 + 松弛变量 + 二次规划（SLSQP）
- **特点**:
  - 引入松弛变量处理线性不可分数据
  - 惩罚参数C控制间隔最大化与误分类的权衡
  - 区分边界支持向量和内部支持向量
  - 对噪声和异常点具有鲁棒性
  - 适用于含噪声或类别重叠的数据

**原始问题**:

$$\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{N}\xi_i$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i=1,2,...,N$$

其中 $\xi_i$ 是松弛变量，允许样本不满足硬间隔约束。

**对偶问题**:

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C, \quad i=1,2,...,N$$

**关键区别**：约束条件从 $\alpha_i \geq 0$ 变为 $0 \leq \alpha_i \leq C$（箱约束）

**参数恢复**:

权重向量和偏置的计算与硬间隔相同，但使用边界支持向量（$0 < \alpha_i < C$）计算偏置更稳定。

**支持向量分类**:

1. **非支持向量** (α_i = 0)：
   - 在间隔边界外，正确分类
   - $y_i(w \cdot x_i + b) > 1$

2. **边界支持向量** (0 < α_i < C)：
   - 在间隔边界上，ξ_i = 0
   - $y_i(w \cdot x_i + b) = 1$

3. **内部支持向量** (α_i = C)：
   - 在间隔内或误分类，ξ_i > 0
   - $y_i(w \cdot x_i + b) < 1$

**惩罚参数C的作用**:

- **C很大**：对误分类惩罚大，接近硬间隔，间隔小，可能过拟合
- **C很小**：允许更多误分类，间隔大，模型简单，可能欠拟合
- **C适中**：平衡间隔最大化和误分类，泛化能力好

**运行示例**:
```bash
python svm/svm_linear_softmargin.py
```

**示例1 - 线性可分数据** (C=100.0):
- 数据：正例 x₁=(3,3), x₂=(4,3)；负例 x₃=(1,1)
- 支持向量：2个边界支持向量，0个内部支持向量
- 准确率：100%
- 说明：C很大时，结果接近硬间隔SVM

**示例2 - 含噪声数据** (比较不同C值):
- 数据：6个样本，包含1个噪声点（负类标签但在正类区域）
- **C=0.1**：容忍噪声，间隔大，噪声点被错分
- **C=1.0**：平衡策略，识别出2个内部支持向量（包括噪声点）
- **C=10.0**：强制正确分类，间隔变窄，完美拟合数据
- 准确率：C=0.1/1.0为83.33%，C=10.0为100%

**示例3 - 类别重叠数据** (20个样本):
- 两类数据分布有重叠
- **C=0.1**：大间隔，10个支持向量（8个内部，2个边界）
- **C=1.0**：中等间隔，4个支持向量（全部内部）
- **C=100.0**：小间隔，3个支持向量（全部边界）
- 全部准确率：100%

**可视化**:
- 绿色圆圈：边界支持向量（在间隔边界上）
- 橙色叉号：内部支持向量（在间隔内或误分类）
- 黑色实线：分离超平面
- 灰色虚线：间隔边界
- 彩色背景：决策区域

**优势**:
- 可处理线性不可分数据
- 对噪声和异常点鲁棒
- 通过C参数控制模型复杂度
- 自动识别不同类型的支持向量
- 在间隔最大化和分类错误之间找到平衡

**应用场景**:
- 含噪声的分类问题
- 类别有部分重叠的数据
- 需要控制过拟合风险的场景
- 真实世界的线性分类任务

---

#### 线性支持向量机 - 随机梯度下降 (Linear SVM - SGD)
- **文件**: `svm/svm_sgd_linear.py`
- **方法**: 随机梯度下降 (Stochastic Gradient Descent)
- **特点**:
  - 使用Hinge损失函数进行在线学习
  - 每次迭代只使用单个样本更新参数
  - 计算效率高，适合大规模数据
  - 支持软间隔（通过参数C控制）
  - 可视化训练过程和损失曲线

**Hinge损失函数**:

$$L(w,b) = \max(0, 1 - y_i(w \cdot x_i + b))$$

**完整目标函数**:

$$J(w,b) = \frac{1}{2}\|w\|^2 + C \cdot \sum_{i=1}^{N} \max(0, 1 - y_i(w \cdot x_i + b))$$

第一项是L2正则化项，第二项是Hinge损失之和。

**SGD更新规则**:

对于每个样本 $(x_i, y_i)$：

如果 $y_i(w \cdot x_i + b) < 1$（样本在间隔内或误分类）：

$$w \leftarrow w - \eta(w - C \cdot y_i \cdot x_i)$$

$$b \leftarrow b - \eta(-C \cdot y_i)$$

否则（样本在间隔外，正确分类）：

$$w \leftarrow w - \eta \cdot w$$

$$b$ 不更新$$

其中 $\eta$ 是学习率。

**算法特点**:

1. **在线学习**: 每次只用一个样本，内存占用小
2. **随机性**: 每轮迭代随机打乱样本顺序
3. **计算效率**: 
   - 对偶形式: $O(N^2)$ 的二次规划
   - SGD: $O(N \cdot T)$，T是迭代轮数
4. **收敛性**: 学习率需要仔细调整

**学习率策略**:

- **固定学习率**: $\eta = \text{const}$
  - 简单但可能不收敛或收敛慢
  
- **衰减学习率**: $\eta_t = \frac{\eta_0}{1 + \lambda t}$
  - 开始大步前进，后期精细调整

**与其他SVM方法对比**:

| 特性 | 对偶形式 (QP) | 软间隔 (QP) | SGD |
|------|--------------|-------------|-----|
| 求解方法 | 二次规划 | 二次规划 | 随机梯度下降 |
| 时间复杂度 | $O(N^2)$ ~ $O(N^3)$ | $O(N^2)$ ~ $O(N^3)$ | $O(N \cdot T)$ |
| 空间复杂度 | $O(N^2)$ | $O(N^2)$ | $O(d)$ |
| 适用规模 | 小到中等 | 小到中等 | 大规模 |
| 精确度 | 精确解 | 精确解 | 近似解 |
| 在线学习 | 不支持 | 不支持 | 支持 |
| 核函数 | 易扩展 | 易扩展 | 需要技巧 |

其中 $N$ 是样本数，$d$ 是特征维度，$T$ 是迭代轮数。

**运行示例**:
```bash
python svm/svm_sgd_linear.py
```

**训练数据** - 14个样本的二分类:
- 正例：7个样本（如 (5,9), (3,12), (2,10) 等）
- 负例：7个样本（如 (1,1), (3,-2), (-1,4) 等）
- 特点：类别有轻微重叠，适合测试软间隔

**训练结果** (C=0.5, η=0.01, 1000轮):
- **训练集准确率**: 85.71% (12/14)
- **支持向量数**: 11个
- **权重向量**: w = (0.109, 0.139)
- **偏置**: b = -1.000
- **分类间隔**: 11.32
- **最终损失**: 0.253

**参数比较实验**:

不同C值的效果：

| C值 | 准确率 | 支持向量数 | ||w|| | 最终损失 |
|-----|--------|-----------|---------|---------|
| 0.1 | 85.71% | 10 | 0.160 | 0.057 |
| 1.0 | 85.71% | 9 | 0.255 | 0.486 |
| 10.0 | 42.86% | 10 | 0.609 | 39.51 |

**观察**:
- **C=0.1**: 大间隔，对误分类容忍度高，损失小
- **C=1.0**: 平衡设置，性能稳定
- **C=10.0**: 过度拟合，学习率过大导致不稳定

**可视化内容**:
- 决策边界和间隔边界
- 训练样本的分类情况
- 训练损失随迭代的变化曲线
- 参数信息（C, 学习率, 训练轮数等）

**优势**:
- 计算速度快，适合大规模数据
- 内存占用小，只存储参数不存储样本
- 支持在线学习和增量学习
- 实现简单，易于理解

**局限性**:
- 需要仔细调整学习率
- 收敛到近似解，不如QP精确
- 学习率过大可能不稳定
- 不易直接扩展到核SVM

**应用场景**:
- 大规模文本分类
- 在线广告点击率预测
- 实时数据流分类
- 内存受限的嵌入式系统
- 需要增量更新的场景

**实现技巧**:
1. **样本归一化**: 对特征进行标准化，加速收敛
2. **学习率调整**: 可以使用学习率衰减策略
3. **提前停止**: 监控验证集损失，防止过拟合
4. **小批量SGD**: 介于批量梯度下降和SGD之间
5. **正则化强度**: C值通过交叉验证选择

**与梯度下降的区别**:

- **批量梯度下降 (BGD)**: 每次用全部样本计算梯度
  - 准确但慢，每轮 $O(N)$
  
- **随机梯度下降 (SGD)**: 每次用一个样本
  - 快速但有噪声，可以跳出局部最优
  
- **小批量SGD (Mini-batch)**: 每次用一小批样本
  - 平衡速度和稳定性

---

#### 非线性支持向量机 - 核函数方法 (Nonlinear SVM - Kernel Methods)
- **文件**: `svm/svm_kernel.py`
- **方法**: 核技巧 + 对偶问题求解
- **特点**:
  - 支持4种核函数（线性、多项式、RBF、Sigmoid）
  - 通过核技巧避免显式高维映射
  - 可处理非线性分类问题
  - 完整的核矩阵计算
  - 自动参数调优（auto gamma）
  - 4个经典演示案例

**核函数类型**:

1. **线性核 (Linear Kernel)**:
   $$K(x, z) = x \cdot z$$

2. **多项式核 (Polynomial Kernel)**:
   $$K(x, z) = (\gamma \cdot x \cdot z + r)^d$$
   
   其中 $d$ 是多项式次数，$\gamma$ 是核系数，$r$ 是独立项。

3. **高斯RBF核 (Radial Basis Function)**:
   $$K(x, z) = \exp(-\gamma \|x-z\|^2)$$
   
   最常用的核函数，$\gamma$ 控制高斯分布的宽度。

4. **Sigmoid核 (Sigmoid Kernel)**:
   $$K(x, z) = \tanh(\gamma \cdot x \cdot z + r)$$

**核技巧 (Kernel Trick)**:

核技巧的关键在于：可以在原始空间通过核函数直接计算高维特征空间中的内积，而无需显式地进行映射。

设映射函数为 $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^D$（$D >> d$），则：

$$K(x, z) = \phi(x) \cdot \phi(z)$$

在对偶问题中，决策函数可以表示为：

$$f(x) = \text{sign}\left(\sum_{i=1}^{N}\alpha_i y_i K(x_i, x) + b\right)$$

**对偶问题（核形式）**:

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j K(x_i, x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C$$

注意：核矩阵 $K_{ij} = K(x_i, x_j)$ 替代了原始空间的内积 $x_i \cdot x_j$。

**参数选择**:

- **C (惩罚参数)**: 控制间隔最大化和误分类的权衡
  - C大：严格分类，可能过拟合
  - C小：允许更多误分类，泛化能力可能更好

- **γ (Gamma)**: RBF、多项式、Sigmoid核的参数
  - γ大：决策边界复杂，可能过拟合
  - γ小：决策边界平滑，可能欠拟合
  - 默认：`gamma='auto'` 时为 $1/n\_features$

- **d (Degree)**: 多项式核的次数
  - 通常取2-5
  - 过大容易过拟合

**运行示例**:
```bash
python svm/svm_kernel.py
```

**演示案例**:

**案例1: XOR问题** (8个样本)
- 描述：经典的非线性分类问题，线性不可分
- 数据：四个角点及其邻近点
- 核函数：RBF核 (γ=1.0, C=1.0)
- 结果：**100.00%准确率**，8个支持向量（全部为边界支持向量）
- 特点：完美解决了线性SVM无法处理的XOR问题

**案例2: 同心圆问题** (40个样本)
- 描述：内圆为正类，外环为负类
- 数据：内圆半径0-1.5，外环半径3-4.5
- 核函数：RBF核 (γ=0.5, C=10.0)
- 结果：**100.00%准确率**，20个支持向量（50%）
- 特点：展示RBF核处理圆形分布数据的能力

**案例3: 抛物线分类** (30个样本)
- 描述：数据分布呈抛物线形状
- 数据：正类在抛物线上方，负类在下方
- 核函数：多项式核 (d=2, γ=1.0, r=1.0, C=1.0)
- 结果：**100.00%准确率**，5个支持向量（16.67%）
- 特点：二次多项式核完美拟合二次边界

**案例4: 核函数对比** (60个样本)
- 描述：在同一混合数据集上测试不同核函数
- 数据：正类为两个簇，负类在中间
- 测试核函数：
  - **线性核**: 51.67%准确率，58个支持向量（无法分离）
  - **多项式核 (d=2)**: 100.00%准确率，4个支持向量
  - **RBF核 (γ=0.5)**: 100.00%准确率，17个支持向量
  - **RBF核 (γ=2.0)**: 100.00%准确率，30个支持向量
- 观察：γ越大，决策边界越复杂，支持向量越多

**核函数选择指南**:

| 核函数 | 适用场景 | 优点 | 缺点 |
|--------|---------|------|------|
| **线性核** | 线性可分、高维稀疏数据 | 快速、参数少、可解释 | 只能处理线性问题 |
| **RBF核** | 通用、未知分布 | 强大、灵活、常用 | 参数敏感、易过拟合 |
| **多项式核** | 图像、文本特征 | 可控次数、全局性 | 参数多、数值不稳定 |
| **Sigmoid核** | 神经网络替代 | 类似神经网络 | 不总是正定核 |

**推荐策略**:
1. 首先尝试RBF核（最通用）
2. 如果数据线性可分，使用线性核
3. 如果需要特定次数的交互，使用多项式核
4. 通过交叉验证选择最佳参数

**可视化内容**:
- 非线性决策边界（RBF核可以是圆形、椭圆等）
- 间隔边界 (f(x) = ±1)
- 支持向量标记（绿色圆圈）
- 决策区域着色
- 参数信息展示

**计算复杂度**:

- **训练时间**: $O(N^2 \sim N^3)$（取决于QP求解器）
- **预测时间**: $O(N_{sv} \cdot N_{test})$
  - $N_{sv}$ 是支持向量数量
- **空间复杂度**: $O(N^2)$（存储核矩阵）

**优势**:
- 可以处理任意非线性问题
- 核技巧避免显式高维计算
- 理论基础完善（VC维理论）
- 支持向量稀疏性
- 可以自定义核函数

**局限性**:
- 大规模数据计算开销大
- 参数选择需要经验或交叉验证
- 核矩阵存储需要大量内存
- 多分类需要组合策略（OvO, OvR）

**应用场景**:
- 图像分类（手写数字识别）
- 文本分类（情感分析）
- 生物信息学（蛋白质分类）
- 人脸识别
- 异常检测

**实现细节**:

1. **核矩阵计算**:
   - 对称矩阵，只需计算上三角
   - 可以批量计算提高效率

2. **数值稳定性**:
   - 使用阈值判断支持向量（α > 1e-5）
   - 区分边界和内部支持向量
   - 偏置b使用多个支持向量求平均

3. **参数初始化**:
   - α初始化为零向量
   - gamma自动设置为1/n_features
   - 使用SLSQP求解器

**与线性SVM的对比**:

| 特性 | 线性SVM | 核SVM |
|------|---------|-------|
| 决策边界 | 直线/平面 | 任意曲线/曲面 |
| 适用问题 | 线性可分 | 非线性可分 |
| 计算复杂度 | 低 | 高 |
| 可解释性 | 强 | 弱 |
| 过拟合风险 | 低 | 中等 |
| 参数数量 | 1 (C) | 2-4 (C, γ, d, r) |

**理论基础**:

Mercer定理：一个对称函数 $K(x,z)$ 是有效核函数的充要条件是，对任意数据集，核矩阵K是半正定的。

常用核函数都满足Mercer条件，保证对偶问题有唯一解。

---

### 9. AdaBoost 提升方法 (Adaptive Boosting)

AdaBoost 是一种迭代的集成学习算法，通过组合多个弱分类器构成强分类器。算法的核心思想是调整训练样本的权重分布，使得后续的弱分类器更关注被前面分类器误分的样本。

#### AdaBoost算法实现
- **文件**: `adaboost/adaboost.py`
- **弱分类器**: 决策树桩 (Decision Stump) - 单层决策树
- **方法**: 迭代加权学习
- **特点**:
  - 自适应调整样本权重
  - 弱分类器加权组合
  - 详细的训练过程输出
  - 可视化每个弱分类器及最终强分类器
  - 展示权重分布和错误率变化

**算法流程**:

**输入**:
- 训练数据集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$
- 弱学习算法
- 迭代次数 $M$

**输出**:
- 最终强分类器 $f(x)$

**步骤**:

1. **初始化样本权重分布**:
   
   $$D_1 = (w_{11}, w_{12}, ..., w_{1N}), \quad w_{1i} = \frac{1}{N}, \quad i=1,2,...,N$$
   
   每个样本的初始权重相等。

2. **对 m = 1, 2, ..., M 进行迭代**:

   a) **使用权重分布 $D_m$ 训练弱分类器**:
   
   $$G_m(x): \mathcal{X} \rightarrow \{-1, +1\}$$
   
   弱分类器在加权训练集上学习。

   b) **计算弱分类器的加权错误率**:
   
   $$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^{N} w_{mi} \mathbb{I}(G_m(x_i) \neq y_i)$$
   
   $\mathbb{I}(\cdot)$ 是指示函数，预测错误时为1。

   c) **计算弱分类器的权重**:
   
   $$\alpha_m = \frac{1}{2} \ln \frac{1 - e_m}{e_m}$$
   
   错误率越低，权重越大。当 $e_m < 0.5$ 时，$\alpha_m > 0$。

   d) **更新样本权重分布**:
   
   $$D_{m+1} = (w_{m+1,1}, w_{m+1,2}, ..., w_{m+1,N})$$
   
   $$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i))$$
   
   其中 $Z_m$ 是规范化因子：
   
   $$Z_m = \sum_{i=1}^{N} w_{mi} \exp(-\alpha_m y_i G_m(x_i))$$
   
   使得 $\sum_{i=1}^{N} w_{m+1,i} = 1$。
   
   **权重更新规则**:
   - 如果 $G_m(x_i) = y_i$（分类正确）：$\exp(-\alpha_m y_i G_m(x_i)) = e^{-\alpha_m} < 1$，权重**减小**
   - 如果 $G_m(x_i) \neq y_i$（分类错误）：$\exp(-\alpha_m y_i G_m(x_i)) = e^{\alpha_m} > 1$，权重**增大**

3. **构建最终强分类器**:
   
   $$f(x) = \text{sign}\left(\sum_{m=1}^{M} \alpha_m G_m(x)\right)$$
   
   即所有弱分类器的加权投票。

**决策树桩 (Decision Stump)**:

决策树桩是最简单的决策树，只有一个分裂节点，形式为：

$$
G(x) = 
\begin{cases}
+1, & \text{if } p \cdot x < p \cdot \text{threshold} \\
-1, & \text{otherwise}
\end{cases}
$$

其中：
- $p \in \{-1, +1\}$ 是极性/方向参数
- threshold 是分裂阈值

**弱分类器的选择**:

在每轮迭代中，遍历所有可能的阈值和方向，选择加权错误率最小的：

$$(\text{threshold}^*, p^*) = \arg\min_{\text{threshold}, p} \sum_{i=1}^{N} w_{mi} \mathbb{I}(G(x_i; \text{threshold}, p) \neq y_i)$$

**AdaBoost的关键性质**:

1. **训练误差指数下降**:
   
   AdaBoost的训练误差上界为：
   
   $$\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(f(x_i) \neq y_i) \leq \prod_{m=1}^{M} \sqrt{1 - 4\gamma_m^2}$$
   
   其中 $\gamma_m = 0.5 - e_m$，只要每个弱分类器的错误率 $e_m < 0.5$，训练误差会指数下降。

2. **无需提前知道弱分类器的错误率上界**:
   
   算法自适应调整，不需要事先知道弱分类器的性能。

3. **对噪声敏感**:
   
   如果数据有噪声（标签错误），AdaBoost可能会过度关注噪声点，导致过拟合。

**运行示例**:
```bash
python adaboost/adaboost.py
```

**训练数据** - 10个一维样本:
- 样本: x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
- 标签: y = [1, 1, 1, -1, -1, -1, 1, 1, 1, -1]
- 弱分类器数量: M = 3

**训练结果**:

**第1轮迭代** (m=1):
- 初始权重: $D_1 = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]$（均匀分布）
- 最优弱分类器: $G_1(x)$
  - 阈值: threshold = -0.5
  - 方向: direction = +1
  - 分类规则: 如果 x < -0.5 则预测 +1，否则预测 -1
- 加权错误率: $e_1 = 0.4000$
- 分类器权重: $\alpha_1 = 0.2027$
- 更新后权重: 误分类样本（索引0,1,2,6,7,8）权重增加

**第2轮迭代** (m=2):
- 当前权重: $D_2$ 更关注第1轮误分的样本
- 最优弱分类器: $G_2(x)$
  - 阈值: threshold = 5.5
  - 方向: direction = +1
  - 分类规则: 如果 x < 5.5 则预测 +1，否则预测 -1
- 加权错误率: $e_2 = 0.3750$
- 分类器权重: $\alpha_2 = 0.2554$（错误率更低，权重更大）
- 更新后权重: 继续调整权重分布

**第3轮迭代** (m=3):
- 当前权重: $D_3$ 根据前两轮结果调整
- 最优弱分类器: $G_3(x)$
  - 阈值: threshold = -0.5
  - 方向: direction = +1
  - 分类规则: 如果 x < -0.5 则预测 +1，否则预测 -1
- 加权错误率: $e_3 = 0.4667$（较高，说明难分样本）
- 分类器权重: $\alpha_3 = 0.0668$（错误率高，权重小）

**最终强分类器**:

$$f(x) = \text{sign}(0.2027 \cdot G_1(x) + 0.2554 \cdot G_2(x) + 0.0668 \cdot G_3(x))$$

**训练集性能**:
- 训练准确率: **60.00%** (6/10)
- 正确分类: 样本 0, 1, 2, 6, 7, 8
- 误分类: 样本 3, 4, 5, 9

**预测详情**:

| 样本 | 真实标签 | 预测标签 | 结果 | 加权得分 |
|:---:|:-------:|:-------:|:----:|:--------:|
| 0 | +1 | +1 | ✓ | +0.4249 |
| 1 | +1 | +1 | ✓ | +0.4249 |
| 2 | +1 | +1 | ✓ | +0.4249 |
| 3 | -1 | +1 | ✗ | +0.4249 |
| 4 | -1 | +1 | ✗ | +0.4249 |
| 5 | -1 | +1 | ✗ | +0.4249 |
| 6 | +1 | +1 | ✓ | +0.0668 |
| 7 | +1 | +1 | ✓ | +0.0668 |
| 8 | +1 | +1 | ✓ | +0.0668 |
| 9 | -1 | -1 | ✓ | -0.3222 |

**可视化内容**:
1. **弱分类器展示**（子图1-3）:
   - 每个弱分类器的决策边界（红色虚线）
   - 分类区域着色（蓝色/红色）
   - 样本点大小表示权重
   - 显示阈值和错误率信息

2. **最终强分类器**（子图4）:
   - 组合后的决策边界
   - 所有弱分类器的加权投票结果
   - 最终分类区域

3. **权重演化**（子图5）:
   - 每轮迭代后的样本权重分布
   - 展示算法如何逐渐关注难分样本

4. **错误率变化**（子图6）:
   - 每个弱分类器的加权错误率 $e_m$
   - 训练准确率随迭代的变化

**AdaBoost vs 其他集成方法**:

| 特性 | AdaBoost | Bagging | Random Forest |
|------|----------|---------|---------------|
| 训练方式 | 串行（顺序） | 并行 | 并行 |
| 样本权重 | 自适应调整 | Bootstrap采样 | Bootstrap采样 |
| 弱学习器 | 任意（常用决策树桩） | 任意（常用深决策树） | 决策树 |
| 减少误差类型 | 偏差+方差 | 主要是方差 | 主要是方差 |
| 对噪声敏感度 | 高 | 低 | 低 |
| 可解释性 | 中等 | 低 | 低 |

**优点**:
- 精度高，可以将弱分类器提升为强分类器
- 不容易过拟合（理论上）
- 不需要特征工程
- 可以识别重要样本（通过权重）
- 可以处理不平衡数据

**缺点**:
- 对噪声和异常值敏感
- 训练时间较长（串行）
- 不适合实时应用
- 参数调优（弱分类器数量M）需要经验

**应用场景**:
- 人脸检测（Viola-Jones算法）
- 文本分类
- 特征选择
- 排序问题（如搜索引擎）
- 任何需要高精度的分类任务

**实现技巧**:
1. **弱分类器选择**: 决策树桩简单高效，防止单个分类器过拟合
2. **提前停止**: 监控验证集性能，防止过拟合
3. **权重平滑**: 避免权重过度集中在少数样本上
4. **异常值处理**: 预处理阶段去除或降低异常值影响

**理论分析**:

**前向分步算法**:

AdaBoost是前向分步加法模型的特例。设基函数为 $G_m(x)$，则加法模型为：

$$f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)$$

前向分步算法在每一步求解：

$$(\alpha_m, G_m) = \arg\min_{\alpha, G} \sum_{i=1}^{N} L\left(y_i, f_{m-1}(x_i) + \alpha G(x_i)\right)$$

其中 $f_{m-1}(x)$ 是前 $m-1$ 轮的模型。

**损失函数**:

AdaBoost使用指数损失函数：

$$L(y, f(x)) = \exp(-y f(x))$$

可以证明，使用指数损失的前向分步算法等价于AdaBoost算法。

**泛化误差界**:

AdaBoost的泛化误差可以用VC维理论或Margin理论来分析，在适当条件下具有较好的泛化能力。

---

#### 前向分步算法实现
- **文件**: `adaboost/forward_stagewise.py`
- **损失函数**: 指数损失、平方损失
- **方法**: 前向分步加法模型
- **特点**:
  - 通用的加法模型学习框架
  - 支持多种损失函数
  - 展示AdaBoost作为前向分步算法的特例
  - 对比不同损失函数的效果
  - 完整的理论对应实现

**前向分步算法原理**:

前向分步算法是一种通用的加法模型学习算法，AdaBoost是其使用指数损失函数的特例。

**加法模型**:

$$f(x) = \sum_{m=1}^{M} \beta_m b(x; \gamma_m)$$

其中：
- $b(x; \gamma_m)$ 是基函数（弱学习器）
- $\gamma_m$ 是基函数的参数
- $\beta_m$ 是基函数的系数

**算法步骤**:

1. **初始化**: $f_0(x) = 0$

2. **对 m = 1, 2, ..., M**:
   
   a) **极小化损失函数**:
   
   $$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + \beta \cdot b(x_i; \gamma))$$
   
   b) **更新模型**:
   
   $$f_m(x) = f_{m-1}(x) + \beta_m \cdot b(x; \gamma_m)$$

3. **输出最终模型**: $f(x) = f_M(x)$

**关键特点**:
- **前向**: 从前向后逐步学习，每次只优化一个基函数
- **分步**: 简化优化问题，避免一次性优化所有参数
- **通用**: 可以使用不同的损失函数

**两种损失函数对比**:

**1. 指数损失函数** (Exponential Loss):

$$L(y, f(x)) = \exp(-y f(x))$$

**性质**:
- AdaBoost使用的损失函数
- 样本权重为 $w_i = \exp(-y_i f(x_i))$
- 对误分类样本惩罚呈指数增长
- 对噪声敏感

**2. 平方损失函数** (Squared Loss):

$$L(y, f(x)) = (y - f(x))^2$$

**性质**:
- 类似梯度提升（GBDT）
- 通过拟合残差学习
- 残差 $r_i = y_i - f_{m-1}(x_i)$
- 对噪声相对鲁棒

**前向分步算法与AdaBoost的关系**:

**定理**: 前向分步算法使用指数损失函数时，等价于AdaBoost算法。

**证明思路**:

在第m步，极小化：

$$\sum_{i=1}^{N} \exp(-y_i (f_{m-1}(x_i) + \alpha G(x_i)))$$

设 $w_{mi} = \exp(-y_i f_{m-1}(x_i))$，则：

$$\sum_{i=1}^{N} w_{mi} \exp(-y_i \alpha G(x_i))$$

分离正确和错误分类的样本：

$$= \sum_{y_i = G(x_i)} w_{mi} e^{-\alpha} + \sum_{y_i \neq G(x_i)} w_{mi} e^{\alpha}$$

$$= e^{-\alpha} \sum_{i=1}^{N} w_{mi} + (e^{\alpha} - e^{-\alpha}) \sum_{y_i \neq G(x_i)} w_{mi}$$

对 $\alpha$ 求导并令其为0，得到：

$$\alpha^* = \frac{1}{2} \ln \frac{1 - e}{e}$$

其中 $e = \frac{\sum_{y_i \neq G(x_i)} w_{mi}}{\sum_{i=1}^{N} w_{mi}}$ 是加权错误率。

这正是AdaBoost中 $\alpha_m$ 的计算公式！

**运行示例**:
```bash
python adaboost/forward_stagewise.py
```

**演示1: 指数损失（等价于AdaBoost）**:
- 训练数据: 10个样本
- 弱分类器数量: 3
- 最终准确率: **100.00%** (10/10)
- 强分类器: $f(x) = \text{sign}(0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7520 \cdot G_3(x))$

**训练过程**:
- **第1轮**: 均匀权重 → $e_1=0.3$, $\alpha_1=0.4236$
- **第2轮**: 调整权重 → $e_2=0.2143$, $\alpha_2=0.6496$ 
- **第3轮**: 继续调整 → $e_3=0.1818$, $\alpha_3=0.7520$
- 训练误差从30%降至0%

**演示2: 平方损失**:
- 训练数据: 10个样本
- 弱分类器数量: 5
- 学习率: 0.5
- 最终准确率: **100.00%** (10/10)

**训练过程**:
- 每轮拟合当前残差
- 逐步减小预测误差
- 最终完美拟合训练数据

**演示3: 两种损失函数对比**:
- 在相同数据集上对比指数损失和平方损失
- 可视化展示决策边界的差异
- 分析不同损失函数的适用场景

**算法对比**:

| 特性 | 指数损失 | 平方损失 |
|------|---------|---------|
| **等价于** | AdaBoost | GBDT（梯度提升） |
| **更新策略** | 权重调整 | 残差拟合 |
| **收敛速度** | 快 | 较慢 |
| **对噪声** | 敏感 | 较鲁棒 |
| **适用场景** | 二分类 | 分类/回归 |
| **计算复杂度** | 低 | 低 |

**可视化内容**:
1. **每个弱分类器**: 显示阈值、分类区域、样本权重
2. **最终强分类器**: 展示所有弱分类器的组合效果
3. **训练误差曲线**: 显示误差随迭代的变化
4. **对比图**: 直观对比两种损失函数的决策边界

**理论意义**:
- 揭示AdaBoost的本质：指数损失下的前向分步算法
- 统一框架：可扩展到其他损失函数（如Logistic损失）
- 为理解集成学习提供深刻洞察

**实践价值**:
- 通用框架易于扩展
- 支持自定义损失函数
- 适合研究和教学

---

## 🛠️ 技术栈

- **Python**: 3.13+
- **NumPy**: 数值计算
- **Pandas**: 数据处理
- **Matplotlib**: 数据可视化
- **SciPy**: 科学计算（优化算法）
- **Scikit-learn**: 机器学习库（用于对比验证）
- **PyTorch**: 深度学习框架（2.9.0+，支持MPS加速）

## 🚀 快速开始

### 1. 克隆项目
```bash
git clone https://github.com/Kshqsz/machine-learning-lab.git
cd machine-learning-lab
```

### 2. 创建虚拟环境
```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate  # Windows
```

### 3. 安装依赖
```bash
# 基础依赖
pip install numpy pandas scipy scikit-learn matplotlib

# PyTorch (可选，用于深度学习)
pip install torch torchvision torchaudio
```

### 4. 运行示例
```bash
python linear_regression/gradient_descent.py
```

## 📝 学习笔记

### 线性回归 - 梯度下降
**核心思想**: 通过不断调整参数，使得预测值与真实值之间的误差最小化。

**损失函数**: 均方误差 (MSE)
$$L(w, b) = \frac{1}{n}\sum_{i=1}^{n}(y_i - (wx_i + b))^2$$

**参数更新**:
$$w := w - \alpha \frac{\partial L}{\partial w}$$
$$b := b - \alpha \frac{\partial L}{\partial b}$$

其中 $\alpha$ 是学习率。

---

### 感知机
**核心思想**: 线性可分数据的二分类模型，通过误分类驱动的学习算法找到分离超平面。

**模型**: 
$$f(x) = \text{sign}(w \cdot x + b)$$

**损失函数**: 误分类点到超平面的总距离
$$L(w, b) = -\sum_{x_i \in M} y_i(w \cdot x_i + b)$$

其中 $M$ 是误分类点的集合。

**原始形式更新规则**:
$$w \leftarrow w + \eta y_i x_i$$
$$b \leftarrow b + \eta y_i$$

**对偶形式表示**:
$$f(x) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i x_i \cdot x + b\right)$$

对偶形式的优势是可以预先计算 Gram 矩阵，当样本数远小于特征维度时更高效。

---

### k近邻法
**核心思想**: 给定一个训练数据集，对于新的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

**距离度量**: 欧氏距离
$$d(x_i, x_j) = \sqrt{\sum_{l=1}^{n}(x_i^{(l)} - x_j^{(l)})^2}$$

**k-d树构造**:
- 选择切分轴：循环选择坐标轴
- 选择切分点：选择该轴坐标的中位数
- 递归构造左右子树

**最近邻搜索**:
1. 从根节点出发，递归地向下访问k-d树
2. 若目标点当前维的坐标小于切分点，则移动到左子节点，否则移动到右子节点
3. 到达叶节点时，计算距离，更新最近点
4. 递归回退，检查是否需要在另一子树中搜索（剪枝）

**时间复杂度**:
- 构造k-d树: $O(n \log n)$
- 搜索: 平均 $O(\log n)$，最坏 $O(n)$

---

### 朴素贝叶斯
**核心思想**: 基于贝叶斯定理与特征条件独立假设，通过训练数据学习先验概率和条件概率，对新实例计算后验概率进行分类。

**贝叶斯定理**:
$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$$

**条件独立性假设**:
$$P(X=x|Y=c_k) = \prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)$$

**极大似然估计 (MLE)**:
$$P(Y=c_k) = \frac{N_{c_k}}{N}$$
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl}}{N_{c_k}}$$

**贝叶斯估计 (加一平滑)**:
$$P(Y=c_k) = \frac{N_{c_k} + \lambda}{N + K \cdot \lambda}$$
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl} + \lambda}{N_{c_k} + S_j \cdot \lambda}$$

其中 $\lambda \geq 0$，常取 $\lambda=1$ (拉普拉斯平滑)。

**分类决策**:
$$y = \arg\max_{c_k} P(Y=c_k) \prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)$$

---

### 决策树
**核心思想**: 通过树形结构表示决策过程，每个内部节点表示一个特征上的测试，每个分支代表测试结果，每个叶节点存放一个类标记或预测值。

**分类树 - 基尼指数**:

基尼指数表示集合的不纯度：
$$\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2$$

其中 $p_k$ 是样本属于第k类的概率。

特征A条件下的基尼指数：
$$\text{Gini}(D, A) = \frac{|D_1|}{|D|}\text{Gini}(D_1) + \frac{|D_2|}{|D|}\text{Gini}(D_2)$$

选择基尼指数最小的特征及其切分点。

**回归树 - 均方误差**:

划分点s处的平方误差：
$$\min_{s}\left[\min_{c_1}\sum_{x_i \in R_1(s)}(y_i - c_1)^2 + \min_{c_2}\sum_{x_i \in R_2(s)}(y_i - c_2)^2\right]$$

其中 $c_1$ 和 $c_2$ 分别是左右区域的输出值（均值）。

**停止条件**:
- 节点中样本属于同一类别
- 达到最大深度
- 样本数小于最小分裂数
- MSE/基尼指数减少量小于阈值

---

### 逻辑斯谛回归
**核心思想**: 通过 Sigmoid 函数将线性模型的输出映射到 (0,1) 区间，表示样本属于某类的概率，是一种广义线性模型。

**Sigmoid 函数**:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

性质：
- 值域为 (0, 1)，可解释为概率
- 单调递增
- $\sigma(0) = 0.5$

**二项逻辑斯谛回归模型**:

$$P(Y=1|x) = \frac{1}{1 + \exp(-(w \cdot x + b))}$$

$$P(Y=0|x) = 1 - P(Y=1|x)$$

**极大似然估计**:

似然函数：

$$L(w,b) = \prod_{i=1}^{n} [p_i]^{y_i}[1-p_i]^{1-y_i}$$

对数似然函数：

$$\log L(w,b) = \sum_{i=1}^{n}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$$

**损失函数（负对数似然）**:

$$J(w,b) = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$$

这也称为交叉熵损失（Cross-Entropy Loss）。

**梯度下降更新规则**:

梯度：

$$\frac{\partial J}{\partial w} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)x_i$$

$$\frac{\partial J}{\partial b} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)$$

参数更新：

$$w \leftarrow w - \alpha \frac{\partial J}{\partial w}$$

$$b \leftarrow b - \alpha \frac{\partial J}{\partial b}$$

其中 $\alpha$ 是学习率。

**决策边界**:

当 $P(Y=1|x) = 0.5$ 时，即 $w \cdot x + b = 0$，这就是决策边界。

对于一维特征：

$$x = -\frac{b}{w}$$

**优点**:
- 输出具有概率意义
- 计算代价低，易于实现
- 可解释性强

**局限性**:
- 只能处理线性可分或近似线性可分的问题
- 对特征共线性敏感

---

### 多项逻辑斯谛回归
**核心思想**: 将二项逻辑斯谛回归推广到多分类问题，使用 Softmax 函数将线性输出转换为概率分布。

**Softmax 函数**:
$$P(Y=k|x) = \frac{\exp(w_k \cdot x + b_k)}{\sum_{j=1}^{K}\exp(w_j \cdot x + b_j)}$$

性质：
- 输出K个概率值，和为1
- 单调性：线性得分越高，概率越大
- 当K=2时退化为二项逻辑斯谛回归

**参数**:
对于K个类别，需要学习K组参数：
$$(w_1, b_1), (w_2, b_2), ..., (w_K, b_K)$$

**损失函数（交叉熵）**:

$$J(W,b) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K} \mathbb{1}(y_i=k) \log P(Y=k|x_i)$$

其中 $\mathbb{1}(\cdot)$ 是指示函数，当 $y_i=k$ 时为1，否则为0。

**梯度计算**:

对于第k类的参数：

$$\frac{\partial J}{\partial w_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))x_i$$

$$\frac{\partial J}{\partial b_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))$$

**优化算法**:
- 梯度下降法：简单但可能较慢
- BFGS拟牛顿法：自动计算近似Hessian矩阵，收敛快
- L-BFGS：BFGS的内存优化版本

**决策规则**:
$$\hat{y} = \arg\max_k P(Y=k|x)$$

选择概率最大的类别作为预测结果。

---

### 最大熵模型
**核心思想**: 在满足约束条件的前提下，选择熵最大的概率分布。熵最大意味着对未知信息不做任何主观假设，是一种最保守的策略。

**熵的定义**:
$$H(P) = -\sum_{x,y} \tilde{P}(x) P(y|x) \log P(y|x)$$

其中 $\tilde{P}(x)$ 是经验分布。

**特征函数**:

$$
f_i(x, y) = 
\begin{cases} 
1, & \text{if } x,y \text{ satisfy certain condition} \\ 
0, & \text{otherwise}
\end{cases}
$$

特征函数表示：如果 $(x,y)$ 满足某个事实或条件，取值为1；否则为0。

**约束条件**:

模型期望 = 经验期望

$$E_P[f_i] = E_{\tilde{P}}[f_i]$$

即：

$$\sum_{x,y} \tilde{P}(x) P(y|x) f_i(x,y) = \sum_{x,y} \tilde{P}(x,y) f_i(x,y)$$

**最大熵模型**:

最优解具有指数形式：

$$P_w(y|x) = \frac{1}{Z_w(x)} \exp\left(\sum_{i=1}^{n} w_i f_i(x,y)\right)$$

归一化因子：

$$Z_w(x) = \sum_y \exp\left(\sum_{i=1}^{n} w_i f_i(x,y)\right)$$

**与多项逻辑斯谛回归的关系**:

最大熵模型在数学形式上等价于多项逻辑斯谛回归：
- 都使用 Softmax 归一化
- 都是对数线性模型
- 都用交叉熵作为损失函数

区别：
- **视角不同**: 最大熵从信息论角度（最大化熵），逻辑回归从概率角度（最大似然）
- **特征构造**: 最大熵强调特征函数 $f_i(x,y)$，逻辑回归强调特征向量 $x$

**极大似然估计**:

对偶问题：

$$\max_w \sum_{x,y} \tilde{P}(x,y) \log P_w(y|x)$$

等价于最小化负对数似然：

$$\min_w -\sum_{x,y} \tilde{P}(x,y) \log P_w(y|x)$$

**梯度**:

$$\frac{\partial L}{\partial w_i} = \sum_{x,y} \tilde{P}(x) [P_w(y|x) f_i(x,y) - \tilde{P}(y|x) f_i(x,y)]$$

简化为：

$$\frac{\partial L}{\partial w_i} = E_P[f_i] - E_{\tilde{P}}[f_i]$$

即模型期望与经验期望的差。

**优化算法**:
- 梯度下降法 (GD)
- 拟牛顿法 (BFGS)
- 改进的迭代尺度法 (IIS)
- 通用迭代尺度法 (GIS)

**应用场景**:
- 自然语言处理（词性标注、命名实体识别）
- 文本分类
- 信息抽取
- 机器翻译

**优势**:
- 特征灵活，可以组合任意特征
- 理论基础扎实（最大熵原理）
- 可解释性强
- 不需要特征独立性假设（相比朴素贝叶斯）

---

### 支持向量机
**核心思想**: 在特征空间中找到间隔最大的分离超平面。支持向量机基于结构风险最小化原则，具有很强的泛化能力。

**原始优化问题（硬间隔）**:

$$\min_{w,b} \frac{1}{2}\|w\|^2$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1, \quad i=1,2,...,N$$

目标是最小化 $\|w\|^2$，等价于最大化间隔 $\frac{2}{\|w\|}$。

**拉格朗日函数**:

引入拉格朗日乘子 $\alpha_i \geq 0$：

$$L(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{N}\alpha_i[y_i(w \cdot x_i + b) - 1]$$

**对偶问题**:

通过对 $w$ 和 $b$ 求偏导并令其为零：

$$w = \sum_{i=1}^{N}\alpha_i y_i x_i$$

$$\sum_{i=1}^{N}\alpha_i y_i = 0$$

代入拉格朗日函数得到对偶问题：

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad \alpha_i \geq 0, \quad i=1,2,...,N$$

**KKT条件**:

$$\alpha_i \geq 0$$

$$y_i(w \cdot x_i + b) - 1 \geq 0$$

$$\alpha_i[y_i(w \cdot x_i + b) - 1] = 0$$

**支持向量**:

满足 $\alpha_i > 0$ 的样本称为支持向量，这些样本位于间隔边界上：

$$y_i(w \cdot x_i + b) = 1$$

非支持向量的 $\alpha_i = 0$，对模型没有影响。

**参数恢复**:

从最优解 $\alpha^*$ 恢复参数：

权重向量：

$$w^* = \sum_{i=1}^{N}\alpha_i^* y_i x_i$$

偏置（选择任一支持向量 $x_s$）：

$$b^* = y_s - w^* \cdot x_s = y_s - \sum_{i=1}^{N}\alpha_i^* y_i (x_i \cdot x_s)$$

**决策函数**:

$$
f(x) = \text{sign}(w^* \cdot x + b^*)
$$

或者用对偶形式表示：

$$
f(x) = \text{sign}\left(\sum_{i=1}^{N}\alpha_i^* y_i (x_i \cdot x) + b^*\right)
$$

**几何间隔**:

分离超平面到最近样本点的距离：

$$\gamma = \frac{1}{\|w^*\|}$$

最大间隔（两个间隔边界之间的距离）：

$$\text{margin} = \frac{2}{\|w^*\|}$$

**对偶形式的优势**:
1. **简化计算**: 对偶问题只依赖于样本的内积 $(x_i \cdot x_j)$
2. **引入核函数**: 可以用核函数 $K(x_i, x_j)$ 替代内积，实现非线性分类
3. **稀疏性**: 只有支持向量的 $\alpha_i > 0$，其余为0
4. **样本数 vs 特征数**: 当样本数 $N$ 远小于特征维度 $d$ 时，对偶形式更高效

**求解方法**:
- 二次规划（QP）：SLSQP, 内点法
- SMO算法（Sequential Minimal Optimization）：专门针对SVM的高效算法
- 坐标上升法

**线性可分的条件**:
训练数据集线性可分的充要条件是对偶问题有解且存在 $\alpha^*$ 使得：

$$\sum_{i=1}^{N}\alpha_i^* y_i = 0, \quad \alpha_i^* \geq 0$$

**优势**:
- 最大间隔准则，泛化能力强
- 只依赖支持向量，模型稀疏
- 理论基础完善（统计学习理论）
- 通过核技巧可处理非线性问题

**局限性**:
- 只适用于二分类（多分类需要组合策略）
- 对大规模数据集计算开销大
- 对参数和核函数的选择敏感

---

### 线性支持向量机 - 软间隔
**核心思想**: 引入松弛变量允许部分样本不满足硬间隔约束，使SVM能够处理线性不可分和含噪声的数据。

**原始问题（软间隔）**:

$$\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{N}\xi_i$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1 - \xi_i$$

$$\xi_i \geq 0, \quad i=1,2,...,N$$

其中：
- $\xi_i$ 是松弛变量，度量样本违反硬间隔约束的程度
- $C > 0$ 是惩罚参数，控制间隔最大化和误分类的权衡

**目标函数的两部分**:
1. $\frac{1}{2}\|w\|^2$：最大化间隔
2. $C\sum_{i=1}^{N}\xi_i$：最小化违反约束的程度

**松弛变量的意义**:

$$\xi_i = \begin{cases}
0, & y_i(w \cdot x_i + b) \geq 1 \quad \text{(正确分类且在间隔外)} \\
1 - y_i(w \cdot x_i + b), & 0 < y_i(w \cdot x_i + b) < 1 \quad \text{(在间隔内)} \\
1 + |w \cdot x_i + b|, & y_i(w \cdot x_i + b) < 0 \quad \text{(误分类)}
\end{cases}$$

**对偶问题**:

形式与硬间隔相同，但约束条件改变：

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0$$

$$0 \leq \alpha_i \leq C, \quad i=1,2,...,N$$

**关键区别**：上界约束 $\alpha_i \leq C$（箱约束/盒约束）

**KKT条件（软间隔）**:

$$\alpha_i \geq 0$$

$$y_i(w \cdot x_i + b) - 1 + \xi_i \geq 0$$

$$\alpha_i[y_i(w \cdot x_i + b) - 1 + \xi_i] = 0$$

$$C - \alpha_i - \mu_i = 0, \quad \mu_i \geq 0$$

$$\mu_i \xi_i = 0$$

**支持向量的三种类型**:

根据 $\alpha_i$ 的值，样本分为三类：

1. **α_i = 0（非支持向量）**:
   - ξ_i = 0，样本在间隔外正确分类
   - $y_i(w \cdot x_i + b) > 1$
   - 对模型没有贡献

2. **0 < α_i < C（边界支持向量）**:
   - ξ_i = 0，μ_i > 0
   - 样本恰好在间隔边界上
   - $y_i(w \cdot x_i + b) = 1$
   - 用于计算偏置 $b$

3. **α_i = C（内部支持向量）**:
   - μ_i = 0，ξ_i > 0
   - 样本在间隔内或被误分类
   - $y_i(w \cdot x_i + b) < 1$
   - 违反了硬间隔约束

**惩罚参数C的作用**:

C控制对误分类的容忍度：

- **C → ∞**：不允许违反约束，退化为硬间隔SVM
  - 优点：完美拟合训练数据
  - 缺点：对噪声敏感，可能过拟合

- **C很小**：允许较多违反约束
  - 优点：间隔大，泛化能力可能更好
  - 缺点：训练误差大，可能欠拟合

- **C适中**：平衡间隔和误差
  - 最佳选择，通常通过交叉验证确定

**参数选择**:

偏置的计算应使用边界支持向量 (0 < α_i < C)：

$$b = y_s - \sum_{i=1}^{N}\alpha_i y_i (x_i \cdot x_s)$$

为了稳定性，使用所有边界支持向量的平均值：

$$b = \frac{1}{|S|}\sum_{s \in S}\left(y_s - \sum_{i=1}^{N}\alpha_i y_i (x_i \cdot x_s)\right)$$

其中集合 S 定义为所有边界支持向量的索引：

$$S = \{i: 0 < \alpha_i < C\}$$

**软间隔 vs 硬间隔**:

| 特性 | 硬间隔 | 软间隔 |
|------|--------|--------|
| 适用数据 | 线性可分 | 线性可分/不可分 |
| 约束条件 | $\alpha_i \geq 0$ | $0 \leq \alpha_i \leq C$ |
| 松弛变量 | 无 | $\xi_i \geq 0$ |
| 对噪声 | 敏感 | 鲁棒 |
| 间隔 | 固定最大 | 可调整 |
| 参数 | 无 | C |

**模型选择**:

C的选择方法：
1. 交叉验证：在验证集上测试不同C值
2. 网格搜索：在对数尺度上搜索（如 $10^{-3}, 10^{-2}, ..., 10^3$）
3. 经验规则：从 $C=1$ 开始调整

**优势**:
- 处理线性不可分数据
- 对噪声和异常值鲁棒
- 灵活控制模型复杂度
- 理论基础完善（结构风险最小化 + 经验风险最小化）

**应用**:
- 真实数据往往含噪声，软间隔更实用
- 文本分类、图像识别等实际问题
- 需要在拟合和泛化之间权衡的场景

---

### AdaBoost 提升方法
**核心思想**: 通过迭代地训练弱分类器，并调整样本权重使后续分类器更关注难分样本，最终将多个弱分类器组合成强分类器。

**算法框架**:

1. **初始化权重**: $D_1 = (w_{11}, ..., w_{1N}), \quad w_{1i} = \frac{1}{N}$

2. **迭代训练** (m = 1, 2, ..., M):
   - 训练弱分类器 $G_m(x)$
   - 计算加权错误率 $e_m = \sum_{i=1}^{N} w_{mi} \mathbb{I}(G_m(x_i) \neq y_i)$
   - 计算分类器权重 $\alpha_m = \frac{1}{2} \ln \frac{1 - e_m}{e_m}$
   - 更新样本权重 $w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i))$

3. **构建强分类器**: $f(x) = \text{sign}\left(\sum_{m=1}^{M} \alpha_m G_m(x)\right)$

**权重更新机制**:
- 分类正确的样本：权重减小（$e^{-\alpha_m}$）
- 分类错误的样本：权重增大（$e^{\alpha_m}$）
- 使得下一轮更关注被误分的样本

**决策树桩** (Decision Stump):

最简单的弱分类器，单层决策树：

$$
G(x) = 
\begin{cases}
+1, & \text{if } p \cdot x < p \cdot \text{threshold} \\
-1, & \text{otherwise}
\end{cases}
$$

**理论基础 - 前向分步算法**:

AdaBoost是前向分步加法模型的特例，使用指数损失函数：

$$L(y, f(x)) = \exp(-y f(x))$$

每一步优化：

$$(\alpha_m, G_m) = \arg\min_{\alpha, G} \sum_{i=1}^{N} \exp\left(-y_i \left(f_{m-1}(x_i) + \alpha G(x_i)\right)\right)$$

**训练误差界**:

$$\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(f(x_i) \neq y_i) \leq \prod_{m=1}^{M} \sqrt{1 - 4\gamma_m^2}$$

其中 $\gamma_m = 0.5 - e_m$，只要 $e_m < 0.5$，训练误差指数下降。

**优点**:
- 精度高，能显著提升弱分类器性能
- 不需要事先知道弱分类器的错误率
- 简单易实现，不需要复杂参数调优
- 可以识别重要样本和特征

**缺点**:
- 对噪声和异常值敏感（可能过拟合噪声点）
- 训练时间较长（串行训练）
- 需要足够多的训练数据

**应用**:
- 人脸检测（Viola-Jones算法）
- 文本分类和情感分析
- 医疗诊断
- 特征选择和排序

---

### 前向分步算法
**核心思想**: 通用的加法模型学习框架，AdaBoost是其使用指数损失的特例。

**加法模型**:

$$f(x) = \sum_{m=1}^{M} \beta_m b(x; \gamma_m)$$

其中 $b(x; \gamma_m)$ 是基函数，$\beta_m$ 是系数。

**算法步骤**:

1. **初始化**: $f_0(x) = 0$

2. **迭代** (m = 1 到 M):
   - 极小化损失：$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + \beta \cdot b(x_i; \gamma))$
   - 更新模型：$f_m(x) = f_{m-1}(x) + \beta_m \cdot b(x; \gamma_m)$

3. **输出**: $f(x) = f_M(x)$

**关键性质**:
- **前向**：从前向后逐步学习
- **分步**：每次只优化一个基函数，简化优化
- **通用**：支持不同损失函数

**两种常用损失函数**:

**指数损失** (AdaBoost):
$$L(y, f(x)) = \exp(-y f(x))$$

特点：对误分类样本惩罚呈指数增长，权重调整机制

**平方损失** (类似GBDT):
$$L(y, f(x)) = (y - f(x))^2$$

特点：通过拟合残差学习，对噪声相对鲁棒

**前向分步算法 = AdaBoost 证明**:

使用指数损失时，第m步极小化：

$$\sum_{i=1}^{N} \exp(-y_i (f_{m-1}(x_i) + \alpha G(x_i)))$$

设 $w_i = \exp(-y_i f_{m-1}(x_i))$，分离正确和错误样本后求导，得到：

$$\alpha^* = \frac{1}{2} \ln \frac{1-e}{e}$$

这正是AdaBoost的 $\alpha_m$ 公式！

**意义**:
- 揭示AdaBoost的理论本质
- 提供统一的集成学习框架
- 可扩展到其他损失函数（Logistic损失、Hinge损失等）

---

## 📈 学习计划与进度

### 监督学习算法

| 状态 | 算法名称 |
|:---:|---------|
| ✅ | **线性回归 - 梯度下降法** |
| ✅ | **感知机 - 原始形式** |
| ✅ | **感知机 - 对偶形式** |
| ✅ | **k近邻法 (k-NN) - k-d树** |
| ✅ | **朴素贝叶斯 - 极大似然估计** |
| ✅ | **朴素贝叶斯 - 贝叶斯估计** |
| ✅ | **决策树 - 分类树 (基尼指数)** |
| ✅ | **决策树 - 回归树 (MSE)** |
| ✅ | **逻辑斯谛回归 - 二项逻辑斯谛回归** |
| ✅ | **逻辑斯谛回归 - 多项逻辑斯谛回归** |
| ✅ | **最大熵模型 - 中文词性标注** |
| ✅ | **支持向量机 - 线性可分SVM (对偶形式)** |
| ✅ | **支持向量机 - 线性SVM (软间隔)** |
| ✅ | **支持向量机 - 线性SVM (随机梯度下降)** |
| ✅ | **支持向量机 - 非线性SVM (核函数方法)** |
| ✅ | **提升方法 (AdaBoost)** |
| ⬜ | EM算法 |
| ⬜ | 隐马尔可夫模型 |

### 其他算法

| 状态 | 算法名称 |
|:---:|---------|
| ⬜ | 多项式回归 |
| ⬜ | 正则化 (Ridge/Lasso) |
| ⬜ | 聚类算法 (K-Means) |
| ⬜ | 主成分分析 (PCA) |
| ⬜ | 神经网络基础 |

**进度统计**: 已完成 16 / 19 个算法 (84.2%)

## 📖 参考资料

- 《机器学习方法（第2版）》- 李航
- 《机器学习》- 周志华
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [NumPy Documentation](https://numpy.org/doc/)

## 🤝 贡献

这是个人学习项目，欢迎提出建议和改进意见！

## 📧 联系方式

如有问题或建议，欢迎通过 GitHub Issues 联系我。

## 📄 许可证

MIT License

---

⭐ 如果这个项目对你有帮助，欢迎 Star！