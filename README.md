# 机器学习实验 (Machine Learning Lab)

个人机器学习算法学习与实践项目

> 📖 **学习资料**：本项目基于李航老师的《机器学习方法（第2版）》进行学习，通过代码实现加深对算法原理的理解。

## 📚 项目简介

这是我的机器学习学习笔记和实验代码仓库。跟随《机器学习方法（第2版）》的章节顺序，从零开始实现各种经典的机器学习算法，将理论与实践相结合。

## 🎯 学习目标

- 理解机器学习算法的数学原理
- 从底层实现常见的机器学习算法
- 掌握 NumPy、Pandas、Matplotlib 等数据科学工具
- 培养算法调试和优化能力

## 📂 项目结构

```
machine-learning-lab/
├── linear_regression/          # 线性回归
│   └── gradient_descent.py    # 梯度下降法实现
├── perceptron/                 # 感知机
│   ├── perceptron_primal.py   # 感知机原始形式
│   └── perceptron_dual.py     # 感知机对偶形式
├── knn/                        # k近邻法
│   └── knn_basic.py           # k-d树实现
├── naive_bayes/                # 朴素贝叶斯
│   ├── naive_bayes_mle.py     # 极大似然估计
│   └── naive_bayes_est.py     # 贝叶斯估计(拉普拉斯平滑)
├── decision_tree/              # 决策树
│   ├── decision_tree_classifier.py  # 分类树(基尼指数)
│   └── decision_tree_regressor.py   # 回归树(MSE)
├── logistic_regression/        # 逻辑斯谛回归
│   ├── binomial_logistic_regression.py   # 二项逻辑斯谛回归
│   └── multinomial_logisitic_regression.py  # 多项逻辑斯谛回归
├── max_entropy/                # 最大熵模型
│   └── max_entropy_nlp_demo.py  # 中文词性标注Demo
├── svm/                        # 支持向量机
│   ├── svm_dual_linear.py     # 线性可分SVM-对偶形式
│   ├── svm_linear_softmargin.py  # 线性SVM-软间隔
│   ├── svm_sgd_linear.py      # 线性SVM-随机梯度下降
│   └── svm_kernel.py          # 非线性SVM-核函数方法
├── adaboost/                   # AdaBoost提升方法
│   ├── adaboost.py            # AdaBoost算法实现
│   ├── forward_stagewise.py   # 前向分步算法实现
│   └── gbdt_regression.py     # GBDT回归算法实现
├── hmm/                        # 隐马尔可夫模型
│   ├── hmm_generate.py        # HMM观测序列生成算法
│   ├── hmm_forward.py         # HMM前向算法（概率计算）
│   ├── hmm_backward.py        # HMM后向算法（概率计算）
│   ├── hmm_baum_welch.py      # HMM Baum-Welch算法（参数学习）
│   └── hmm_viterbi.py         # HMM Viterbi算法（状态预测）
├── crf/                        # 条件随机场
│   ├── crf_forward_backward.py  # CRF前向后向算法（推断）
│   ├── crf_viterbi.py         # CRF Viterbi算法（解码）
│   ├── crf_train.py           # CRF BFGS训练算法（学习）
│   └── crf_complete_example.py  # CRF完整示例（中文分词、NER等）
├── clustering/                 # 聚类算法
│   ├── hierarchical_clustering.py  # 层次聚类（凝聚式）
│   └── kmeans_clustering.py   # K-Means聚类
├── svd/                        # 奇异值分解
│   └── svd_decomposition.py   # SVD分解与应用
├── pca/                        # 主成分分析
│   └── pca_algorithm.py       # PCA/增量PCA/核PCA
├── em/                         # EM算法系列
│   ├── em_algorithm.py        # 标准EM算法（硬币投掷、混合伯努利）
│   ├── generalized_em.py      # 广义EM算法（GEM）
│   ├── gmm_em.py              # 高斯混合模型EM算法
│   └── variational_em.py      # 变分EM算法（VBEM）
├── pytorch/                    # PyTorch深度学习
│   └── tensor_basics.py       # 张量基础操作
├── venv/                       # Python虚拟环境
├── .gitignore                  # Git忽略文件
└── README.md                   # 项目说明
```

## 🔬 已实现的算法

### 1. 线性回归 (Linear Regression)

#### 梯度下降法 (Gradient Descent)
- **文件**: `linear_regression/gradient_descent.py`
- **功能**: 使用梯度下降算法从头实现线性回归
- **特点**:
  - 生成带高斯噪声的训练数据
  - 实现完整的梯度下降优化过程
  - 可视化拟合结果和损失函数变化
  - 对比学习参数与真实参数

**运行示例**:
```bash
python linear_regression/gradient_descent.py
```

**效果展示**:
- 训练数据: 10个样本，符合 y = 0.5x + 1.5 + 噪声
- 学习率: 0.01
- 迭代次数: 1000次
- 输出: 拟合直线对比图 + 损失曲线图

---

### 2. 感知机 (Perceptron)

感知机是二分类的线性分类模型，是神经网络和支持向量机的基础。

#### 感知机原始形式 (Primal Form)
- **文件**: `perceptron/perceptron_primal.py`
- **模型**: $f(x) = \text{sign}(w \cdot x + b)$
- **算法**: 随机梯度下降
- **特点**:
  - 直接更新权重向量 $w$ 和偏置 $b$
  - 详细输出每次迭代的更新过程
  - 可视化分类结果和分离超平面
  - 适合特征维度较低的情况

**运行示例**:
```bash
python perceptron/perceptron_primal.py
```

#### 感知机对偶形式 (Dual Form)
- **文件**: `perceptron/perceptron_dual.py`
- **模型**: $f(x) = \text{sign}(\sum_{i=1}^{N} \alpha_i y_i x_i \cdot x + b)$
- **算法**: 基于样本更新次数的对偶表示
- **特点**:
  - 通过 $\alpha$ 向量记录每个样本的更新次数
  - 预先计算 Gram 矩阵 $G_{ij} = x_i \cdot x_j$ 提高效率
  - 可以恢复原始形式的参数
  - 适合样本数量较少的情况

**运行示例**:
```bash
python perceptron/perceptron_dual.py
```

**训练数据**:
- 正样本: x₁=(3,3)ᵀ, x₂=(4,3)ᵀ
- 负样本: x₃=(1,1)ᵀ
- 学习率: η = 1

---

### 3. k近邻法 (k-Nearest Neighbors)

k近邻法是一种基本的分类与回归方法，通过找到与待分类点最近的k个训练样本来进行预测。

#### k-d树实现 (k-d Tree)
- **文件**: `knn/knn_basic.py`
- **数据结构**: k-d树（k-dimensional tree）
- **功能**: 构造平衡k-d树，高效搜索最近邻
- **特点**:
  - 实现平衡k-d树的构造算法
  - 支持最近邻搜索和k近邻搜索
  - 详细输出树的结构和搜索过程
  - 可视化训练数据、查询点和搜索路径
  - 支持自定义查询点和k值

**运行示例**:
```bash
python knn/knn_basic.py
```

**使用方法**:
```python
# 使用默认查询点 (3, 4.5)
main()

# 自定义查询点
main(query_point=[7, 5])

# 搜索k个最近邻
main(query_point=[5, 5], k=3)
```

**训练数据**:
- 6个样本点: (2,3), (5,4), (9,6), (4,7), (8,1), (7,2)
- 默认查询点: (3, 4.5)
- 构造平衡k-d树，深度优先搜索最近邻

---

### 4. 朴素贝叶斯 (Naive Bayes)

朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法，简单高效且适用于多分类问题。

#### 极大似然估计 (MLE)
- **文件**: `naive_bayes/naive_bayes_mle.py`
- **方法**: Maximum Likelihood Estimation
- **特点**:
  - 使用极大似然估计计算先验概率和条件概率
  - 详细输出所有概率计算过程
  - 适用于训练数据充足的情况

**先验概率**:
$$P(Y=c_k) = \frac{N_{c_k}}{N}$$

**条件概率**:
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl}}{N_{c_k}}$$

**运行示例**:
```bash
python naive_bayes/naive_bayes_mle.py
```

#### 贝叶斯估计 / 拉普拉斯平滑 (EST)
- **文件**: `naive_bayes/naive_bayes_est.py`
- **方法**: Bayesian Estimation with Laplace Smoothing
- **特点**:
  - 使用贝叶斯估计（加一平滑）避免概率为0
  - $\lambda=1$ 时为拉普拉斯平滑
  - 解决训练数据不足导致的概率估计问题

**先验概率**:
$$P(Y=c_k) = \frac{N_{c_k} + \lambda}{N + K \cdot \lambda}$$

**条件概率**:
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl} + \lambda}{N_{c_k} + S_j \cdot \lambda}$$

**运行示例**:
```bash
python naive_bayes/naive_bayes_est.py
```

**训练数据**:
- 特征：X^(1) ∈ {1, 2, 3}, X^(2) ∈ {S, M, L}
- 类别：Y ∈ {1, -1}
- 15个训练样本
- 测试样本：(2, S)
- **预测结果**：两种方法都预测为 Y = -1

---

### 5. 决策树 (Decision Tree)

决策树是一种基本的分类与回归方法，通过树形结构进行决策。

#### 分类树 (Classification Tree)
- **文件**: `decision_tree/decision_tree_classifier.py`
- **分裂准则**: 基尼指数 (Gini Index)
- **特点**:
  - 使用基尼指数选择最优特征和切分点
  - 递归构建决策树
  - 支持类别型特征（自动编码）
  - 输出可读的决策规则
  - 适用于分类问题

**基尼指数**:
$$\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2$$

**分裂后的基尼指数**:
$$\text{Gini}(D, A) = \frac{|D_1|}{|D|}\text{Gini}(D_1) + \frac{|D_2|}{|D|}\text{Gini}(D_2)$$

**运行示例**:
```bash
python decision_tree/decision_tree_classifier.py
```

**训练数据** - 贷款审批数据集:
- 14个样本，4个特征（年龄、有工作、有房子、信贷情况）
- 2个类别（同意贷款、拒绝贷款）
- 训练集准确率：100%
- **决策规则示例**: 
  - 有房子 → 同意贷款
  - 无房子且信贷好 → 同意贷款
  - 无房子且信贷一般 → 拒绝贷款

#### 回归树 (Regression Tree)
- **文件**: `decision_tree/decision_tree_regressor.py`
- **分裂准则**: 均方误差 (MSE - Mean Squared Error)
- **特点**:
  - 使用MSE最小化选择分裂点
  - 叶节点预测值为区域内样本均值
  - 可视化拟合曲线
  - 适用于回归问题

**均方误差**:

$$\text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \bar{y})^2$$

**分裂后的MSE**:

$$\text{MSE}_\text{split} = \frac{n_\text{left}}{n}\text{MSE}_\text{left} + \frac{n_\text{right}}{n}\text{MSE}_\text{right}$$

**运行示例**:
```bash
python decision_tree/decision_tree_regressor.py
```

**训练数据**:
- 10个样本点：(1, 4.50), (2, 4.75), ..., (10, 9.00)
- 训练集 MSE: 0.0525
- 训练集 R²: 0.9810

---

### 6. 逻辑斯谛回归 (Logistic Regression)

逻辑斯谛回归是一种广义线性模型，用于解决分类问题，特别是二分类问题。

#### 二项逻辑斯谛回归 (Binomial Logistic Regression)
- **文件**: `logistic_regression/binomial_logistic_regression.py`
- **方法**: 梯度下降法
- **特点**:
  - 使用 Sigmoid 函数将线性组合映射到 [0,1] 区间
  - 基于极大似然估计的损失函数
  - 梯度下降法优化参数
  - 可视化决策边界和训练过程
  - 适用于二分类问题

**Sigmoid 函数**:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**模型**:

$$P(Y=1|x) = \frac{1}{1 + \exp(-(w \cdot x + b))}$$

**损失函数（负对数似然）**:

$$L(w,b) = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$$

**梯度**:

$$\frac{\partial L}{\partial w} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)x_i$$

$$\frac{\partial L}{\partial b} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)$$

**运行示例**:
```bash
python logistic_regression/binomial_logistic_regression.py
```

**训练数据** - 学生考试通过预测:
- 20个样本：学习时长（0.5-5.5小时）与考试结果（通过/未通过）
- 通过人数：10人，未通过人数：10人
- 训练集准确率：80.00%
- **决策边界**: 学习时长约 2.7 小时
- **预测示例**:
  - 学习 1.0 小时 → 通过概率 7% → 未通过
  - 学习 3.0 小时 → 通过概率 61% → 通过
  - 学习 5.0 小时 → 通过概率 97% → 通过

#### 多项逻辑斯谛回归 (Multinomial Logistic Regression)
- **文件**: `logistic_regression/multinomial_logisitic_regression.py`
- **方法**: BFGS拟牛顿法
- **特点**:
  - 使用 Softmax 函数处理多分类问题
  - 基于极大似然估计
  - BFGS优化算法，收敛速度快
  - 可视化多分类决策边界
  - 适用于多分类问题（K ≥ 2）

**Softmax 函数**:

$$P(Y=k|x) = \frac{\exp(w_k \cdot x + b_k)}{\sum_{j=1}^{K}\exp(w_j \cdot x + b_j)}$$

**模型**:

对于 K 个类别，需要学习 K 组参数 $(w_1, b_1), ..., (w_K, b_K)$

**损失函数（交叉熵）**:

$$J(W,b) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K} \mathbb{1}(y_i=k) \log P(Y=k|x_i)$$

其中 $\mathbb{1}(\cdot)$ 是指示函数。

**梯度**:

$$\frac{\partial J}{\partial w_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))x_i$$

$$\frac{\partial J}{\partial b_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))$$

**运行示例**:
```bash
python logistic_regression/multinomial_logisitic_regression.py
```

**训练数据** - 鸢尾花分类:
- 48个样本：花瓣长度、花瓣宽度 → 3种鸢尾花（Setosa、Versicolor、Virginica）
- 类别分布：Setosa 16个，Versicolor 16个，Virginica 16个
- 训练集准确率：77.08%
- **特点**: 使用BFGS优化，自动计算梯度，收敛快且稳定
- **决策边界**: 可视化展示三个类别的非线性分离超平面

---

### 7. 最大熵模型 (Maximum Entropy Model)

最大熵模型是一种基于最大熵原理的分类模型，属于对数线性模型。在满足已知约束条件的前提下，选择熵最大的模型。

#### 最大熵NLP演示 - 中文词性标注 (POS Tagging)
- **文件**: `max_entropy/max_entropy_nlp_demo.py`
- **方法**: 梯度下降法
- **应用场景**: 中文词性标注（Part-of-Speech Tagging）
- **特点**:
  - 丰富的特征工程（7种特征类型）
  - 手工标注的中文训练数据
  - 支持8种常见词性标签
  - 梯度下降优化，过程可视化
  - 完整的训练、测试和预测流程

**最大熵模型**:

条件概率分布：

$$P(y|x) = \frac{1}{Z(x)}\exp\left(\sum_{i=1}^{n}w_i f_i(x,y)\right)$$

归一化因子：

$$Z(x) = \sum_{y}\exp\left(\sum_{i=1}^{n}w_i f_i(x,y)\right)$$

**与多项逻辑斯谛回归的关系**:

最大熵模型在形式上等价于多项逻辑斯谛回归：
- 都使用 Softmax 进行归一化
- 都是对数线性模型
- 区别在于特征函数的构造方式

**特征工程** - 7种特征类型:

1. **当前词特征**: `word=我`, `word=喜欢`
2. **前一个词**: `prev_word=我`, `prev_word=喜欢`
3. **后一个词**: `next_word=喜欢`, `next_word=中国`
4. **词长度**: `word_len=1`, `word_len=2`
5. **包含数字**: `has_digit=True`
6. **前缀**: `prefix_1=学`, `prefix_2=学习`
7. **后缀**: `suffix_1=习`, `suffix_2=学习`
8. **偏置**: `bias=1`（所有样本）

**词性标签** (8种):

| 标签 | 词性 | 示例 |
|:---:|------|------|
| n | 名词 (Noun) | 中国、音乐、书 |
| v | 动词 (Verb) | 爱、喜欢、学习 |
| a | 形容词 (Adjective) | 好、冷、干净 |
| d | 副词 (Adverb) | 很、非常、都 |
| p | 介词 (Preposition) | 在、从、对 |
| m | 数词 (Numeral) | 一、五、十 |
| q | 量词 (Quantifier) | 本、个、条 |
| r | 代词 (Pronoun) | 我、他、这 |

**损失函数**:

负对数似然 + L2正则化：

$$L(w) = -\sum_{(x,y)}\log P(y|x) + \lambda \|w\|^2$$

**梯度**:

$$\frac{\partial L}{\partial w_i} = \sum_{(x,y)}[P(y|x)f_i(x,y) - f_i(x,y_{\text{true}})] + 2\lambda w_i$$

**优化算法**:

使用梯度下降法：
- 学习率: 0.1
- 最大迭代: 50次
- 收敛阈值: 1e-4
- 监控权重变化和损失变化

**运行示例**:
```bash
python max_entropy/max_entropy_nlp_demo.py
```

**训练数据** - 中文句子标注:
- 训练集：21个句子，涵盖日常用语
- 测试集：5个句子
- 特征总数：252个
- 训练集准确率：100.00%
- 测试集准确率：94.44%

**标注示例**:
```
句子: 我 喜欢 音乐
标注: 我(r) 喜欢(v) 音乐(n)

句子: 这 是 一 本 书
标注: 这(r) 是(v) 一(m) 本(q) 书(n)

句子: 天气 很 冷
标注: 天气(n) 很(d) 冷(a)
```

**预测功能**:
- 对新句子进行词性标注
- 输出每个词的Top-3概率分布
- 可视化预测结果

**优势**:
- 特征灵活，易于添加新特征
- 概率输出，具有可解释性
- 适用于序列标注任务
- 训练过程透明，可监控优化进展

---

### 8. 支持向量机 (Support Vector Machine)

支持向量机是一种二分类模型，其基本思想是在特征空间中找到间隔最大的分离超平面。

#### 线性可分支持向量机 - 对偶形式 (Linear SVM - Dual Form)
- **文件**: `svm/svm_dual_linear.py`
- **方法**: 拉格朗日对偶 + 二次规划（SLSQP）
- **特点**:
  - 将原始问题转化为对偶问题求解
  - 使用二次规划求解最优拉格朗日乘子
  - 自动识别支持向量（α > 0 的样本）
  - 计算最大间隔分离超平面
  - 可视化决策边界、间隔边界和支持向量

**原始问题**:

$$\min_{w,b} \frac{1}{2}\|w\|^2$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1, \quad i=1,2,...,N$$

**对偶问题**:

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad \alpha_i \geq 0$$

**参数恢复**:

权重向量：

$$w^* = \sum_{i=1}^{N}\alpha_i^* y_i x_i$$

偏置（选择任一支持向量 $x_j$，满足 $\alpha_j^* > 0$）：

$$b^* = y_j - \sum_{i=1}^{N}\alpha_i^* y_i (x_i \cdot x_j)$$

**分离超平面**:

$$w^* \cdot x + b^* = 0$$

**决策函数**:

$$f(x) = \text{sign}(w^* \cdot x + b^*)$$

**分类间隔**:

$$\text{margin} = \frac{2}{\|w^*\|}$$

**运行示例**:
```bash
python svm/svm_dual_linear.py
```

**训练数据**:
- 正例：x₁=(3,3), x₂=(4,3)  标签 y=+1
- 负例：x₃=(1,1)            标签 y=-1
- 样本数：3个

**训练结果**:
- **拉格朗日乘子**：α₁=0.25, α₂=0, α₃=0.25
- **支持向量**：x₁=(3,3) 和 x₃=(1,1)（2个）
- **权重向量**：w=(0.5, 0.5)
- **偏置**：b=-2.0
- **分离超平面**：x₁ + x₂ = 4
- **分类间隔**：≈ 2.828
- **训练准确率**：100%

**关键观察**:
- x₂=(4,3) 的 α₂=0，不是支持向量（在间隔边界之外）
- 只有位于间隔边界上的样本成为支持向量（f(x)=±1）
- 支持向量决定了分离超平面，其他样本可以移除而不影响结果

**可视化**:
- 展示决策边界（黑色实线）
- 展示间隔边界（灰色虚线，f(x)=±1）
- 标记支持向量（绿色圆圈）
- 显示决策区域（彩色背景）

**优势**:
- 理论基础扎实（结构风险最小化）
- 最大间隔准则，泛化能力强
- 只依赖支持向量，模型稀疏
- 对偶形式便于引入核函数

---

#### 线性支持向量机 - 软间隔 (Linear SVM - Soft Margin)
- **文件**: `svm/svm_linear_softmargin.py`
- **方法**: 拉格朗日对偶 + 松弛变量 + 二次规划（SLSQP）
- **特点**:
  - 引入松弛变量处理线性不可分数据
  - 惩罚参数C控制间隔最大化与误分类的权衡
  - 区分边界支持向量和内部支持向量
  - 对噪声和异常点具有鲁棒性
  - 适用于含噪声或类别重叠的数据

**原始问题**:

$$\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{N}\xi_i$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i=1,2,...,N$$

其中 $\xi_i$ 是松弛变量，允许样本不满足硬间隔约束。

**对偶问题**:

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C, \quad i=1,2,...,N$$

**关键区别**：约束条件从 $\alpha_i \geq 0$ 变为 $0 \leq \alpha_i \leq C$（箱约束）

**参数恢复**:

权重向量和偏置的计算与硬间隔相同，但使用边界支持向量（$0 < \alpha_i < C$）计算偏置更稳定。

**支持向量分类**:

1. **非支持向量** (α_i = 0)：
   - 在间隔边界外，正确分类
   - $y_i(w \cdot x_i + b) > 1$

2. **边界支持向量** (0 < α_i < C)：
   - 在间隔边界上，ξ_i = 0
   - $y_i(w \cdot x_i + b) = 1$

3. **内部支持向量** (α_i = C)：
   - 在间隔内或误分类，ξ_i > 0
   - $y_i(w \cdot x_i + b) < 1$

**惩罚参数C的作用**:

- **C很大**：对误分类惩罚大，接近硬间隔，间隔小，可能过拟合
- **C很小**：允许更多误分类，间隔大，模型简单，可能欠拟合
- **C适中**：平衡间隔最大化和误分类，泛化能力好

**运行示例**:
```bash
python svm/svm_linear_softmargin.py
```

**示例1 - 线性可分数据** (C=100.0):
- 数据：正例 x₁=(3,3), x₂=(4,3)；负例 x₃=(1,1)
- 支持向量：2个边界支持向量，0个内部支持向量
- 准确率：100%
- 说明：C很大时，结果接近硬间隔SVM

**示例2 - 含噪声数据** (比较不同C值):
- 数据：6个样本，包含1个噪声点（负类标签但在正类区域）
- **C=0.1**：容忍噪声，间隔大，噪声点被错分
- **C=1.0**：平衡策略，识别出2个内部支持向量（包括噪声点）
- **C=10.0**：强制正确分类，间隔变窄，完美拟合数据
- 准确率：C=0.1/1.0为83.33%，C=10.0为100%

**示例3 - 类别重叠数据** (20个样本):
- 两类数据分布有重叠
- **C=0.1**：大间隔，10个支持向量（8个内部，2个边界）
- **C=1.0**：中等间隔，4个支持向量（全部内部）
- **C=100.0**：小间隔，3个支持向量（全部边界）
- 全部准确率：100%

**可视化**:
- 绿色圆圈：边界支持向量（在间隔边界上）
- 橙色叉号：内部支持向量（在间隔内或误分类）
- 黑色实线：分离超平面
- 灰色虚线：间隔边界
- 彩色背景：决策区域

**优势**:
- 可处理线性不可分数据
- 对噪声和异常点鲁棒
- 通过C参数控制模型复杂度
- 自动识别不同类型的支持向量
- 在间隔最大化和分类错误之间找到平衡

**应用场景**:
- 含噪声的分类问题
- 类别有部分重叠的数据
- 需要控制过拟合风险的场景
- 真实世界的线性分类任务

---

#### 线性支持向量机 - 随机梯度下降 (Linear SVM - SGD)
- **文件**: `svm/svm_sgd_linear.py`
- **方法**: 随机梯度下降 (Stochastic Gradient Descent)
- **特点**:
  - 使用Hinge损失函数进行在线学习
  - 每次迭代只使用单个样本更新参数
  - 计算效率高，适合大规模数据
  - 支持软间隔（通过参数C控制）
  - 可视化训练过程和损失曲线

**Hinge损失函数**:

$$L(w,b) = \max(0, 1 - y_i(w \cdot x_i + b))$$

**完整目标函数**:

$$J(w,b) = \frac{1}{2}\|w\|^2 + C \cdot \sum_{i=1}^{N} \max(0, 1 - y_i(w \cdot x_i + b))$$

第一项是L2正则化项，第二项是Hinge损失之和。

**SGD更新规则**:

对于每个样本 $(x_i, y_i)$：

如果 $y_i(w \cdot x_i + b) < 1$（样本在间隔内或误分类）：

$$w \leftarrow w - \eta(w - C \cdot y_i \cdot x_i)$$

$$b \leftarrow b - \eta(-C \cdot y_i)$$

否则（样本在间隔外，正确分类）：

$$w \leftarrow w - \eta \cdot w$$

$$b$ 不更新$$

其中 $\eta$ 是学习率。

**算法特点**:

1. **在线学习**: 每次只用一个样本，内存占用小
2. **随机性**: 每轮迭代随机打乱样本顺序
3. **计算效率**: 
   - 对偶形式: $O(N^2)$ 的二次规划
   - SGD: $O(N \cdot T)$，T是迭代轮数
4. **收敛性**: 学习率需要仔细调整

**学习率策略**:

- **固定学习率**: $\eta = \text{const}$
  - 简单但可能不收敛或收敛慢
  
- **衰减学习率**: $\eta_t = \frac{\eta_0}{1 + \lambda t}$
  - 开始大步前进，后期精细调整

**与其他SVM方法对比**:

| 特性 | 对偶形式 (QP) | 软间隔 (QP) | SGD |
|------|--------------|-------------|-----|
| 求解方法 | 二次规划 | 二次规划 | 随机梯度下降 |
| 时间复杂度 | $O(N^2)$ ~ $O(N^3)$ | $O(N^2)$ ~ $O(N^3)$ | $O(N \cdot T)$ |
| 空间复杂度 | $O(N^2)$ | $O(N^2)$ | $O(d)$ |
| 适用规模 | 小到中等 | 小到中等 | 大规模 |
| 精确度 | 精确解 | 精确解 | 近似解 |
| 在线学习 | 不支持 | 不支持 | 支持 |
| 核函数 | 易扩展 | 易扩展 | 需要技巧 |

其中 $N$ 是样本数，$d$ 是特征维度，$T$ 是迭代轮数。

**运行示例**:
```bash
python svm/svm_sgd_linear.py
```

**训练数据** - 14个样本的二分类:
- 正例：7个样本（如 (5,9), (3,12), (2,10) 等）
- 负例：7个样本（如 (1,1), (3,-2), (-1,4) 等）
- 特点：类别有轻微重叠，适合测试软间隔

**训练结果** (C=0.5, η=0.01, 1000轮):
- **训练集准确率**: 85.71% (12/14)
- **支持向量数**: 11个
- **权重向量**: w = (0.109, 0.139)
- **偏置**: b = -1.000
- **分类间隔**: 11.32
- **最终损失**: 0.253

**参数比较实验**:

不同C值的效果：

| C值 | 准确率 | 支持向量数 | ||w|| | 最终损失 |
|-----|--------|-----------|---------|---------|
| 0.1 | 85.71% | 10 | 0.160 | 0.057 |
| 1.0 | 85.71% | 9 | 0.255 | 0.486 |
| 10.0 | 42.86% | 10 | 0.609 | 39.51 |

**观察**:
- **C=0.1**: 大间隔，对误分类容忍度高，损失小
- **C=1.0**: 平衡设置，性能稳定
- **C=10.0**: 过度拟合，学习率过大导致不稳定

**可视化内容**:
- 决策边界和间隔边界
- 训练样本的分类情况
- 训练损失随迭代的变化曲线
- 参数信息（C, 学习率, 训练轮数等）

**优势**:
- 计算速度快，适合大规模数据
- 内存占用小，只存储参数不存储样本
- 支持在线学习和增量学习
- 实现简单，易于理解

**局限性**:
- 需要仔细调整学习率
- 收敛到近似解，不如QP精确
- 学习率过大可能不稳定
- 不易直接扩展到核SVM

**应用场景**:
- 大规模文本分类
- 在线广告点击率预测
- 实时数据流分类
- 内存受限的嵌入式系统
- 需要增量更新的场景

**实现技巧**:
1. **样本归一化**: 对特征进行标准化，加速收敛
2. **学习率调整**: 可以使用学习率衰减策略
3. **提前停止**: 监控验证集损失，防止过拟合
4. **小批量SGD**: 介于批量梯度下降和SGD之间
5. **正则化强度**: C值通过交叉验证选择

**与梯度下降的区别**:

- **批量梯度下降 (BGD)**: 每次用全部样本计算梯度
  - 准确但慢，每轮 $O(N)$
  
- **随机梯度下降 (SGD)**: 每次用一个样本
  - 快速但有噪声，可以跳出局部最优
  
- **小批量SGD (Mini-batch)**: 每次用一小批样本
  - 平衡速度和稳定性

---

#### 非线性支持向量机 - 核函数方法 (Nonlinear SVM - Kernel Methods)
- **文件**: `svm/svm_kernel.py`
- **方法**: 核技巧 + 对偶问题求解
- **特点**:
  - 支持4种核函数（线性、多项式、RBF、Sigmoid）
  - 通过核技巧避免显式高维映射
  - 可处理非线性分类问题
  - 完整的核矩阵计算
  - 自动参数调优（auto gamma）
  - 4个经典演示案例

**核函数类型**:

1. **线性核 (Linear Kernel)**:
   $$K(x, z) = x \cdot z$$

2. **多项式核 (Polynomial Kernel)**:
   $$K(x, z) = (\gamma \cdot x \cdot z + r)^d$$
   
   其中 $d$ 是多项式次数，$\gamma$ 是核系数，$r$ 是独立项。

3. **高斯RBF核 (Radial Basis Function)**:
   $$K(x, z) = \exp(-\gamma \|x-z\|^2)$$
   
   最常用的核函数，$\gamma$ 控制高斯分布的宽度。

4. **Sigmoid核 (Sigmoid Kernel)**:
   $$K(x, z) = \tanh(\gamma \cdot x \cdot z + r)$$

**核技巧 (Kernel Trick)**:

核技巧的关键在于：可以在原始空间通过核函数直接计算高维特征空间中的内积，而无需显式地进行映射。

设映射函数为 $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^D$（$D >> d$），则：

$$K(x, z) = \phi(x) \cdot \phi(z)$$

在对偶问题中，决策函数可以表示为：

$$f(x) = \text{sign}\left(\sum_{i=1}^{N}\alpha_i y_i K(x_i, x) + b\right)$$

**对偶问题（核形式）**:

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j K(x_i, x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad 0 \leq \alpha_i \leq C$$

注意：核矩阵 $K_{ij} = K(x_i, x_j)$ 替代了原始空间的内积 $x_i \cdot x_j$。

**参数选择**:

- **C (惩罚参数)**: 控制间隔最大化和误分类的权衡
  - C大：严格分类，可能过拟合
  - C小：允许更多误分类，泛化能力可能更好

- **γ (Gamma)**: RBF、多项式、Sigmoid核的参数
  - γ大：决策边界复杂，可能过拟合
  - γ小：决策边界平滑，可能欠拟合
  - 默认：`gamma='auto'` 时为 $1/n\_features$

- **d (Degree)**: 多项式核的次数
  - 通常取2-5
  - 过大容易过拟合

**运行示例**:
```bash
python svm/svm_kernel.py
```

**演示案例**:

**案例1: XOR问题** (8个样本)
- 描述：经典的非线性分类问题，线性不可分
- 数据：四个角点及其邻近点
- 核函数：RBF核 (γ=1.0, C=1.0)
- 结果：**100.00%准确率**，8个支持向量（全部为边界支持向量）
- 特点：完美解决了线性SVM无法处理的XOR问题

**案例2: 同心圆问题** (40个样本)
- 描述：内圆为正类，外环为负类
- 数据：内圆半径0-1.5，外环半径3-4.5
- 核函数：RBF核 (γ=0.5, C=10.0)
- 结果：**100.00%准确率**，20个支持向量（50%）
- 特点：展示RBF核处理圆形分布数据的能力

**案例3: 抛物线分类** (30个样本)
- 描述：数据分布呈抛物线形状
- 数据：正类在抛物线上方，负类在下方
- 核函数：多项式核 (d=2, γ=1.0, r=1.0, C=1.0)
- 结果：**100.00%准确率**，5个支持向量（16.67%）
- 特点：二次多项式核完美拟合二次边界

**案例4: 核函数对比** (60个样本)
- 描述：在同一混合数据集上测试不同核函数
- 数据：正类为两个簇，负类在中间
- 测试核函数：
  - **线性核**: 51.67%准确率，58个支持向量（无法分离）
  - **多项式核 (d=2)**: 100.00%准确率，4个支持向量
  - **RBF核 (γ=0.5)**: 100.00%准确率，17个支持向量
  - **RBF核 (γ=2.0)**: 100.00%准确率，30个支持向量
- 观察：γ越大，决策边界越复杂，支持向量越多

**核函数选择指南**:

| 核函数 | 适用场景 | 优点 | 缺点 |
|--------|---------|------|------|
| **线性核** | 线性可分、高维稀疏数据 | 快速、参数少、可解释 | 只能处理线性问题 |
| **RBF核** | 通用、未知分布 | 强大、灵活、常用 | 参数敏感、易过拟合 |
| **多项式核** | 图像、文本特征 | 可控次数、全局性 | 参数多、数值不稳定 |
| **Sigmoid核** | 神经网络替代 | 类似神经网络 | 不总是正定核 |

**推荐策略**:
1. 首先尝试RBF核（最通用）
2. 如果数据线性可分，使用线性核
3. 如果需要特定次数的交互，使用多项式核
4. 通过交叉验证选择最佳参数

**可视化内容**:
- 非线性决策边界（RBF核可以是圆形、椭圆等）
- 间隔边界 (f(x) = ±1)
- 支持向量标记（绿色圆圈）
- 决策区域着色
- 参数信息展示

**计算复杂度**:

- **训练时间**: $O(N^2 \sim N^3)$（取决于QP求解器）
- **预测时间**: $O(N_{sv} \cdot N_{test})$
  - $N_{sv}$ 是支持向量数量
- **空间复杂度**: $O(N^2)$（存储核矩阵）

**优势**:
- 可以处理任意非线性问题
- 核技巧避免显式高维计算
- 理论基础完善（VC维理论）
- 支持向量稀疏性
- 可以自定义核函数

**局限性**:
- 大规模数据计算开销大
- 参数选择需要经验或交叉验证
- 核矩阵存储需要大量内存
- 多分类需要组合策略（OvO, OvR）

**应用场景**:
- 图像分类（手写数字识别）
- 文本分类（情感分析）
- 生物信息学（蛋白质分类）
- 人脸识别
- 异常检测

**实现细节**:

1. **核矩阵计算**:
   - 对称矩阵，只需计算上三角
   - 可以批量计算提高效率

2. **数值稳定性**:
   - 使用阈值判断支持向量（α > 1e-5）
   - 区分边界和内部支持向量
   - 偏置b使用多个支持向量求平均

3. **参数初始化**:
   - α初始化为零向量
   - gamma自动设置为1/n_features
   - 使用SLSQP求解器

**与线性SVM的对比**:

| 特性 | 线性SVM | 核SVM |
|------|---------|-------|
| 决策边界 | 直线/平面 | 任意曲线/曲面 |
| 适用问题 | 线性可分 | 非线性可分 |
| 计算复杂度 | 低 | 高 |
| 可解释性 | 强 | 弱 |
| 过拟合风险 | 低 | 中等 |
| 参数数量 | 1 (C) | 2-4 (C, γ, d, r) |

**理论基础**:

Mercer定理：一个对称函数 $K(x,z)$ 是有效核函数的充要条件是，对任意数据集，核矩阵K是半正定的。

常用核函数都满足Mercer条件，保证对偶问题有唯一解。

---

### 9. AdaBoost 提升方法 (Adaptive Boosting)

AdaBoost 是一种迭代的集成学习算法，通过组合多个弱分类器构成强分类器。算法的核心思想是调整训练样本的权重分布，使得后续的弱分类器更关注被前面分类器误分的样本。

#### AdaBoost算法实现
- **文件**: `adaboost/adaboost.py`
- **弱分类器**: 决策树桩 (Decision Stump) - 单层决策树
- **方法**: 迭代加权学习
- **特点**:
  - 自适应调整样本权重
  - 弱分类器加权组合
  - 详细的训练过程输出
  - 可视化每个弱分类器及最终强分类器
  - 展示权重分布和错误率变化

**算法流程**:

**输入**:
- 训练数据集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$
- 弱学习算法
- 迭代次数 $M$

**输出**:
- 最终强分类器 $f(x)$

**步骤**:

1. **初始化样本权重分布**:
   
   $$D_1 = (w_{11}, w_{12}, ..., w_{1N}), \quad w_{1i} = \frac{1}{N}, \quad i=1,2,...,N$$
   
   每个样本的初始权重相等。

2. **对 m = 1, 2, ..., M 进行迭代**:

   a) **使用权重分布 $D_m$ 训练弱分类器**:
   
   $$G_m(x): \mathcal{X} \rightarrow \{-1, +1\}$$
   
   弱分类器在加权训练集上学习。

   b) **计算弱分类器的加权错误率**:
   
   $$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^{N} w_{mi} \mathbb{I}(G_m(x_i) \neq y_i)$$
   
   $\mathbb{I}(\cdot)$ 是指示函数，预测错误时为1。

   c) **计算弱分类器的权重**:
   
   $$\alpha_m = \frac{1}{2} \ln \frac{1 - e_m}{e_m}$$
   
   错误率越低，权重越大。当 $e_m < 0.5$ 时，$\alpha_m > 0$。

   d) **更新样本权重分布**:
   
   $$D_{m+1} = (w_{m+1,1}, w_{m+1,2}, ..., w_{m+1,N})$$
   
   $$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i))$$
   
   其中 $Z_m$ 是规范化因子：
   
   $$Z_m = \sum_{i=1}^{N} w_{mi} \exp(-\alpha_m y_i G_m(x_i))$$
   
   使得 $\sum_{i=1}^{N} w_{m+1,i} = 1$。
   
   **权重更新规则**:
   - 如果 $G_m(x_i) = y_i$（分类正确）：$\exp(-\alpha_m y_i G_m(x_i)) = e^{-\alpha_m} < 1$，权重**减小**
   - 如果 $G_m(x_i) \neq y_i$（分类错误）：$\exp(-\alpha_m y_i G_m(x_i)) = e^{\alpha_m} > 1$，权重**增大**

3. **构建最终强分类器**:
   
   $$f(x) = \text{sign}\left(\sum_{m=1}^{M} \alpha_m G_m(x)\right)$$
   
   即所有弱分类器的加权投票。

**决策树桩 (Decision Stump)**:

决策树桩是最简单的决策树，只有一个分裂节点，形式为：

$$
G(x) = 
\begin{cases}
+1, & \text{if } p \cdot x < p \cdot \text{threshold} \\
-1, & \text{otherwise}
\end{cases}
$$

其中：
- $p \in \{-1, +1\}$ 是极性/方向参数
- threshold 是分裂阈值

**弱分类器的选择**:

在每轮迭代中，遍历所有可能的阈值和方向，选择加权错误率最小的：

$$(\text{threshold}^*, p^*) = \arg\min_{\text{threshold}, p} \sum_{i=1}^{N} w_{mi} \mathbb{I}(G(x_i; \text{threshold}, p) \neq y_i)$$

**AdaBoost的关键性质**:

1. **训练误差指数下降**:
   
   AdaBoost的训练误差上界为：
   
   $$\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(f(x_i) \neq y_i) \leq \prod_{m=1}^{M} \sqrt{1 - 4\gamma_m^2}$$
   
   其中 $\gamma_m = 0.5 - e_m$，只要每个弱分类器的错误率 $e_m < 0.5$，训练误差会指数下降。

2. **无需提前知道弱分类器的错误率上界**:
   
   算法自适应调整，不需要事先知道弱分类器的性能。

3. **对噪声敏感**:
   
   如果数据有噪声（标签错误），AdaBoost可能会过度关注噪声点，导致过拟合。

**运行示例**:
```bash
python adaboost/adaboost.py
```

**训练数据** - 10个一维样本:
- 样本: x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
- 标签: y = [1, 1, 1, -1, -1, -1, 1, 1, 1, -1]
- 弱分类器数量: M = 3

**训练结果**:

**第1轮迭代** (m=1):
- 初始权重: $D_1 = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]$（均匀分布）
- 最优弱分类器: $G_1(x)$
  - 阈值: threshold = -0.5
  - 方向: direction = +1
  - 分类规则: 如果 x < -0.5 则预测 +1，否则预测 -1
- 加权错误率: $e_1 = 0.4000$
- 分类器权重: $\alpha_1 = 0.2027$
- 更新后权重: 误分类样本（索引0,1,2,6,7,8）权重增加

**第2轮迭代** (m=2):
- 当前权重: $D_2$ 更关注第1轮误分的样本
- 最优弱分类器: $G_2(x)$
  - 阈值: threshold = 5.5
  - 方向: direction = +1
  - 分类规则: 如果 x < 5.5 则预测 +1，否则预测 -1
- 加权错误率: $e_2 = 0.3750$
- 分类器权重: $\alpha_2 = 0.2554$（错误率更低，权重更大）
- 更新后权重: 继续调整权重分布

**第3轮迭代** (m=3):
- 当前权重: $D_3$ 根据前两轮结果调整
- 最优弱分类器: $G_3(x)$
  - 阈值: threshold = -0.5
  - 方向: direction = +1
  - 分类规则: 如果 x < -0.5 则预测 +1，否则预测 -1
- 加权错误率: $e_3 = 0.4667$（较高，说明难分样本）
- 分类器权重: $\alpha_3 = 0.0668$（错误率高，权重小）

**最终强分类器**:

$$f(x) = \text{sign}(0.2027 \cdot G_1(x) + 0.2554 \cdot G_2(x) + 0.0668 \cdot G_3(x))$$

**训练集性能**:
- 训练准确率: **60.00%** (6/10)
- 正确分类: 样本 0, 1, 2, 6, 7, 8
- 误分类: 样本 3, 4, 5, 9

**预测详情**:

| 样本 | 真实标签 | 预测标签 | 结果 | 加权得分 |
|:---:|:-------:|:-------:|:----:|:--------:|
| 0 | +1 | +1 | ✓ | +0.4249 |
| 1 | +1 | +1 | ✓ | +0.4249 |
| 2 | +1 | +1 | ✓ | +0.4249 |
| 3 | -1 | +1 | ✗ | +0.4249 |
| 4 | -1 | +1 | ✗ | +0.4249 |
| 5 | -1 | +1 | ✗ | +0.4249 |
| 6 | +1 | +1 | ✓ | +0.0668 |
| 7 | +1 | +1 | ✓ | +0.0668 |
| 8 | +1 | +1 | ✓ | +0.0668 |
| 9 | -1 | -1 | ✓ | -0.3222 |

**可视化内容**:
1. **弱分类器展示**（子图1-3）:
   - 每个弱分类器的决策边界（红色虚线）
   - 分类区域着色（蓝色/红色）
   - 样本点大小表示权重
   - 显示阈值和错误率信息

2. **最终强分类器**（子图4）:
   - 组合后的决策边界
   - 所有弱分类器的加权投票结果
   - 最终分类区域

3. **权重演化**（子图5）:
   - 每轮迭代后的样本权重分布
   - 展示算法如何逐渐关注难分样本

4. **错误率变化**（子图6）:
   - 每个弱分类器的加权错误率 $e_m$
   - 训练准确率随迭代的变化

**AdaBoost vs 其他集成方法**:

| 特性 | AdaBoost | Bagging | Random Forest |
|------|----------|---------|---------------|
| 训练方式 | 串行（顺序） | 并行 | 并行 |
| 样本权重 | 自适应调整 | Bootstrap采样 | Bootstrap采样 |
| 弱学习器 | 任意（常用决策树桩） | 任意（常用深决策树） | 决策树 |
| 减少误差类型 | 偏差+方差 | 主要是方差 | 主要是方差 |
| 对噪声敏感度 | 高 | 低 | 低 |
| 可解释性 | 中等 | 低 | 低 |

**优点**:
- 精度高，可以将弱分类器提升为强分类器
- 不容易过拟合（理论上）
- 不需要特征工程
- 可以识别重要样本（通过权重）
- 可以处理不平衡数据

**缺点**:
- 对噪声和异常值敏感
- 训练时间较长（串行）
- 不适合实时应用
- 参数调优（弱分类器数量M）需要经验

**应用场景**:
- 人脸检测（Viola-Jones算法）
- 文本分类
- 特征选择
- 排序问题（如搜索引擎）
- 任何需要高精度的分类任务

**实现技巧**:
1. **弱分类器选择**: 决策树桩简单高效，防止单个分类器过拟合
2. **提前停止**: 监控验证集性能，防止过拟合
3. **权重平滑**: 避免权重过度集中在少数样本上
4. **异常值处理**: 预处理阶段去除或降低异常值影响

**理论分析**:

**前向分步算法**:

AdaBoost是前向分步加法模型的特例。设基函数为 $G_m(x)$，则加法模型为：

$$f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)$$

前向分步算法在每一步求解：

$$(\alpha_m, G_m) = \arg\min_{\alpha, G} \sum_{i=1}^{N} L\left(y_i, f_{m-1}(x_i) + \alpha G(x_i)\right)$$

其中 $f_{m-1}(x)$ 是前 $m-1$ 轮的模型。

**损失函数**:

AdaBoost使用指数损失函数：

$$L(y, f(x)) = \exp(-y f(x))$$

可以证明，使用指数损失的前向分步算法等价于AdaBoost算法。

**泛化误差界**:

AdaBoost的泛化误差可以用VC维理论或Margin理论来分析，在适当条件下具有较好的泛化能力。

---

#### 前向分步算法实现
- **文件**: `adaboost/forward_stagewise.py`
- **损失函数**: 指数损失、平方损失
- **方法**: 前向分步加法模型
- **特点**:
  - 通用的加法模型学习框架
  - 支持多种损失函数
  - 展示AdaBoost作为前向分步算法的特例
  - 对比不同损失函数的效果
  - 完整的理论对应实现

**前向分步算法原理**:

前向分步算法是一种通用的加法模型学习算法，AdaBoost是其使用指数损失函数的特例。

**加法模型**:

$$f(x) = \sum_{m=1}^{M} \beta_m b(x; \gamma_m)$$

其中：
- $b(x; \gamma_m)$ 是基函数（弱学习器）
- $\gamma_m$ 是基函数的参数
- $\beta_m$ 是基函数的系数

**算法步骤**:

1. **初始化**: $f_0(x) = 0$

2. **对 m = 1, 2, ..., M**:
   
   a) **极小化损失函数**:
   
   $$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + \beta \cdot b(x_i; \gamma))$$
   
   b) **更新模型**:
   
   $$f_m(x) = f_{m-1}(x) + \beta_m \cdot b(x; \gamma_m)$$

3. **输出最终模型**: $f(x) = f_M(x)$

**关键特点**:
- **前向**: 从前向后逐步学习，每次只优化一个基函数
- **分步**: 简化优化问题，避免一次性优化所有参数
- **通用**: 可以使用不同的损失函数

**两种损失函数对比**:

**1. 指数损失函数** (Exponential Loss):

$$L(y, f(x)) = \exp(-y f(x))$$

**性质**:
- AdaBoost使用的损失函数
- 样本权重为 $w_i = \exp(-y_i f(x_i))$
- 对误分类样本惩罚呈指数增长
- 对噪声敏感

**2. 平方损失函数** (Squared Loss):

$$L(y, f(x)) = (y - f(x))^2$$

**性质**:
- 类似梯度提升（GBDT）
- 通过拟合残差学习
- 残差 $r_i = y_i - f_{m-1}(x_i)$
- 对噪声相对鲁棒

**前向分步算法与AdaBoost的关系**:

**定理**: 前向分步算法使用指数损失函数时，等价于AdaBoost算法。

**证明思路**:

在第m步，极小化：

$$\sum_{i=1}^{N} \exp(-y_i (f_{m-1}(x_i) + \alpha G(x_i)))$$

设 $w_{mi} = \exp(-y_i f_{m-1}(x_i))$，则：

$$\sum_{i=1}^{N} w_{mi} \exp(-y_i \alpha G(x_i))$$

分离正确和错误分类的样本：

$$= \sum_{y_i = G(x_i)} w_{mi} e^{-\alpha} + \sum_{y_i \neq G(x_i)} w_{mi} e^{\alpha}$$

$$= e^{-\alpha} \sum_{i=1}^{N} w_{mi} + (e^{\alpha} - e^{-\alpha}) \sum_{y_i \neq G(x_i)} w_{mi}$$

对 $\alpha$ 求导并令其为0，得到：

$$\alpha^* = \frac{1}{2} \ln \frac{1 - e}{e}$$

其中 $e = \frac{\sum_{y_i \neq G(x_i)} w_{mi}}{\sum_{i=1}^{N} w_{mi}}$ 是加权错误率。

这正是AdaBoost中 $\alpha_m$ 的计算公式！

**运行示例**:
```bash
python adaboost/forward_stagewise.py
```

**演示1: 指数损失（等价于AdaBoost）**:
- 训练数据: 10个样本
- 弱分类器数量: 3
- 最终准确率: **100.00%** (10/10)
- 强分类器: $f(x) = \text{sign}(0.4236 \cdot G_1(x) + 0.6496 \cdot G_2(x) + 0.7520 \cdot G_3(x))$

**训练过程**:
- **第1轮**: 均匀权重 → $e_1=0.3$, $\alpha_1=0.4236$
- **第2轮**: 调整权重 → $e_2=0.2143$, $\alpha_2=0.6496$ 
- **第3轮**: 继续调整 → $e_3=0.1818$, $\alpha_3=0.7520$
- 训练误差从30%降至0%

**演示2: 平方损失**:
- 训练数据: 10个样本
- 弱分类器数量: 5
- 学习率: 0.5
- 最终准确率: **100.00%** (10/10)

**训练过程**:
- 每轮拟合当前残差
- 逐步减小预测误差
- 最终完美拟合训练数据

**演示3: 两种损失函数对比**:
- 在相同数据集上对比指数损失和平方损失
- 可视化展示决策边界的差异
- 分析不同损失函数的适用场景

**算法对比**:

| 特性 | 指数损失 | 平方损失 |
|------|---------|---------|
| **等价于** | AdaBoost | GBDT（梯度提升） |
| **更新策略** | 权重调整 | 残差拟合 |
| **收敛速度** | 快 | 较慢 |
| **对噪声** | 敏感 | 较鲁棒 |
| **适用场景** | 二分类 | 分类/回归 |
| **计算复杂度** | 低 | 低 |

**可视化内容**:
1. **每个弱分类器**: 显示阈值、分类区域、样本权重
2. **最终强分类器**: 展示所有弱分类器的组合效果
3. **训练误差曲线**: 显示误差随迭代的变化
4. **对比图**: 直观对比两种损失函数的决策边界

**理论意义**:
- 揭示AdaBoost的本质：指数损失下的前向分步算法
- 统一框架：可扩展到其他损失函数（如Logistic损失）
- 为理解集成学习提供深刻洞察

**实践价值**:
- 通用框架易于扩展
- 支持自定义损失函数
- 适合研究和教学

---

#### GBDT回归算法实现
- **文件**: `adaboost/gbdt_regression.py`
- **全称**: Gradient Boosting Decision Tree - Regression
- **方法**: 梯度提升框架 + 回归树
- **特点**:
  - 拟合损失函数的负梯度（残差）
  - 使用回归树作为弱学习器
  - 学习率控制每棵树的贡献
  - 详细的训练过程可视化
  - 展示每轮迭代的拟合效果

**GBDT原理**:

GBDT（梯度提升决策树）是一种强大的集成学习算法，通过梯度提升框架组合多个决策树。

**加法模型**:

$$f(x) = f_0(x) + \sum_{m=1}^{M} \nu \cdot T_m(x)$$

其中：
- $f_0(x)$ 是初始模型（通常是常数）
- $T_m(x)$ 是第m棵回归树
- $\nu \in (0, 1]$ 是学习率（收缩参数）
- $M$ 是树的总数

**算法步骤**:

1. **初始化模型**:
   
   $$f_0(x) = \arg\min_c \sum_{i=1}^{N} L(y_i, c)$$
   
   对于平方损失函数，$f_0(x) = \text{mean}(y)$

2. **对 m = 1, 2, ..., M 进行迭代**:
   
   a) **计算负梯度（残差）**:
   
   $$r_{mi} = -\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\bigg|_{f=f_{m-1}}$$
   
   对于平方损失 $L(y, f) = \frac{1}{2}(y - f)^2$，负梯度为：
   
   $$r_{mi} = y_i - f_{m-1}(x_i)$$
   
   即**残差**。
   
   b) **拟合回归树到残差**:
   
   训练回归树 $T_m(x)$ 使其预测残差 $r_m$
   
   c) **更新模型**:
   
   $$f_m(x) = f_{m-1}(x) + \nu \cdot T_m(x)$$

3. **输出最终模型**: $f(x) = f_M(x)$

**平方损失函数**:

$$L(y, f(x)) = \frac{1}{2}(y - f(x))^2$$

**梯度**:

$$\frac{\partial L}{\partial f} = -(y - f) = f - y$$

**负梯度（残差）**:

$$-\frac{\partial L}{\partial f} = y - f$$

这就是为什么GBDT回归每轮都在拟合残差！

**回归树作为弱学习器**:

- **分裂准则**: 最小化均方误差（MSE）
- **叶节点值**: 区域内样本的均值
- **树深度**: 通常使用浅树（深度1-3），防止过拟合
- **分段常数函数**: 回归树将输入空间划分为若干区域，每个区域预测一个常数

**学习率的作用**:

学习率 $\nu$ 控制每棵树对最终模型的贡献：

- **ν较大**（如0.5-1.0）：
  - 快速收敛
  - 可能过拟合
  - 需要较少的树

- **ν较小**（如0.01-0.1）：
  - 缓慢收敛
  - 更好的泛化能力
  - 需要更多的树
  - 更稳定的训练

**运行示例**:
```bash
python adaboost/gbdt_regression.py
```

**训练数据** - 10个样本:
```
X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
y = [5.56, 5.70, 5.91, 6.40, 6.80, 7.05, 8.90, 8.70, 9.00, 9.05]
```

**训练配置**:
- 树的数量: M = 6
- 学习率: ν = 0.1
- 树的最大深度: 2
- 损失函数: 平方损失

**训练过程**:

**初始化**:
- $f_0(x) = 7.3070$（所有样本的均值）
- 初始MSE: 1.9114

**迭代过程**:

| 轮次 | 残差范围 | MSE | RMSE | 改善 |
|:---:|:-------:|:----:|:----:|:----:|
| 初始 | - | 1.9114 | 1.3825 | - |
| 1 | [-1.75, 1.74] | 1.5539 | 1.2466 | ↓18.7% |
| 2 | [-1.59, 1.57] | 1.2643 | 1.1244 | ↓18.6% |
| 3 | [-1.45, 1.42] | 1.0296 | 1.0147 | ↓18.6% |
| 4 | [-1.33, 1.28] | 0.8386 | 0.9157 | ↓18.6% |
| 5 | [-1.21, 1.15] | 0.6837 | 0.8269 | ↓18.5% |
| 6 | [-1.12, 1.04] | **0.5577** | **0.7468** | ↓18.4% |

**最终模型**:

$$f(x) = 7.3070 + 0.1 \cdot T_1(x) + 0.1 \cdot T_2(x) + ... + 0.1 \cdot T_6(x)$$

**最终性能**:
- 训练集 MSE: **0.5577**
- 训练集 RMSE: **0.7468**
- 相比初始MSE改善: **70.8%**

**预测结果示例**:

| 样本 | X | 真实Y | 预测Y | 误差 |
|:---:|:---:|:-----:|:-----:|:----:|
| 1 | 1 | 5.56 | 6.58 | -1.02 |
| 4 | 4 | 6.40 | 6.92 | -0.52 |
| 6 | 6 | 7.05 | 7.08 | -0.03 |
| 8 | 8 | 8.70 | 8.01 | +0.69 |
| 10 | 10 | 9.05 | 8.11 | +0.94 |

**回归树结构示例** (第1轮):
```
分裂节点: x <= 6.5
├─ 左子树: x <= 3.5
│  ├─ 叶节点: -1.5837
│  └─ 叶节点: -0.5570
└─ 右子树: x <= 8.5
   ├─ 叶节点: 1.4930
   └─ 叶节点: 1.7180
```

**可视化内容**:
1. **每轮迭代图** (6个子图):
   - 训练数据散点图
   - 累积预测曲线（蓝色实线）
   - 当前树的贡献（绿色虚线）
   - 当前MSE显示

2. **最终模型图**:
   - 完整的拟合曲线（红色粗线）
   - 展示模型的整体拟合效果
   - MSE和RMSE统计

3. **MSE变化曲线**:
   - 展示训练误差的持续下降
   - 最终MSE标注线

**GBDT vs AdaBoost vs 前向分步算法**:

| 特性 | GBDT | AdaBoost | 前向分步（指数损失） |
|------|------|----------|---------------------|
| **核心思想** | 拟合负梯度 | 调整样本权重 | 极小化损失函数 |
| **损失函数** | 平方损失 | 指数损失 | 指数损失 |
| **弱学习器** | 回归树 | 分类树桩 | 任意 |
| **更新方式** | $f + \nu T$ | $f + \alpha G$ | $f + \beta b$ |
| **适用问题** | 回归+分类 | 主要分类 | 分类+回归 |
| **对噪声** | 较鲁棒 | 敏感 | 取决于损失函数 |
| **学习率** | 显式 ν | 自适应 α | 取决于优化 |
| **训练方式** | 拟合残差 | 加权训练 | 梯度下降 |

**GBDT的优势**:
- **灵活性强**: 可以使用不同的损失函数（平方损失、绝对损失、Huber损失等）
- **预测精度高**: 在很多任务上性能优异
- **特征重要性**: 可以评估特征的重要性
- **处理混合数据**: 可以同时处理数值型和类别型特征
- **鲁棒性好**: 对异常值相对不敏感（取决于损失函数）

**GBDT的应用**:
- **回归预测**: 房价预测、销量预测、股票预测
- **分类任务**: 信用评分、客户流失预测
- **排序问题**: 搜索引擎排序、推荐系统
- **特征工程**: 自动特征组合和选择
- **竞赛利器**: Kaggle等数据科学竞赛的常用算法

**实现细节**:

1. **RegressionTree类**:
   - 递归构建CART回归树
   - MSE作为分裂准则
   - 支持最大深度和最小样本数控制
   - 叶节点预测值为区域均值

2. **GBDTRegressor类**:
   - 通用的梯度提升框架
   - 支持自定义学习率和树参数
   - 详细的训练日志输出
   - 每轮显示残差统计和树结构

3. **梯度提升过程**:
   - 初始化为所有样本的均值
   - 每轮计算当前模型的残差
   - 训练新树拟合残差
   - 按学习率更新模型

**理论扩展**:

GBDT可以推广到任意可微的损失函数：

- **分类**: 使用对数损失（交叉熵）
- **鲁棒回归**: 使用绝对损失或Huber损失
- **排序**: 使用pairwise或listwise损失

这使得GBDT成为一个非常通用的机器学习框架。

**与XGBoost、LightGBM的关系**:

- **GBDT**: 基础的梯度提升框架
- **XGBoost**: GBDT的优化实现，添加了正则化、并行计算等
- **LightGBM**: 使用直方图算法加速的GBDT变体
- **CatBoost**: 专门处理类别特征的GBDT变体

**关键洞察**:

1. **残差学习**: 每棵树学习前面所有树的残差，逐步减小误差
2. **加法模型**: 最终模型是所有树的加权和
3. **梯度下降视角**: GBDT是函数空间上的梯度下降
4. **偏差-方差权衡**: 学习率控制偏差和方差的平衡

---

### 10. 隐马尔可夫模型 (Hidden Markov Model)

隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态序列，再由各个状态生成一个观测而产生观测序列的过程。

#### HMM观测序列生成算法
- **文件**: `hmm/hmm_generate.py`
- **功能**: 根据HMM三要素生成观测序列
- **三要素**:
  - **初始状态概率分布** π: P(q₁=sᵢ)
  - **状态转移概率矩阵** A: P(q_{t+1}=sⱼ|q_t=sᵢ)
  - **观测概率矩阵** B: P(o_t=vₖ|q_t=sᵢ)

**问题示例 - 盒子抽球模型**:

设有4个盒子，每个盒子里有红球和白球：
- 盒子1: 红球5个，白球5个
- 盒子2: 红球3个，白球7个
- 盒子3: 红球6个，白球4个
- 盒子4: 红球8个，白球2个

**状态转移规则**:
- 盒子1 → 盒子2 (概率1.0)
- 盒子2 → 盒子1 (0.4) 或 盒子3 (0.6)
- 盒子3 → 盒子2 (0.4) 或 盒子4 (0.6)
- 盒子4 → 盒子3 (0.5) 或 盒子4 (0.5)

**生成过程**:
1. 随机选择一个盒子作为初始状态
2. 从当前盒子中随机抽取一个球，记录颜色，放回
3. 按照转移规则转移到下一个盒子
4. 重复步骤2-3，生成长度为T的观测序列

**运行示例**:
```bash
python hmm/hmm_generate.py
```

**输出结果**:
```
隐状态序列: ['盒子2', '盒子3', '盒子2', '盒子1', '盒子2']
观测序列:   ['白球', '红球', '红球', '白球', '白球']
```

**HMM的三个基本问题**:

1. **概率计算问题**（前向-后向算法）
   - 给定模型 λ=(A,B,π) 和观测序列 O，计算 P(O|λ)
   - 应用：评估模型与观测序列的匹配程度

2. **学习问题**（Baum-Welch算法，EM算法）
   - 给定观测序列 O，估计模型参数 λ=(A,B,π)
   - 应用：从数据中学习HMM模型

3. **预测问题**（维特比算法）
   - 给定模型 λ=(A,B,π) 和观测序列 O，求最可能的状态序列
   - 应用：序列标注、词性标注、语音识别

**HMM的两个基本假设**:

1. **齐次马尔可夫假设**: 隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态
   $$P(q_t|q_{t-1}, q_{t-2}, \ldots, q_1) = P(q_t|q_{t-1})$$

2. **观测独立性假设**: 任意时刻的观测只依赖于该时刻的马尔可夫链的状态
   $$P(o_t|q_t, q_{t-1}, \ldots, q_1, o_{t-1}, \ldots, o_1) = P(o_t|q_t)$$

**模型表示**:

HMM模型 λ 由三元组确定：
$$\lambda = (A, B, \pi)$$

- **状态转移概率矩阵** A:
  $$A = [a_{ij}]_{N \times N}, \quad a_{ij} = P(q_{t+1}=s_j|q_t=s_i)$$
  
- **观测概率矩阵** B:
  $$B = [b_j(k)]_{N \times M}, \quad b_j(k) = P(o_t=v_k|q_t=s_j)$$
  
- **初始状态概率向量** π:
  $$\pi = (\pi_i), \quad \pi_i = P(q_1=s_i)$$

**观测序列生成算法**:

输入：HMM模型 λ=(A,B,π)，观测序列长度 T

输出：观测序列 O=(o₁,o₂,...,o_T)

步骤：
1. 按照初始状态分布 π 生成状态 q₁
2. 令 t=1
3. 按照状态 q_t 的观测概率分布 b_{q_t}(k) 生成观测 o_t
4. 按照状态 q_t 的转移概率分布 a_{q_t,j} 生成状态 q_{t+1}
5. 令 t=t+1，如果 t<T，转步骤3；否则终止

**实现特点**:

1. **完整的HMM类**:
   - 支持自定义状态集合和观测集合
   - 三要素参数化（A, B, π）
   - 详细的生成过程日志

2. **可视化输出**:
   - 隐状态序列图（盒子选择过程）
   - 观测序列图（球颜色序列）
   - 直观展示HMM的生成机制

3. **参数验证**:
   - 自动检查概率矩阵的合法性
   - 支持随机种子复现结果
   - 生成多个示例对比

**应用场景**:

- **语音识别**: 状态=音素，观测=声学特征
- **词性标注**: 状态=词性，观测=单词
- **生物信息学**: 状态=基因结构，观测=DNA序列
- **金融分析**: 状态=市场状态，观测=价格变化
- **行为识别**: 状态=动作类型，观测=传感器数据

**关键概念**:

- **隐状态**: 不可直接观测的系统内部状态
- **观测**: 可观测到的系统输出
- **马尔可夫性**: 当前状态只依赖于前一状态
- **发射概率**: 从状态生成观测的概率

**与其他模型的关系**:

- **马尔可夫链**: HMM的隐藏部分，状态完全可观测时退化为马尔可夫链
- **朴素贝叶斯**: HMM在时序数据上的扩展，考虑了状态间的转移
- **条件随机场(CRF)**: HMM的判别式版本，直接建模 P(状态|观测)

**实践价值**:

通过编写观测序列生成算法，可以深入理解：
1. HMM的三要素如何协同工作
2. 隐状态和观测之间的关系
3. 马尔可夫假设的含义
4. 为后续学习三个基本问题打下基础

---

#### HMM前向算法（Forward Algorithm）
- **文件**: `hmm/hmm_forward.py`
- **功能**: 计算观测序列概率 P(O|λ)
- **问题类型**: HMM三个基本问题之一——概率计算问题

**前向概率定义**:

$$\alpha_t(i) = P(o_1, o_2, \ldots, o_t, q_t=s_i|\lambda)$$

表示：到时刻t为止的观测序列为 o₁,o₂,...,o_t 且时刻t处于状态 s_i 的概率

**前向算法步骤**:

1. **初始化** (t=1):
   $$\alpha_1(i) = \pi_i \cdot b_i(o_1), \quad i=1,2,\ldots,N$$

2. **递推** (t=1,2,...,T-1):
   $$\alpha_{t+1}(i) = \left[\sum_{j=1}^{N} \alpha_t(j) \cdot a_{ji}\right] \cdot b_i(o_{t+1}), \quad i=1,2,\ldots,N$$

3. **终止** (t=T):
   $$P(O|\lambda) = \sum_{i=1}^{N} \alpha_T(i)$$

**算法特点**:

- **时间复杂度**: O(N²T)，相比直接计算的 O(N^T·T) 大幅降低
- **计算方向**: 从前向后（t=1 → T）
- **动态规划**: 利用前一时刻的结果计算当前时刻
- **局部概率**: 每个α_t(i)表示到时刻t的局部观测概率

**运行示例**:
```bash
python hmm/hmm_forward.py
```

**输出结果**（李航书例10.2）:
```
观测序列: ['红', '白', '红']
P(O|λ) = 0.130218

前向概率矩阵：
时刻1: α₁ = [0.1000, 0.1600, 0.2800]
时刻2: α₂ = [0.0770, 0.1104, 0.0606]
时刻3: α₃ = [0.0419, 0.0355, 0.0528]
```

**可视化输出**:
- 前向概率热力图：展示各时刻各状态的前向概率
- 前向概率曲线：展示每个状态的概率随时间变化
- 多序列对比：比较不同观测序列的概率

**实现亮点**:

1. **详细的计算过程**:
   - 打印每一步的中间结果
   - 显示求和的详细展开
   - 便于理解算法原理

2. **教学友好**:
   - 使用李航书中的标准例子
   - 结果与教材完全一致
   - 适合学习和验证

3. **丰富的测试**:
   - 测试多个不同的观测序列
   - 分析最可能和最不可能的序列
   - 验证概率值的合理性

---

#### HMM后向算法（Backward Algorithm）
- **文件**: `hmm/hmm_backward.py`
- **功能**: 从另一角度计算观测序列概率 P(O|λ)
- **问题类型**: 概率计算问题（与前向算法等价）

**后向概率定义**:

$$\beta_t(i) = P(o_{t+1}, o_{t+2}, \ldots, o_T|q_t=s_i, \lambda)$$

表示：在时刻t状态为 s_i 的条件下，从时刻t+1到T的观测序列为 o_{t+1},...,o_T 的概率

**后向算法步骤**:

1. **初始化** (t=T):
   $$\beta_T(i) = 1, \quad i=1,2,\ldots,N$$

2. **递推** (t=T-1, T-2, ..., 1):
   $$\beta_t(i) = \sum_{j=1}^{N} a_{ij} \cdot b_j(o_{t+1}) \cdot \beta_{t+1}(j), \quad i=1,2,\ldots,N$$

3. **终止** (t=1):
   $$P(O|\lambda) = \sum_{i=1}^{N} \pi_i \cdot b_i(o_1) \cdot \beta_1(i)$$

**算法特点**:

- **时间复杂度**: O(N²T)，与前向算法相同
- **计算方向**: 从后向前（t=T → 1）
- **互补性**: 与前向算法方向相反，结果相同
- **应用**: 常与前向算法结合用于参数学习（Baum-Welch算法）

**运行示例**:
```bash
python hmm/hmm_backward.py
```

**输出结果**（同样使用例10.2）:
```
观测序列: ['红', '白', '红']
P(O|λ) = 0.130218

后向概率矩阵：
时刻1: β₁ = [0.2451, 0.2622, 0.2277]
时刻2: β₂ = [0.5400, 0.4900, 0.5700]
时刻3: β₃ = [1.0000, 1.0000, 1.0000]
```

**前向-后向算法对比**:

| 特性 | 前向算法 | 后向算法 |
|------|---------|---------|
| **计算方向** | 从前向后 (→) | 从后向前 (←) |
| **概率含义** | 到t时刻的观测概率 | 从t+1到T的观测概率 |
| **初始化** | α₁(i) = πᵢ·bᵢ(o₁) | β_T(i) = 1 |
| **终止条件** | Σᵢ α_T(i) | Σᵢ πᵢ·bᵢ(o₁)·β₁(i) |
| **结果** | P(O\|λ) | P(O\|λ) |
| **应用** | 概率计算、滤波 | 参数学习、平滑 |

**验证结果**:
```
前向算法: P(O|λ) = 0.13021800
后向算法: P(O|λ) = 0.13021800
绝对误差: 0.00e+00 ✅
```

**实现特点**:

1. **算法对比功能**:
   - `HMMForwardBackward`类同时实现两种算法
   - 自动对比计算结果
   - 验证数值精度

2. **可视化对比**:
   - 4个子图展示前向和后向概率
   - 热力图对比两种概率的分布
   - 曲线图展示时序变化趋势

3. **多序列测试**:
   - 批量测试不同观测序列
   - 验证两种算法的一致性
   - 展示误差在机器精度范围内

**前向-后向算法的重要性**:

1. **Baum-Welch算法基础**: 
   - 同时需要α_t(i)和β_t(i)计算期望
   - 用于HMM的参数学习（EM算法）

2. **平滑问题**: 
   - 结合前向后向概率计算 γ_t(i) = P(q_t=s_i|O,λ)
   - 给定全部观测，求某时刻状态的概率

3. **维特比算法对比**:
   - 前向算法：所有路径的概率和
   - 维特比算法：最优路径的概率（最大值）

**数学关系**:

在任意时刻t，有：
$$P(O|\lambda) = \sum_{i=1}^{N} \alpha_t(i) \cdot \beta_t(i)$$

这个关系在所有时刻都成立，是Baum-Welch算法的理论基础。

---

#### HMM Baum-Welch算法（EM算法）
- **文件**: `hmm/hmm_baum_welch.py`
- **功能**: 从观测序列学习HMM参数 λ=(A,B,π)
- **问题类型**: HMM三个基本问题之二——学习问题（参数估计）

**问题定义**:

给定观测序列 O=(o₁,o₂,...,o_T)，估计模型参数 λ=(A,B,π)，使得 P(O|λ) 最大化。

**Baum-Welch算法本质**:

Baum-Welch算法是**EM算法**在HMM中的应用：
- **E步（Expectation）**: 利用当前参数计算期望（γ和ξ）
- **M步（Maximization）**: 最大化期望，更新参数

**核心概率变量**:

1. **γ_t(i)**: 给定观测序列O和模型λ，时刻t处于状态s_i的概率
   $$\gamma_t(i) = P(q_t=s_i|O,\lambda) = \frac{\alpha_t(i) \cdot \beta_t(i)}{\sum_{j=1}^{N} \alpha_t(j) \cdot \beta_t(j)}$$

2. **ξ_t(i,j)**: 给定观测序列O和模型λ，时刻t处于状态s_i且时刻t+1处于状态s_j的概率
   $$\xi_t(i,j) = P(q_t=s_i, q_{t+1}=s_j|O,\lambda) = \frac{\alpha_t(i) \cdot a_{ij} \cdot b_j(o_{t+1}) \cdot \beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_t(i) \cdot a_{ij} \cdot b_j(o_{t+1}) \cdot \beta_{t+1}(j)}$$

**参数更新公式**:

1. **初始状态概率**:
   $$\pi_i = \gamma_1(i)$$

2. **状态转移概率**:
   $$a_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}$$
   
   含义：从状态i转移到状态j的期望次数 / 从状态i出发的期望次数

3. **观测概率**:
   $$b_j(k) = \frac{\sum_{t=1,o_t=v_k}^{T} \gamma_t(j)}{\sum_{t=1}^{T} \gamma_t(j)}$$
   
   含义：在状态j观测到v_k的期望次数 / 处于状态j的期望次数

**算法步骤**:

1. **初始化**: 随机初始化参数 λ=(A,B,π)

2. **E步**: 
   - 计算前向概率 α_t(i)
   - 计算后向概率 β_t(i)
   - 计算 γ_t(i) 和 ξ_t(i,j)

3. **M步**: 
   - 用上述公式更新 π, A, B

4. **迭代**: 重复E步和M步直到收敛

5. **终止**: 当对数似然变化小于阈值或达到最大迭代次数

**运行示例**:
```bash
python hmm/hmm_baum_welch.py
```

**输出结果**（示例1：已知真实参数验证）:
```
真实参数:
π = [0.2, 0.4, 0.4]
A = [[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]
B = [[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]]

训练数据: 10条序列，每条长度50

学习后参数（50次迭代）:
π = [0.41, 0.02, 0.57]
A = [[0.32, 0.41, 0.27], [0.44, 0.35, 0.20], [0.21, 0.29, 0.50]]
B = [[0.84, 0.16], [0.29, 0.71], [0.43, 0.57]]

平均参数误差: 0.193
```

**算法特点**:

1. **无监督学习**:
   - 只需要观测序列，不需要状态标签
   - 自动发现隐藏状态的规律

2. **EM算法保证**:
   - 似然函数单调不减
   - 收敛到局部最优解

3. **数值稳定性**:
   - 使用缩放因子避免下溢
   - 在对数空间计算似然

4. **多序列学习**:
   - 支持从多条观测序列联合学习
   - 提高参数估计的准确性

**实现亮点**:

1. **完整的EM框架**:
   - 清晰的E步和M步分离
   - 详细的迭代日志
   - 收敛性检测

2. **参数初始化**:
   - 支持随机初始化
   - 支持均匀初始化
   - 可手动设置初始值

3. **可视化学习过程**:
   - 对数似然曲线
   - 参数对比分析
   - 误差统计

4. **实际应用示例**:
   - 示例1：已知真实参数，验证算法正确性
   - 示例2：纯无监督学习，发现隐藏规律

**应用场景**:

- **语音识别**: 从声学特征学习音素模型
- **词性标注**: 从文本数据学习词性转移规律
- **生物信息学**: 从DNA序列学习基因结构
- **金融建模**: 从价格序列学习市场状态
- **行为分析**: 从传感器数据学习行为模式

**算法优势**:

1. **理论保证**: EM算法保证似然单调增
2. **通用性强**: 适用于各种HMM变体
3. **可解释性**: 参数有明确的物理意义
4. **实现简单**: 公式推导清晰，易于实现

**局限性**:

1. **局部最优**: 可能收敛到局部最优解，依赖初始化
2. **计算复杂度**: O(N²T) 每次迭代，对长序列较慢
3. **状态数选择**: 需要预先指定状态数N
4. **标识性问题**: 不同参数可能生成相同的观测分布

**改进方向**:

- **初始化策略**: 使用K-means等方法初始化
- **正则化**: 添加先验避免过拟合
- **变分推断**: 使用变分Bayes方法
- **在线学习**: 增量式参数更新

---

#### HMM Viterbi算法（预测问题）
- **文件**: `hmm/hmm_viterbi.py`
- **功能**: 给定模型和观测序列，求最可能的状态序列
- **问题类型**: HMM三个基本问题之三——预测问题（解码问题）

**问题定义**:

给定模型 λ=(A,B,π) 和观测序列 O=(o₁,o₂,...,o_T)，求最可能的状态序列 Q*=(q₁*,q₂*,...,q_T*)。

即求：

$$Q^* = \arg\max_Q P(Q|O,\lambda)$$

**Viterbi算法原理**:

Viterbi算法是一种**动态规划算法**，通过递推计算最优路径。

**核心变量定义**:

1. **δ_t(i)**: 到时刻t为止，以状态s_i为终点的所有路径中概率最大的路径的概率
   
   $$\delta_t(i) = \max_{q_1,q_2,\ldots,q_{t-1}} P(q_1,q_2,\ldots,q_t=s_i, o_1,o_2,\ldots,o_t|\lambda)$$

2. **ψ_t(i)**: 在时刻t状态为s_i时，时刻t-1的最优前驱状态
   
   $$\psi_t(i) = \arg\max_{1 \leq j \leq N} [\delta_{t-1}(j) \cdot a_{ji}]$$

**算法步骤**:

**步骤1：初始化** (t=1):

$$\delta_1(i) = \pi_i \cdot b_i(o_1), \quad i=1,2,\ldots,N$$

$$\psi_1(i) = 0$$

初始化时，每个状态的概率为初始概率乘以观测概率。

**步骤2：递推** (t=2,3,...,T):

对每个状态i，计算：

$$\delta_t(i) = \max_{1 \leq j \leq N} [\delta_{t-1}(j) \cdot a_{ji}] \cdot b_i(o_t), \quad i=1,2,\ldots,N$$

$$\psi_t(i) = \arg\max_{1 \leq j \leq N} [\delta_{t-1}(j) \cdot a_{ji}], \quad i=1,2,\ldots,N$$

这一步找出到达每个状态的最优路径及其前驱状态。

**步骤3：终止** (t=T):

求最优路径的概率和终点状态：

$$P^* = \max_{1 \leq i \leq N} \delta_T(i)$$

$$q_T^* = \arg\max_{1 \leq i \leq N} \delta_T(i)$$

**步骤4：路径回溯** (t=T-1,T-2,...,1):

从终点状态开始，沿着ψ矩阵回溯最优路径：

$$q_t^* = \psi_{t+1}(q_{t+1}^*), \quad t=T-1,T-2,\ldots,1$$

**算法特点**:

- **时间复杂度**: O(N²T)，与前向算法相同
- **空间复杂度**: O(NT)，需要存储δ和ψ矩阵
- **动态规划**: 局部最优解构成全局最优解
- **确定性**: 输出唯一的最优路径

**Viterbi vs 前向算法对比**:

| 特性 | 前向算法 | Viterbi算法 |
|------|---------|------------|
| **目标** | 所有路径的概率和 | 最优路径的概率 |
| **操作** | 求和 Σ | 求最大值 max |
| **结果** | P(O\|λ) | P*, Q* |
| **含义** | 观测序列的总概率 | 最可能的状态序列 |
| **应用** | 模型评估、概率计算 | 序列标注、解码 |
| **关系** | P(O\|λ) ≥ P* | P* 是 P(O\|λ) 的一部分 |

**数学关系**:

前向算法计算所有路径的概率和：

$$P(O|\lambda) = \sum_{Q} P(O,Q|\lambda)$$

Viterbi算法找概率最大的单一路径：

$$P^* = \max_{Q} P(O,Q|\lambda)$$

因此总是有：**P(O|λ) ≥ P***

**运行示例**:
```bash
python hmm/hmm_viterbi.py
```

**输出结果**（李航书例10.3）:
```
观测序列: ['红', '白', '红']

【Viterbi算法结果】
最优状态序列: ['盒子3', '盒子3', '盒子3']
最大概率: P* = 0.01470000

【前向算法结果】
P(O|λ) = 0.13021800

【验证】
P(O|λ) ≥ P*: 0.13022 ≥ 0.01470 ✓
比值: P(所有路径) / P(最优路径) = 8.86
```

**详细计算过程**:

**初始化 (t=1)**:
```
观测 o₁ = '红'
δ₁(盒子1) = π₁ × b₁(红) = 0.2 × 0.5 = 0.100
δ₁(盒子2) = π₂ × b₂(红) = 0.4 × 0.4 = 0.160
δ₁(盒子3) = π₃ × b₃(红) = 0.4 × 0.7 = 0.280  ← 最大
```

**递推 (t=2)**:
```
观测 o₂ = '白'
对于每个状态j，计算 max_i[δ₁(i) × a_ij] × b_j(白)

δ₂(盒子1) = max[0.100×0.5, 0.160×0.3, 0.280×0.2] × 0.5
           = 0.056 × 0.5 = 0.028
           前驱: ψ₂(盒子1) = 盒子3

δ₂(盒子2) = max[0.100×0.2, 0.160×0.5, 0.280×0.3] × 0.6
           = 0.084 × 0.6 = 0.050  ← 最大
           前驱: ψ₂(盒子2) = 盒子3

δ₂(盒子3) = max[0.100×0.3, 0.160×0.2, 0.280×0.5] × 0.3
           = 0.140 × 0.3 = 0.042
           前驱: ψ₂(盒子3) = 盒子3
```

**递推 (t=3)**:
```
观测 o₃ = '红'
δ₃(盒子1) = max[0.028×0.5, 0.050×0.3, 0.042×0.2] × 0.5
           = 0.015 × 0.5 = 0.0076
           前驱: ψ₃(盒子1) = 盒子2

δ₃(盒子2) = max[0.028×0.2, 0.050×0.5, 0.042×0.3] × 0.4
           = 0.025 × 0.4 = 0.0101
           前驱: ψ₃(盒子2) = 盒子2

δ₃(盒子3) = max[0.028×0.3, 0.050×0.2, 0.042×0.5] × 0.7
           = 0.021 × 0.7 = 0.0147  ← 最大
           前驱: ψ₃(盒子3) = 盒子3
```

**终止**:
```
P* = max(δ₃) = 0.0147
q₃* = argmax(δ₃) = 盒子3
```

**路径回溯**:
```
q₃* = 盒子3
q₂* = ψ₃(盒子3) = 盒子3
q₁* = ψ₂(盒子3) = 盒子3

最优状态序列: [盒子3, 盒子3, 盒子3]
```

**实现特点**:

1. **详细的迭代日志**:
   - 每一步的δ值计算
   - 前驱状态的选择过程
   - 完整的数学推导展示

2. **可视化**:
   - **δ矩阵热力图**: 展示各时刻各状态的局部最优概率
   - **ψ矩阵**: 显示最优前驱状态
   - **最优路径图**: 用箭头连接最优状态序列
   - **δ曲线**: 展示每个状态的概率随时间变化
   - **结果表格**: 汇总路径信息

3. **前向-Viterbi对比功能**:
   - 同时运行前向算法和Viterbi算法
   - 验证 P(O|λ) ≥ P* 关系
   - 分析比值，理解两种算法的差异

4. **多序列测试**:
   - 测试不同观测序列的解码结果
   - 分析状态选择的规律
   - 观察最优路径的特点

**应用场景**:

1. **词性标注**:
   - 观测序列: 句子中的单词
   - 隐状态: 词性（名词、动词、形容词等）
   - 目标: 为每个单词标注最可能的词性

2. **语音识别**:
   - 观测序列: 声学特征（MFCC等）
   - 隐状态: 音素或词
   - 目标: 将声音转换为文本

3. **生物信息学**:
   - 观测序列: DNA碱基序列（A,T,C,G）
   - 隐状态: 基因结构（外显子、内含子等）
   - 目标: 基因预测和注释

4. **金融分析**:
   - 观测序列: 股票价格变化
   - 隐状态: 市场状态（牛市、熊市、震荡）
   - 目标: 识别当前市场状态

**Viterbi算法的优势**:

1. **全局最优**: 动态规划保证找到全局最优路径
2. **高效**: O(N²T) 比暴力枚举的 O(N^T) 快得多
3. **确定性**: 输出唯一确定的最优序列
4. **可追溯**: 通过ψ矩阵可以回溯决策过程

**与其他解码方法对比**:

| 方法 | 输出 | 特点 |
|------|------|------|
| **Viterbi** | 最优路径 | 全局最优，确定性 |
| **前向-后向** | 边缘概率 | 每个时刻的状态分布 |
| **Beam Search** | 近似最优路径 | 保留top-k路径，速度快 |

**实现细节**:

1. **数值稳定性**:
   - 对于长序列，概率值可能非常小
   - 可以在对数空间计算避免下溢
   - 当前实现适用于中等长度序列

2. **路径回溯**:
   - 从终点状态开始
   - 沿着ψ矩阵记录的最优前驱回溯
   - 构造完整的状态序列

3. **比较分析**:
   - 提供与前向算法的对比
   - 验证理论关系
   - 帮助理解算法本质

**HMM三个基本问题总结**:

| 问题 | 算法 | 输入 | 输出 | 复杂度 |
|------|------|------|------|--------|
| **概率计算** | 前向/后向 | λ, O | P(O\|λ) | O(N²T) |
| **参数学习** | Baum-Welch | O (多条) | λ=(A,B,π) | O(N²T)×迭代次数 |
| **状态预测** | Viterbi | λ, O | Q* | O(N²T) |

这三个问题构成了HMM的完整理论体系，解决了从模型评估、参数学习到序列解码的所有关键问题。

---

### 条件随机场 (CRF)
**核心思想**: 条件随机场是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场。CRF是一种判别式模型，直接建模条件概率P(Y|X)，相比HMM等生成式模型能够更灵活地利用观测序列的各种特征。

**模型定义**: 线性链条件随机场
$$P(y|x) = \frac{1}{Z(x)} \exp\left(\sum_{t=1}^{T} \sum_{k} w_k f_k(y_{t-1}, y_t, x, t)\right)$$

其中:
- $x = (x_1, x_2, ..., x_T)$ 是观测序列
- $y = (y_1, y_2, ..., y_T)$ 是标签序列
- $f_k$ 是特征函数（包括转移特征和状态特征）
- $w_k$ 是特征权重
- $Z(x)$ 是归一化因子（配分函数）：
$$Z(x) = \sum_{y} \exp\left(\sum_{t=1}^{T} \sum_{k} w_k f_k(y_{t-1}, y_t, x, t)\right)$$

#### 1. CRF前向-后向算法（推断）

**算法目的**: 计算归一化因子 $Z(x)$ 和边缘概率 $P(y_t|x)$

**前向算法** (crf_forward_backward.py):

定义前向变量（对数空间）:
$$\alpha_t(y) = \log \sum_{y'} \exp(\alpha_{t-1}(y') + \psi_t(y', y|x))$$

其中 $\psi_t(y', y|x) = \sum_k w_k f_k(y', y, x, t)$ 是局部势函数

递推步骤:
```
初始化: α_1(y) = ψ_1(START, y|x)
递推: α_t(y) = logsumexp_{y'} (α_{t-1}(y') + ψ_t(y', y|x))
终止: log Z(x) = logsumexp_y α_T(y)
```

**后向算法**:

定义后向变量:
$$\beta_t(y) = \log \sum_{y'} \exp(\psi_{t+1}(y, y'|x) + \beta_{t+1}(y'))$$

递推步骤:
```
初始化: β_T(y) = 0
递推: β_t(y) = logsumexp_{y'} (ψ_{t+1}(y, y'|x) + β_{t+1}(y'))
```

**边缘概率计算**:
$$P(y_t|x) = \frac{\exp(\alpha_t(y_t) + \beta_t(y_t))}{Z(x)}$$

**运行示例**:
```bash
python crf/crf_forward_backward.py
```

输出包括:
- 归一化因子 Z(x)
- 每个位置的边缘概率分布
- 前向/后向变量矩阵可视化
- 概率曲线图

#### 2. CRF Viterbi算法（解码）

**算法目的**: 找到最优标签序列 $y^* = \arg\max_y P(y|x)$

**动态规划公式** (crf_viterbi.py):

定义局部最优得分:
$$\delta_t(y) = \max_{y_1,...,y_{t-1}} \left[\sum_{i=1}^{t} \sum_k w_k f_k(y_{i-1}, y_i, x, i)\right]$$

递推关系:
$$\delta_t(y) = \max_{y'} [\delta_{t-1}(y') + \psi_t(y', y|x)]$$

前驱记录:
$$\psi_t(y) = \arg\max_{y'} [\delta_{t-1}(y') + \psi_t(y', y|x)]$$

**算法步骤**:
```
1. 初始化: δ_1(y) = ψ_1(START, y|x)
2. 递推: 
   对 t = 2 到 T:
     δ_t(y) = max_{y'} [δ_{t-1}(y') + ψ_t(y', y|x)]
     ψ_t(y) = argmax_{y'} [δ_{t-1}(y') + ψ_t(y', y|x)]
3. 终止: y_T* = argmax_y δ_T(y)
4. 回溯: y_t* = ψ_{t+1}(y_{t+1}*) for t = T-1 到 1
```

**运行示例**:
```bash
python crf/crf_viterbi.py
```

输出包括:
- 最优标签序列
- 最优路径得分
- δ矩阵热力图
- ψ矩阵可视化
- 最优路径追踪图

#### 3. CRF BFGS训练算法（学习）

**算法目的**: 从标注数据学习特征权重 $w = (w_1, w_2, ..., w_K)$

**目标函数** (crf_train.py):

负对数似然损失（加L2正则化）:
$$L(w) = -\sum_{i=1}^{N} \log P(y^{(i)}|x^{(i)}) + \frac{\lambda}{2}||w||^2$$

$$= -\sum_{i=1}^{N} \left[\sum_k w_k F_k(x^{(i)}, y^{(i)}) - \log Z(x^{(i)})\right] + \frac{\lambda}{2}||w||^2$$

其中 $F_k(x, y) = \sum_t f_k(y_{t-1}, y_t, x, t)$ 是全局特征函数

**梯度计算**:

$$\frac{\partial L}{\partial w_k} = -\sum_{i=1}^{N} [F_k(x^{(i)}, y^{(i)}) - E_{P(y|x^{(i)})}[F_k(x^{(i)}, y)]] + \lambda w_k$$

$$= -\sum_{i=1}^{N} [\text{经验特征计数} - \text{期望特征计数}] + \lambda w_k$$

期望特征计数通过前向-后向算法计算:
$$E_{P(y|x)}[F_k(x, y)] = \sum_t \sum_{y'} \sum_{y} P(y_{t-1}=y', y_t=y|x) \cdot f_k(y', y, x, t)$$

**优化算法**: L-BFGS-B (拟牛顿法)
- 利用梯度信息构造Hessian矩阵近似
- 比标准梯度下降收敛更快
- 支持盒约束（可限制权重范围）

**训练步骤**:
```
1. 初始化权重 w = 0
2. 重复直到收敛:
   a. 计算损失 L(w)
   b. 对每个训练样本:
      - 计算经验特征计数（直接从标注数据）
      - 用前向-后向算法计算期望特征计数
   c. 计算梯度 ∇L(w)
   d. L-BFGS-B更新权重
3. 返回最优权重 w*
```

**运行示例**:
```bash
python crf/crf_train.py
```

输出包括:
- 训练迭代信息（损失、梯度范数）
- 学到的特征权重
- 训练/测试准确率
- 损失曲线和梯度范数曲线

#### 4. CRF完整应用示例

文件 `crf_complete_example.py` 集成了上述三个算法，展示了CRF在以下任务中的应用:

**应用1: 中文分词** (BMES标注)
- **状态集**: B(词首), M(词中), E(词尾), S(单字成词)
- **特征**: 字符特征、转移特征
- **训练数据**: 8个中文句子
- **效果**: 能够正确切分"机器学习"、"自然语言处理"等词汇

```bash
输入: "我爱自然语言处理"
输出: "我/爱/自然语言处理" (B-M-E标注)
```

**应用2: 命名实体识别** (BIO标注)
- **状态集**: B-PER, I-PER, B-LOC, I-LOC, O
- **任务**: 识别人名(PER)和地名(LOC)
- **训练数据**: 5个标注句子
- **效果**: 能识别"钱八"(人名)、"成都"(地名)

```bash
输入: "钱八在成都工作"
输出: 钱八(PER), 成都(LOC)
```

**应用3: 词性标注**
- **状态集**: n(名词), v(动词), a(形容词), d(副词), p(介词)
- **训练数据**: 3个标注句子

**运行完整示例**:
```bash
python crf/crf_complete_example.py
```

输出包括:
- 三个应用的训练和测试结果
- 可视化图表（中文分词示例）
- CRF vs HMM对比分析
- 算法总结

#### CRF vs HMM 对比

| 特性 | HMM | CRF |
|------|-----|-----|
| **模型类型** | 生成式模型 | 判别式模型 |
| **建模对象** | 联合概率 P(X,Y) | 条件概率 P(Y\|X) |
| **独立性假设** | 观测独立假设（强） | 无观测独立性假设 |
| **特征使用** | 局限于单个观测 | 可使用任意全局特征 |
| **训练算法** | EM算法（Baum-Welch） | 梯度优化（BFGS） |
| **推断算法** | 前向-后向 | 前向-后向（对数空间） |
| **解码算法** | Viterbi | Viterbi |
| **优势** | 无监督/半监督学习 | 特征工程灵活、精度高 |
| **劣势** | 观测独立假设限制强 | 需要标注数据、训练慢 |
| **典型应用** | 语音识别、基因预测 | 分词、NER、词性标注 |

**CRF三个基本问题总结**:

| 问题 | 算法 | 输入 | 输出 | 复杂度 |
|------|------|------|------|--------|
| **概率计算/推断** | 前向-后向 | w, x | Z(x), P(y_t\|x) | O(S²T) |
| **参数学习** | BFGS | (x,y)多条 | w* | O(S²T×N)×迭代次数 |
| **序列解码** | Viterbi | w, x | y* | O(S²T) |

其中 S 是状态数，T 是序列长度，N 是训练样本数。

**关键差异**:
1. **推断**: CRF计算归一化因子Z(x)，HMM计算观测序列概率P(O|λ)
2. **学习**: CRF优化判别式目标函数，HMM最大化数据似然
3. **解码**: 两者都使用Viterbi，但CRF可以利用更丰富的特征

---

### 层次聚类
**核心思想**: 通过计算数据点间的相似度创建有层次的嵌套聚类树。凝聚式层次聚类从每个样本作为单独的簇开始，逐步合并最相似的簇。

**算法步骤** (凝聚式):
```
1. 初始化: 每个样本单独成簇（n个簇）
2. 计算: 所有簇对之间的距离矩阵
3. 合并: 距离最近的两个簇
4. 更新: 重新计算涉及新簇的距离
5. 重复: 直到达到目标簇数量
```

**链接方法** (簇间距离):

| 方法 | 公式 | 特点 |
|------|------|------|
| Single | $d = \min_{x \in C_i, y \in C_j} d(x,y)$ | 最近邻，易产生链式效应 |
| Complete | $d = \max_{x \in C_i, y \in C_j} d(x,y)$ | 最远邻，形成紧凑球形簇 |
| Average | $d = \frac{1}{n_i n_j} \sum\sum d(x,y)$ | 平均距离，较为稳健 |
| Ward | $d = \sqrt{\frac{n_i n_j}{n_i+n_j}} \|\mu_i - \mu_j\|$ | 最小化方差增量，常用 |

**距离度量**:
- **欧几里得**: $d = \sqrt{\sum(x_i-y_i)^2}$
- **曼哈顿**: $d = \sum|x_i-y_i|$  
- **余弦**: $d = 1 - \frac{x \cdot y}{\|x\|\|y\|}$

**树状图** (Dendrogram):
- 可视化层次聚类过程
- 纵轴=合并距离，横轴=样本索引
- 在不同高度"切割"得到不同簇数

**选择簇数**:
1. 肘部法则（观察距离突变）
2. 树状图切割点
3. 轮廓系数评估

**复杂度**: $O(n^2 \log n)$ 到 $O(n^3)$

**优缺点**:

优点:
- 不需预先指定簇数
- 可发现任意形状的簇
- 完整层次结构
- 结果可解释（树状图）

缺点:
- 不适合大规模数据
- 合并不可撤销
- 对噪声敏感
- 需选择链接方法和距离

**典型应用**:
- 生物信息学（基因/物种分类）
- 文档聚类
- 图像分割
- 社交网络分析

---

### K-Means聚类 (K-Means Clustering)
**核心思想**: K-Means是基于划分的聚类算法，通过迭代优化将数据点分配到K个簇中，使得簇内平方误差（SSE）最小化。每个簇由其质心（均值）表示。

**目标函数**: 最小化簇内平方误差（SSE / Inertia）

$$J = \sum_{k=1}^{K} \sum_{x \in C_k} \|x - \mu_k\|^2$$

其中:
- $K$ 是簇的数量
- $C_k$ 是第k个簇
- $\mu_k$ 是第k个簇的中心（均值）
- $\|x - \mu_k\|$ 是样本到簇中心的欧几里得距离

**算法步骤**:

```
1. 初始化: 选择K个初始聚类中心 μ₁, μ₂, ..., μₖ
2. 分配: 将每个样本分配到最近的聚类中心
   C_k = {x : ||x - μ_k|| ≤ ||x - μ_j|| for all j}
3. 更新: 重新计算每个簇的中心（均值）
   μ_k = (1/|C_k|) Σ_{x∈C_k} x
4. 重复: 重复步骤2-3直到收敛
   - 中心不再变化 (或变化 < 阈值)
   - 达到最大迭代次数
```

**初始化方法**:

1. **随机初始化**:
   - 从数据集中随机选择K个点作为初始中心
   - 优点：简单快速
   - 缺点：结果依赖初始选择，可能陷入局部最优

2. **K-Means++** (推荐):
   - 改进的初始化方法，选择彼此距离较远的初始中心
   - 算法：
     ```
     a. 随机选择第一个中心
     b. 对每个点x，计算其到最近中心的距离D(x)
     c. 以概率 D(x)²/Σ D(x)² 选择下一个中心
     d. 重复b-c直到选出K个中心
     ```
   - 优点：提高收敛速度和结果质量，减少对初始化的敏感性
   - 复杂度：O(kn)

**评估指标**:

1. **SSE（簇内平方误差）/ Inertia**:
   - 值越小表示簇内聚合度越高
   - 缺点：K越大SSE越小（极端情况K=n时SSE=0）

2. **轮廓系数** (Silhouette Score):
   $$s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$$
   其中:
   - $a(i)$: 样本i到同簇其他点的平均距离（簇内距离）
   - $b(i)$: 样本i到最近其他簇所有点的平均距离（簇间距离）
   - 取值范围：[-1, 1]
   - 值越接近1表示聚类质量越好
   - s(i) > 0: 样本被合理分类
   - s(i) ≈ 0: 样本在簇边界上
   - s(i) < 0: 样本可能被分到错误的簇

**选择最佳K值**:

1. **肘部法则** (Elbow Method):
   - 绘制K值与SSE的关系曲线
   - 寻找曲线的"肘部"（斜率变化最大的点）
   - 肘部对应的K值通常是较好的选择

2. **轮廓系数法**:
   - 计算不同K值的平均轮廓系数
   - 选择轮廓系数最大的K值

**运行示例**:
```bash
python clustering/kmeans_clustering.py
```

**输出内容**:

- **示例1**: 初始化方法对比（Random vs K-Means++）
  * 150个样本的3簇数据
  * 对比两种初始化方法的收敛速度和结果质量
  * 计算SSE和轮廓系数
  * 保存为 `kmeans_basic_comparison.png`

- **示例2**: 肘部法则选择最佳K值
  * 测试K从2到10
  * 绘制SSE曲线和轮廓系数曲线
  * 展示最佳K值的聚类结果
  * 保存为 `kmeans_elbow_method.png`

- **示例3**: 收敛过程可视化
  * 展示K-Means迭代过程
  * 显示第0, 1, 2, 5, 10次迭代的中间结果
  * SSE收敛曲线
  * 保存为 `kmeans_convergence.png`

- **示例4**: 不同K值效果对比
  * 展示K=2,3,4,5,6,8的聚类结果
  * 每个K值的SSE和轮廓系数
  * 保存为 `kmeans_different_k.png`

**算法特点**:

优点:
- 简单易懂，实现容易
- 时间复杂度 O(nkt)（n样本数，k簇数，t迭代次数）
- 空间复杂度 O(n)
- 适合大规模数据集
- 收敛速度快（通常几十次迭代）

缺点:
- 需要预先指定K值
- 对初始中心敏感（K-Means++可改善）
- 只能发现凸形簇（球形簇）
- 对噪声和异常值敏感
- 不同尺度特征需要标准化
- 可能收敛到局部最优

**与层次聚类对比**:

| 特性 | K-Means | 层次聚类 |
|------|---------|----------|
| 聚类类型 | 划分式 | 层次式 |
| 簇数指定 | 需要预先指定 | 可事后选择 |
| 复杂度 | O(nkt) | O(n²logn) ~ O(n³) |
| 适用规模 | 大规模数据 | 小中规模 |
| 簇形状 | 凸形/球形 | 任意形状 |
| 结果稳定性 | 依赖初始化 | 确定性 |
| 可视化 | 散点图 | 树状图 |

**典型应用**:
- 客户细分（市场营销）
- 图像分割和压缩
- 文档聚类
- 异常检测
- 推荐系统
- 数据预处理（特征量化）

**实现亮点**:
- ✅ 标准K-Means算法
- ✅ K-Means++初始化
- ✅ 收敛判断（中心变化阈值）
- ✅ 轮廓系数评估
- ✅ 肘部法则可视化
- ✅ 收敛过程可视化
- ✅ 详细的训练日志

---

## 🛠️ 技术栈

- **Python**: 3.13+
- **NumPy**: 数值计算
- **Pandas**: 数据处理
- **Matplotlib**: 数据可视化
- **SciPy**: 科学计算（优化算法）
- **Scikit-learn**: 机器学习库（用于对比验证）
- **PyTorch**: 深度学习框架（2.9.0+，支持MPS加速）

## 🚀 快速开始

### 1. 克隆项目
```bash
git clone https://github.com/Kshqsz/machine-learning-lab.git
cd machine-learning-lab
```

### 2. 创建虚拟环境
```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate  # Windows
```

### 3. 安装依赖
```bash
# 基础依赖
pip install numpy pandas scipy scikit-learn matplotlib

# PyTorch (可选，用于深度学习)
pip install torch torchvision torchaudio
```

### 4. 运行示例
```bash
python linear_regression/gradient_descent.py
```

## 📝 学习笔记

### 线性回归 - 梯度下降
**核心思想**: 通过不断调整参数，使得预测值与真实值之间的误差最小化。

**损失函数**: 均方误差 (MSE)
$$L(w, b) = \frac{1}{n}\sum_{i=1}^{n}(y_i - (wx_i + b))^2$$

**参数更新**:
$$w := w - \alpha \frac{\partial L}{\partial w}$$
$$b := b - \alpha \frac{\partial L}{\partial b}$$

其中 $\alpha$ 是学习率。

---

### 感知机
**核心思想**: 线性可分数据的二分类模型，通过误分类驱动的学习算法找到分离超平面。

**模型**: 
$$f(x) = \text{sign}(w \cdot x + b)$$

**损失函数**: 误分类点到超平面的总距离
$$L(w, b) = -\sum_{x_i \in M} y_i(w \cdot x_i + b)$$

其中 $M$ 是误分类点的集合。

**原始形式更新规则**:
$$w \leftarrow w + \eta y_i x_i$$
$$b \leftarrow b + \eta y_i$$

**对偶形式表示**:
$$f(x) = \text{sign}\left(\sum_{i=1}^{N} \alpha_i y_i x_i \cdot x + b\right)$$

对偶形式的优势是可以预先计算 Gram 矩阵，当样本数远小于特征维度时更高效。

---

### k近邻法
**核心思想**: 给定一个训练数据集，对于新的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

**距离度量**: 欧氏距离
$$d(x_i, x_j) = \sqrt{\sum_{l=1}^{n}(x_i^{(l)} - x_j^{(l)})^2}$$

**k-d树构造**:
- 选择切分轴：循环选择坐标轴
- 选择切分点：选择该轴坐标的中位数
- 递归构造左右子树

**最近邻搜索**:
1. 从根节点出发，递归地向下访问k-d树
2. 若目标点当前维的坐标小于切分点，则移动到左子节点，否则移动到右子节点
3. 到达叶节点时，计算距离，更新最近点
4. 递归回退，检查是否需要在另一子树中搜索（剪枝）

**时间复杂度**:
- 构造k-d树: $O(n \log n)$
- 搜索: 平均 $O(\log n)$，最坏 $O(n)$

---

### 朴素贝叶斯
**核心思想**: 基于贝叶斯定理与特征条件独立假设，通过训练数据学习先验概率和条件概率，对新实例计算后验概率进行分类。

**贝叶斯定理**:
$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$$

**条件独立性假设**:
$$P(X=x|Y=c_k) = \prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)$$

**极大似然估计 (MLE)**:
$$P(Y=c_k) = \frac{N_{c_k}}{N}$$
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl}}{N_{c_k}}$$

**贝叶斯估计 (加一平滑)**:
$$P(Y=c_k) = \frac{N_{c_k} + \lambda}{N + K \cdot \lambda}$$
$$P(X^{(j)}=a_{jl}|Y=c_k) = \frac{N_{c_k,jl} + \lambda}{N_{c_k} + S_j \cdot \lambda}$$

其中 $\lambda \geq 0$，常取 $\lambda=1$ (拉普拉斯平滑)。

**分类决策**:
$$y = \arg\max_{c_k} P(Y=c_k) \prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)$$

---

### 决策树
**核心思想**: 通过树形结构表示决策过程，每个内部节点表示一个特征上的测试，每个分支代表测试结果，每个叶节点存放一个类标记或预测值。

**分类树 - 基尼指数**:

基尼指数表示集合的不纯度：
$$\text{Gini}(D) = 1 - \sum_{k=1}^{K} p_k^2$$

其中 $p_k$ 是样本属于第k类的概率。

特征A条件下的基尼指数：
$$\text{Gini}(D, A) = \frac{|D_1|}{|D|}\text{Gini}(D_1) + \frac{|D_2|}{|D|}\text{Gini}(D_2)$$

选择基尼指数最小的特征及其切分点。

**回归树 - 均方误差**:

划分点s处的平方误差：
$$\min_{s}\left[\min_{c_1}\sum_{x_i \in R_1(s)}(y_i - c_1)^2 + \min_{c_2}\sum_{x_i \in R_2(s)}(y_i - c_2)^2\right]$$

其中 $c_1$ 和 $c_2$ 分别是左右区域的输出值（均值）。

**停止条件**:
- 节点中样本属于同一类别
- 达到最大深度
- 样本数小于最小分裂数
- MSE/基尼指数减少量小于阈值

---

### 逻辑斯谛回归
**核心思想**: 通过 Sigmoid 函数将线性模型的输出映射到 (0,1) 区间，表示样本属于某类的概率，是一种广义线性模型。

**Sigmoid 函数**:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

性质：
- 值域为 (0, 1)，可解释为概率
- 单调递增
- $\sigma(0) = 0.5$

**二项逻辑斯谛回归模型**:

$$P(Y=1|x) = \frac{1}{1 + \exp(-(w \cdot x + b))}$$

$$P(Y=0|x) = 1 - P(Y=1|x)$$

**极大似然估计**:

似然函数：

$$L(w,b) = \prod_{i=1}^{n} [p_i]^{y_i}[1-p_i]^{1-y_i}$$

对数似然函数：

$$\log L(w,b) = \sum_{i=1}^{n}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$$

**损失函数（负对数似然）**:

$$J(w,b) = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(p_i) + (1-y_i)\log(1-p_i)]$$

这也称为交叉熵损失（Cross-Entropy Loss）。

**梯度下降更新规则**:

梯度：

$$\frac{\partial J}{\partial w} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)x_i$$

$$\frac{\partial J}{\partial b} = \frac{1}{n}\sum_{i=1}^{n}(p_i - y_i)$$

参数更新：

$$w \leftarrow w - \alpha \frac{\partial J}{\partial w}$$

$$b \leftarrow b - \alpha \frac{\partial J}{\partial b}$$

其中 $\alpha$ 是学习率。

**决策边界**:

当 $P(Y=1|x) = 0.5$ 时，即 $w \cdot x + b = 0$，这就是决策边界。

对于一维特征：

$$x = -\frac{b}{w}$$

**优点**:
- 输出具有概率意义
- 计算代价低，易于实现
- 可解释性强

**局限性**:
- 只能处理线性可分或近似线性可分的问题
- 对特征共线性敏感

---

### 多项逻辑斯谛回归
**核心思想**: 将二项逻辑斯谛回归推广到多分类问题，使用 Softmax 函数将线性输出转换为概率分布。

**Softmax 函数**:
$$P(Y=k|x) = \frac{\exp(w_k \cdot x + b_k)}{\sum_{j=1}^{K}\exp(w_j \cdot x + b_j)}$$

性质：
- 输出K个概率值，和为1
- 单调性：线性得分越高，概率越大
- 当K=2时退化为二项逻辑斯谛回归

**参数**:
对于K个类别，需要学习K组参数：
$$(w_1, b_1), (w_2, b_2), ..., (w_K, b_K)$$

**损失函数（交叉熵）**:

$$J(W,b) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K} \mathbb{1}(y_i=k) \log P(Y=k|x_i)$$

其中 $\mathbb{1}(\cdot)$ 是指示函数，当 $y_i=k$ 时为1，否则为0。

**梯度计算**:

对于第k类的参数：

$$\frac{\partial J}{\partial w_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))x_i$$

$$\frac{\partial J}{\partial b_k} = \frac{1}{n}\sum_{i=1}^{n}(P(Y=k|x_i) - \mathbb{1}(y_i=k))$$

**优化算法**:
- 梯度下降法：简单但可能较慢
- BFGS拟牛顿法：自动计算近似Hessian矩阵，收敛快
- L-BFGS：BFGS的内存优化版本

**决策规则**:
$$\hat{y} = \arg\max_k P(Y=k|x)$$

选择概率最大的类别作为预测结果。

---

### 最大熵模型
**核心思想**: 在满足约束条件的前提下，选择熵最大的概率分布。熵最大意味着对未知信息不做任何主观假设，是一种最保守的策略。

**熵的定义**:
$$H(P) = -\sum_{x,y} \tilde{P}(x) P(y|x) \log P(y|x)$$

其中 $\tilde{P}(x)$ 是经验分布。

**特征函数**:

$$
f_i(x, y) = 
\begin{cases} 
1, & \text{if } x,y \text{ satisfy certain condition} \\ 
0, & \text{otherwise}
\end{cases}
$$

特征函数表示：如果 $(x,y)$ 满足某个事实或条件，取值为1；否则为0。

**约束条件**:

模型期望 = 经验期望

$$E_P[f_i] = E_{\tilde{P}}[f_i]$$

即：

$$\sum_{x,y} \tilde{P}(x) P(y|x) f_i(x,y) = \sum_{x,y} \tilde{P}(x,y) f_i(x,y)$$

**最大熵模型**:

最优解具有指数形式：

$$P_w(y|x) = \frac{1}{Z_w(x)} \exp\left(\sum_{i=1}^{n} w_i f_i(x,y)\right)$$

归一化因子：

$$Z_w(x) = \sum_y \exp\left(\sum_{i=1}^{n} w_i f_i(x,y)\right)$$

**与多项逻辑斯谛回归的关系**:

最大熵模型在数学形式上等价于多项逻辑斯谛回归：
- 都使用 Softmax 归一化
- 都是对数线性模型
- 都用交叉熵作为损失函数

区别：
- **视角不同**: 最大熵从信息论角度（最大化熵），逻辑回归从概率角度（最大似然）
- **特征构造**: 最大熵强调特征函数 $f_i(x,y)$，逻辑回归强调特征向量 $x$

**极大似然估计**:

对偶问题：

$$\max_w \sum_{x,y} \tilde{P}(x,y) \log P_w(y|x)$$

等价于最小化负对数似然：

$$\min_w -\sum_{x,y} \tilde{P}(x,y) \log P_w(y|x)$$

**梯度**:

$$\frac{\partial L}{\partial w_i} = \sum_{x,y} \tilde{P}(x) [P_w(y|x) f_i(x,y) - \tilde{P}(y|x) f_i(x,y)]$$

简化为：

$$\frac{\partial L}{\partial w_i} = E_P[f_i] - E_{\tilde{P}}[f_i]$$

即模型期望与经验期望的差。

**优化算法**:
- 梯度下降法 (GD)
- 拟牛顿法 (BFGS)
- 改进的迭代尺度法 (IIS)
- 通用迭代尺度法 (GIS)

**应用场景**:
- 自然语言处理（词性标注、命名实体识别）
- 文本分类
- 信息抽取
- 机器翻译

**优势**:
- 特征灵活，可以组合任意特征
- 理论基础扎实（最大熵原理）
- 可解释性强
- 不需要特征独立性假设（相比朴素贝叶斯）

---

### 支持向量机
**核心思想**: 在特征空间中找到间隔最大的分离超平面。支持向量机基于结构风险最小化原则，具有很强的泛化能力。

**原始优化问题（硬间隔）**:

$$\min_{w,b} \frac{1}{2}\|w\|^2$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1, \quad i=1,2,...,N$$

目标是最小化 $\|w\|^2$，等价于最大化间隔 $\frac{2}{\|w\|}$。

**拉格朗日函数**:

引入拉格朗日乘子 $\alpha_i \geq 0$：

$$L(w,b,\alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^{N}\alpha_i[y_i(w \cdot x_i + b) - 1]$$

**对偶问题**:

通过对 $w$ 和 $b$ 求偏导并令其为零：

$$w = \sum_{i=1}^{N}\alpha_i y_i x_i$$

$$\sum_{i=1}^{N}\alpha_i y_i = 0$$

代入拉格朗日函数得到对偶问题：

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0, \quad \alpha_i \geq 0, \quad i=1,2,...,N$$

**KKT条件**:

$$\alpha_i \geq 0$$

$$y_i(w \cdot x_i + b) - 1 \geq 0$$

$$\alpha_i[y_i(w \cdot x_i + b) - 1] = 0$$

**支持向量**:

满足 $\alpha_i > 0$ 的样本称为支持向量，这些样本位于间隔边界上：

$$y_i(w \cdot x_i + b) = 1$$

非支持向量的 $\alpha_i = 0$，对模型没有影响。

**参数恢复**:

从最优解 $\alpha^*$ 恢复参数：

权重向量：

$$w^* = \sum_{i=1}^{N}\alpha_i^* y_i x_i$$

偏置（选择任一支持向量 $x_s$）：

$$b^* = y_s - w^* \cdot x_s = y_s - \sum_{i=1}^{N}\alpha_i^* y_i (x_i \cdot x_s)$$

**决策函数**:

$$
f(x) = \text{sign}(w^* \cdot x + b^*)
$$

或者用对偶形式表示：

$$
f(x) = \text{sign}\left(\sum_{i=1}^{N}\alpha_i^* y_i (x_i \cdot x) + b^*\right)
$$

**几何间隔**:

分离超平面到最近样本点的距离：

$$\gamma = \frac{1}{\|w^*\|}$$

最大间隔（两个间隔边界之间的距离）：

$$\text{margin} = \frac{2}{\|w^*\|}$$

**对偶形式的优势**:
1. **简化计算**: 对偶问题只依赖于样本的内积 $(x_i \cdot x_j)$
2. **引入核函数**: 可以用核函数 $K(x_i, x_j)$ 替代内积，实现非线性分类
3. **稀疏性**: 只有支持向量的 $\alpha_i > 0$，其余为0
4. **样本数 vs 特征数**: 当样本数 $N$ 远小于特征维度 $d$ 时，对偶形式更高效

**求解方法**:
- 二次规划（QP）：SLSQP, 内点法
- SMO算法（Sequential Minimal Optimization）：专门针对SVM的高效算法
- 坐标上升法

**线性可分的条件**:
训练数据集线性可分的充要条件是对偶问题有解且存在 $\alpha^*$ 使得：

$$\sum_{i=1}^{N}\alpha_i^* y_i = 0, \quad \alpha_i^* \geq 0$$

**优势**:
- 最大间隔准则，泛化能力强
- 只依赖支持向量，模型稀疏
- 理论基础完善（统计学习理论）
- 通过核技巧可处理非线性问题

**局限性**:
- 只适用于二分类（多分类需要组合策略）
- 对大规模数据集计算开销大
- 对参数和核函数的选择敏感

---

### 线性支持向量机 - 软间隔
**核心思想**: 引入松弛变量允许部分样本不满足硬间隔约束，使SVM能够处理线性不可分和含噪声的数据。

**原始问题（软间隔）**:

$$\min_{w,b,\xi} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{N}\xi_i$$

约束条件：

$$y_i(w \cdot x_i + b) \geq 1 - \xi_i$$

$$\xi_i \geq 0, \quad i=1,2,...,N$$

其中：
- $\xi_i$ 是松弛变量，度量样本违反硬间隔约束的程度
- $C > 0$ 是惩罚参数，控制间隔最大化和误分类的权衡

**目标函数的两部分**:
1. $\frac{1}{2}\|w\|^2$：最大化间隔
2. $C\sum_{i=1}^{N}\xi_i$：最小化违反约束的程度

**松弛变量的意义**:

$$\xi_i = \begin{cases}
0, & y_i(w \cdot x_i + b) \geq 1 \quad \text{(正确分类且在间隔外)} \\
1 - y_i(w \cdot x_i + b), & 0 < y_i(w \cdot x_i + b) < 1 \quad \text{(在间隔内)} \\
1 + |w \cdot x_i + b|, & y_i(w \cdot x_i + b) < 0 \quad \text{(误分类)}
\end{cases}$$

**对偶问题**:

形式与硬间隔相同，但约束条件改变：

$$\max_{\alpha} \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_j y_i y_j (x_i \cdot x_j)$$

约束条件：

$$\sum_{i=1}^{N}\alpha_i y_i = 0$$

$$0 \leq \alpha_i \leq C, \quad i=1,2,...,N$$

**关键区别**：上界约束 $\alpha_i \leq C$（箱约束/盒约束）

**KKT条件（软间隔）**:

$$\alpha_i \geq 0$$

$$y_i(w \cdot x_i + b) - 1 + \xi_i \geq 0$$

$$\alpha_i[y_i(w \cdot x_i + b) - 1 + \xi_i] = 0$$

$$C - \alpha_i - \mu_i = 0, \quad \mu_i \geq 0$$

$$\mu_i \xi_i = 0$$

**支持向量的三种类型**:

根据 $\alpha_i$ 的值，样本分为三类：

1. **α_i = 0（非支持向量）**:
   - ξ_i = 0，样本在间隔外正确分类
   - $y_i(w \cdot x_i + b) > 1$
   - 对模型没有贡献

2. **0 < α_i < C（边界支持向量）**:
   - ξ_i = 0，μ_i > 0
   - 样本恰好在间隔边界上
   - $y_i(w \cdot x_i + b) = 1$
   - 用于计算偏置 $b$

3. **α_i = C（内部支持向量）**:
   - μ_i = 0，ξ_i > 0
   - 样本在间隔内或被误分类
   - $y_i(w \cdot x_i + b) < 1$
   - 违反了硬间隔约束

**惩罚参数C的作用**:

C控制对误分类的容忍度：

- **C → ∞**：不允许违反约束，退化为硬间隔SVM
  - 优点：完美拟合训练数据
  - 缺点：对噪声敏感，可能过拟合

- **C很小**：允许较多违反约束
  - 优点：间隔大，泛化能力可能更好
  - 缺点：训练误差大，可能欠拟合

- **C适中**：平衡间隔和误差
  - 最佳选择，通常通过交叉验证确定

**参数选择**:

偏置的计算应使用边界支持向量 (0 < α_i < C)：

$$b = y_s - \sum_{i=1}^{N}\alpha_i y_i (x_i \cdot x_s)$$

为了稳定性，使用所有边界支持向量的平均值：

$$b = \frac{1}{|S|}\sum_{s \in S}\left(y_s - \sum_{i=1}^{N}\alpha_i y_i (x_i \cdot x_s)\right)$$

其中集合 S 定义为所有边界支持向量的索引：

$$S = \{i: 0 < \alpha_i < C\}$$

**软间隔 vs 硬间隔**:

| 特性 | 硬间隔 | 软间隔 |
|------|--------|--------|
| 适用数据 | 线性可分 | 线性可分/不可分 |
| 约束条件 | $\alpha_i \geq 0$ | $0 \leq \alpha_i \leq C$ |
| 松弛变量 | 无 | $\xi_i \geq 0$ |
| 对噪声 | 敏感 | 鲁棒 |
| 间隔 | 固定最大 | 可调整 |
| 参数 | 无 | C |

**模型选择**:

C的选择方法：
1. 交叉验证：在验证集上测试不同C值
2. 网格搜索：在对数尺度上搜索（如 $10^{-3}, 10^{-2}, ..., 10^3$）
3. 经验规则：从 $C=1$ 开始调整

**优势**:
- 处理线性不可分数据
- 对噪声和异常值鲁棒
- 灵活控制模型复杂度
- 理论基础完善（结构风险最小化 + 经验风险最小化）

**应用**:
- 真实数据往往含噪声，软间隔更实用
- 文本分类、图像识别等实际问题
- 需要在拟合和泛化之间权衡的场景

---

### AdaBoost 提升方法
**核心思想**: 通过迭代地训练弱分类器，并调整样本权重使后续分类器更关注难分样本，最终将多个弱分类器组合成强分类器。

**算法框架**:

1. **初始化权重**: $D_1 = (w_{11}, ..., w_{1N}), \quad w_{1i} = \frac{1}{N}$

2. **迭代训练** (m = 1, 2, ..., M):
   - 训练弱分类器 $G_m(x)$
   - 计算加权错误率 $e_m = \sum_{i=1}^{N} w_{mi} \mathbb{I}(G_m(x_i) \neq y_i)$
   - 计算分类器权重 $\alpha_m = \frac{1}{2} \ln \frac{1 - e_m}{e_m}$
   - 更新样本权重 $w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i))$

3. **构建强分类器**: $f(x) = \text{sign}\left(\sum_{m=1}^{M} \alpha_m G_m(x)\right)$

**权重更新机制**:
- 分类正确的样本：权重减小（$e^{-\alpha_m}$）
- 分类错误的样本：权重增大（$e^{\alpha_m}$）
- 使得下一轮更关注被误分的样本

**决策树桩** (Decision Stump):

最简单的弱分类器，单层决策树：

$$
G(x) = 
\begin{cases}
+1, & \text{if } p \cdot x < p \cdot \text{threshold} \\
-1, & \text{otherwise}
\end{cases}
$$

**理论基础 - 前向分步算法**:

AdaBoost是前向分步加法模型的特例，使用指数损失函数：

$$L(y, f(x)) = \exp(-y f(x))$$

每一步优化：

$$(\alpha_m, G_m) = \arg\min_{\alpha, G} \sum_{i=1}^{N} \exp\left(-y_i \left(f_{m-1}(x_i) + \alpha G(x_i)\right)\right)$$

**训练误差界**:

$$\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}(f(x_i) \neq y_i) \leq \prod_{m=1}^{M} \sqrt{1 - 4\gamma_m^2}$$

其中 $\gamma_m = 0.5 - e_m$，只要 $e_m < 0.5$，训练误差指数下降。

**优点**:
- 精度高，能显著提升弱分类器性能
- 不需要事先知道弱分类器的错误率
- 简单易实现，不需要复杂参数调优
- 可以识别重要样本和特征

**缺点**:
- 对噪声和异常值敏感（可能过拟合噪声点）
- 训练时间较长（串行训练）
- 需要足够多的训练数据

**应用**:
- 人脸检测（Viola-Jones算法）
- 文本分类和情感分析
- 医疗诊断
- 特征选择和排序

---

### 前向分步算法
**核心思想**: 通用的加法模型学习框架，AdaBoost是其使用指数损失的特例。

**加法模型**:

$$f(x) = \sum_{m=1}^{M} \beta_m b(x; \gamma_m)$$

其中 $b(x; \gamma_m)$ 是基函数，$\beta_m$ 是系数。

**算法步骤**:

1. **初始化**: $f_0(x) = 0$

2. **迭代** (m = 1 到 M):
   - 极小化损失：$(\beta_m, \gamma_m) = \arg\min_{\beta, \gamma} \sum_{i=1}^{N} L(y_i, f_{m-1}(x_i) + \beta \cdot b(x_i; \gamma))$
   - 更新模型：$f_m(x) = f_{m-1}(x) + \beta_m \cdot b(x; \gamma_m)$

3. **输出**: $f(x) = f_M(x)$

**关键性质**:
- **前向**：从前向后逐步学习
- **分步**：每次只优化一个基函数，简化优化
- **通用**：支持不同损失函数

**两种常用损失函数**:

**指数损失** (AdaBoost):
$$L(y, f(x)) = \exp(-y f(x))$$

特点：对误分类样本惩罚呈指数增长，权重调整机制

**平方损失** (类似GBDT):
$$L(y, f(x)) = (y - f(x))^2$$

特点：通过拟合残差学习，对噪声相对鲁棒

**前向分步算法 = AdaBoost 证明**:

使用指数损失时，第m步极小化：

$$\sum_{i=1}^{N} \exp(-y_i (f_{m-1}(x_i) + \alpha G(x_i)))$$

设 $w_i = \exp(-y_i f_{m-1}(x_i))$，分离正确和错误样本后求导，得到：

$$\alpha^* = \frac{1}{2} \ln \frac{1-e}{e}$$

这正是AdaBoost的 $\alpha_m$ 公式！

**意义**:
- 揭示AdaBoost的理论本质
- 提供统一的集成学习框架
- 可扩展到其他损失函数（Logistic损失、Hinge损失等）

---

### GBDT (梯度提升决策树)
**核心思想**: 通过拟合损失函数的**负梯度**逐步提升模型，每一步用回归树拟合残差。

**算法框架**:

1. **初始化**: $f_0(x) = \arg\min_c \sum_{i=1}^{N} L(y_i, c)$ (常数，对MSE就是均值)

2. **迭代** (m = 1 到 M):
   - 计算负梯度（伪残差）：$r_{im} = -\left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right]_{f=f_{m-1}}$
   - 拟合回归树：$T(x; \theta_m) = \arg\min_\theta \sum_{i=1}^{N} (r_{im} - T(x_i; \theta))^2$
   - 更新模型：$f_m(x) = f_{m-1}(x) + \nu \cdot T(x; \theta_m)$ （$\nu$是学习率）

3. **输出**: $f(x) = f_M(x)$

**平方损失函数**:
$$L(y, f(x)) = \frac{1}{2}(y - f(x))^2$$

负梯度：$r = y - f(x)$ （就是残差！）

**关键参数**:
- **n_estimators**: 树的数量（M）
- **learning_rate**: 学习率（$\nu$），控制每棵树的贡献
- **max_depth**: 树的最大深度，控制模型复杂度

**GBDT vs AdaBoost**:
- **损失函数**: GBDT支持多种损失（MSE、绝对误差、Huber等），AdaBoost固定指数损失
- **更新方式**: GBDT拟合负梯度，AdaBoost调整样本权重
- **应用场景**: GBDT更适合回归和排序，AdaBoost更适合二分类
- **鲁棒性**: GBDT对异常值更鲁棒（尤其使用绝对误差）

**实现要点**:
- CART回归树：递归分裂，MSE最小化准则
- 学习率权衡：小学习率+多棵树 = 更好泛化
- 正则化：限制树深度、最小样本数防止过拟合

---

### 隐马尔可夫模型 (HMM)
**核心思想**: 描述由隐藏的马尔可夫链生成不可观测的状态序列，再由各状态生成观测序列的过程。

**模型三要素** λ = (A, B, π):

1. **状态转移概率矩阵** A: 
   $$a_{ij} = P(q_{t+1}=s_j|q_t=s_i)$$

2. **观测概率矩阵** B:
   $$b_j(k) = P(o_t=v_k|q_t=s_j)$$

3. **初始状态概率** π:
   $$\pi_i = P(q_1=s_i)$$

**两个基本假设**:

- **齐次马尔可夫假设**: $P(q_t|q_{t-1}, \ldots, q_1) = P(q_t|q_{t-1})$
- **观测独立性假设**: $P(o_t|q_t, o_{t-1}, \ldots, o_1) = P(o_t|q_t)$

**观测序列生成算法**:

1. 按初始概率 π 生成状态 q₁
2. 按观测概率 b_{q_t}(k) 生成观测 o_t
3. 按转移概率 a_{q_t,j} 生成下一状态 q_{t+1}
4. 重复步骤2-3直到序列完成

**HMM的三个基本问题**:

1. **概率计算**（前向-后向算法）: 给定λ和O，计算P(O|λ)
2. **学习问题**（Baum-Welch算法）: 给定O，估计参数λ
3. **预测问题**（维特比算法）: 给定λ和O，求最优状态序列

**典型应用**:
- 语音识别（状态=音素，观测=声学特征）
- 词性标注（状态=词性，观测=单词）
- 生物信息学（基因序列分析）
- 金融时序分析

**关键特点**:
- 状态不可直接观测（隐藏）
- 观测值由状态概率性生成
- 状态转移满足马尔可夫性
- 是时序数据建模的重要工具

**前向算法**（概率计算问题）:

前向概率：$\alpha_t(i) = P(o_1,\ldots,o_t, q_t=s_i|\lambda)$

算法步骤：
1. 初始化：$\alpha_1(i) = \pi_i \cdot b_i(o_1)$
2. 递推：$\alpha_{t+1}(i) = [\sum_j \alpha_t(j) \cdot a_{ji}] \cdot b_i(o_{t+1})$
3. 终止：$P(O|\lambda) = \sum_i \alpha_T(i)$

特点：从前向后计算，时间复杂度O(N²T)

**后向算法**（概率计算问题）:

后向概率：$\beta_t(i) = P(o_{t+1},\ldots,o_T|q_t=s_i, \lambda)$

算法步骤：
1. 初始化：$\beta_T(i) = 1$
2. 递推：$\beta_t(i) = \sum_j a_{ij} \cdot b_j(o_{t+1}) \cdot \beta_{t+1}(j)$
3. 终止：$P(O|\lambda) = \sum_i \pi_i \cdot b_i(o_1) \cdot \beta_1(i)$

特点：从后向前计算，与前向算法结果相同

**前向-后向算法的关系**:
- 计算方向相反，结果一致
- 结合使用：$P(O|\lambda) = \sum_i \alpha_t(i) \cdot \beta_t(i)$ （任意t）
- 应用：Baum-Welch算法需要同时使用两者

**Baum-Welch算法**（参数学习问题）:

本质：**EM算法**在HMM中的应用

核心变量：
- $\gamma_t(i) = P(q_t=s_i|O,\lambda)$：时刻t处于状态i的概率
- $\xi_t(i,j) = P(q_t=s_i,q_{t+1}=s_j|O,\lambda)$：状态转移的概率

计算公式：
- $\gamma_t(i) = \frac{\alpha_t(i) \cdot \beta_t(i)}{\sum_j \alpha_t(j) \cdot \beta_t(j)}$
- $\xi_t(i,j) = \frac{\alpha_t(i) \cdot a_{ij} \cdot b_j(o_{t+1}) \cdot \beta_{t+1}(j)}{\sum_{i,j} \alpha_t(i) \cdot a_{ij} \cdot b_j(o_{t+1}) \cdot \beta_{t+1}(j)}$

参数更新：
1. $\pi_i = \gamma_1(i)$
2. $a_{ij} = \frac{\sum_t \xi_t(i,j)}{\sum_t \gamma_t(i)}$
3. $b_j(k) = \frac{\sum_{t,o_t=v_k} \gamma_t(j)}{\sum_t \gamma_t(j)}$

特点：
- 无监督学习，只需观测序列
- EM算法保证似然单调增
- 收敛到局部最优解
- 依赖初始化

**Viterbi算法**（状态预测问题）:

目标：给定λ和O，求最可能的状态序列Q*

核心变量：
- $\delta_t(i)$：到时刻t状态为i的所有路径中概率最大值
- $\psi_t(i)$：时刻t状态为i时的最优前驱状态

算法步骤：
1. **初始化**: $\delta_1(i) = \pi_i \cdot b_i(o_1)$, $\psi_1(i) = 0$
2. **递推**: $\delta_t(i) = \max_j[\delta_{t-1}(j) \cdot a_{ji}] \cdot b_i(o_t)$
            $\psi_t(i) = \arg\max_j[\delta_{t-1}(j) \cdot a_{ji}]$
3. **终止**: $P^* = \max_i \delta_T(i)$, $q_T^* = \arg\max_i \delta_T(i)$
4. **回溯**: $q_t^* = \psi_{t+1}(q_{t+1}^*)$, t=T-1,...,1

特点：
- 动态规划，时间复杂度O(N²T)
- 找到全局最优路径
- 与前向算法关系：P(O|λ) ≥ P*（所有路径和 ≥ 最优路径）

**HMM三个基本问题对比**:

| 问题 | 算法 | 输入 | 输出 | 操作 | 应用 |
|------|------|------|------|------|------|
| 概率计算 | 前向/后向 | λ,O | P(O\|λ) | 求和Σ | 模型评估 |
| 参数学习 | Baum-Welch | O | λ | EM迭代 | 无监督学习 |
| 状态预测 | Viterbi | λ,O | Q* | 求最大max | 序列标注 |

---

### 条件随机场 (CRF)
**核心思想**: 判别式序列标注模型，给定观测序列X，直接对标签序列Y建模条件概率P(Y|X)。相比HMM，CRF无需观测独立性假设，可以灵活使用任意全局特征。

**模型定义**: 线性链CRF

$$P(y|x) = \frac{1}{Z(x)} \exp\left(\sum_{t=1}^{T} \sum_k w_k f_k(y_{t-1}, y_t, x, t)\right)$$

其中:
- $f_k$: 特征函数（转移特征+状态特征）
- $w_k$: 特征权重
- $Z(x) = \sum_y \exp(\sum w_k f_k)$: 归一化因子

**CRF的三个基本问题**:

1. **推断问题**（前向-后向算法）: 给定w和x，计算Z(x)和边缘概率P(y_t|x)
2. **学习问题**（BFGS算法）: 给定训练数据(x,y)，学习最优权重w*
3. **解码问题**（Viterbi算法）: 给定w和x，求最优标签序列y*

**前向-后向算法**（推断问题）:

前向变量（对数空间）: 
$$\alpha_t(y) = \log \sum_{y'} \exp(\alpha_{t-1}(y') + \psi_t(y',y|x))$$

后向变量:
$$\beta_t(y) = \log \sum_{y'} \exp(\psi_{t+1}(y,y'|x) + \beta_{t+1}(y'))$$

边缘概率:
$$P(y_t|x) = \frac{\exp(\alpha_t(y_t) + \beta_t(y_t))}{Z(x)}$$

其中 $\psi_t$ 是局部势函数，$Z(x) = \text{logsumexp}_y \alpha_T(y)$

**BFGS训练算法**（学习问题）:

目标函数（负对数似然+L2正则）:
$$L(w) = -\sum_i \left[\sum_k w_k F_k(x^{(i)}, y^{(i)}) - \log Z(x^{(i)})\right] + \frac{\lambda}{2}||w||^2$$

梯度:
$$\frac{\partial L}{\partial w_k} = -\sum_i [\text{经验特征计数} - \text{期望特征计数}] + \lambda w_k$$

期望特征计数通过前向-后向算法计算:
$$E[F_k] = \sum_t \sum_{y',y} P(y_{t-1}=y',y_t=y|x) \cdot f_k(y',y,x,t)$$

**Viterbi算法**（解码问题）:

与HMM类似，但使用CRF的局部势函数:

1. **初始化**: $\delta_1(y) = \psi_1(\text{START}, y|x)$
2. **递推**: $\delta_t(y) = \max_{y'} [\delta_{t-1}(y') + \psi_t(y',y|x)]$
3. **终止**: $y_T^* = \arg\max_y \delta_T(y)$
4. **回溯**: 沿着最优路径回溯得到 $y^*$

**典型应用**:
- 中文分词（BMES标注）
- 命名实体识别（BIO标注）
- 词性标注
- 语义角色标注

**CRF vs HMM**:

| 特性 | HMM | CRF |
|------|-----|-----|
| 模型类型 | 生成式 | 判别式 |
| 建模目标 | P(X,Y) | P(Y\|X) |
| 独立性假设 | 需要观测独立性 | 无需观测独立性 |
| 特征灵活性 | 局限 | 可用任意全局特征 |
| 训练效率 | 快（EM） | 慢（梯度优化） |
| 精度 | 较低 | 较高 |

**关键优势**:
- 突破观测独立性假设限制
- 可利用丰富的上下文特征
- 判别式训练，序列标注任务精度更高
- 是自然语言处理中的重要基础模型

**CRF三个基本问题对比**:

| 问题 | 算法 | 输入 | 输出 | 操作 | 应用 |
|------|------|------|------|------|------|
| 推断计算 | 前向-后向 | w,x | Z(x), P(y_t\|x) | logsumexp | 概率计算 |
| 参数学习 | BFGS | (x,y) | w* | 梯度优化 | 监督学习 |
| 序列解码 | Viterbi | w,x | y* | 求最大max | 序列标注 |

---

### 层次聚类 (Hierarchical Clustering)
**核心思想**: 通过计算数据点间的相似度创建有层次的嵌套聚类树。凝聚层次聚类采用自底向上的合并策略，从每个样本作为单独的簇开始，逐步合并最相似的簇，直到达到期望的簇数量。

**算法类型**:
1. **凝聚式聚类** (Agglomerative): 自底向上合并（本实现）
2. **分裂式聚类** (Divisive): 自顶向下分裂

**算法步骤** (凝聚式):

1. **初始化**: 每个样本作为一个簇，共n个簇
2. **计算距离**: 计算所有簇对之间的距离矩阵
3. **合并簇**: 找到距离最近的两个簇并合并
4. **更新距离**: 重新计算涉及新簇的距离
5. **重复**: 重复步骤3-4直到达到目标簇数量

**链接方法** (计算簇间距离):

1. **单链接** (Single Linkage):
   $$d(C_i, C_j) = \min_{x \in C_i, y \in C_j} d(x, y)$$
   - 特点：最近邻距离，容易产生"链式"效应
   - 适用：识别长链状或不规则形状的簇

2. **全链接** (Complete Linkage):
   $$d(C_i, C_j) = \max_{x \in C_i, y \in C_j} d(x, y)$$
   - 特点：最远邻距离，倾向于形成紧凑的球形簇
   - 适用：簇大小相近且形状规则的情况

3. **平均链接** (Average Linkage):
   $$d(C_i, C_j) = \frac{1}{|C_i| \cdot |C_j|} \sum_{x \in C_i} \sum_{y \in C_j} d(x, y)$$
   - 特点：平均距离，折中方案
   - 适用：一般情况，较为稳健

4. **Ward方法**:
   $$d(C_i, C_j) = \sqrt{\frac{|C_i| \cdot |C_j|}{|C_i| + |C_j|}} \|\mu_i - \mu_j\|$$
   - 特点：最小化类内方差增量
   - 适用：簇大小相近的情况，常用于实践

**距离度量**:

- **欧几里得距离**: $d(x, y) = \sqrt{\sum(x_i - y_i)^2}$
- **曼哈顿距离**: $d(x, y) = \sum|x_i - y_i|$
- **余弦距离**: $d(x, y) = 1 - \frac{x \cdot y}{\|x\| \|y\|}$

**树状图** (Dendrogram):
- 可视化层次聚类的合并过程
- 纵轴表示合并时的距离
- 横轴表示样本索引
- 通过在不同高度"切割"树来得到不同数量的簇

**选择最佳簇数**:
1. **肘部法则**: 观察最大合并距离的变化
2. **树状图**: 在距离突变处切割
3. **轮廓系数**: 评估聚类质量

**运行示例**:
```bash
python clustering/hierarchical_clustering.py
```

**输出内容**:
- 示例1: 四种链接方法比较（single, complete, average, ward）
  * 60个样本的3簇数据
  * 聚类结果散点图
  * 树状图可视化
  * 保存为 `hierarchical_clustering_comparison.png`

- 示例2: 三种距离度量比较（euclidean, manhattan, cosine）
  * 30个样本的2簇数据
  * 使用average linkage
  * 保存为 `hierarchical_distance_metrics.png`

- 示例3: 肘部法则选择最佳簇数
  * 测试簇数从2到10
  * 绘制肘部曲线
  * 展示最佳簇数(k=3)的聚类结果
  * 保存为 `hierarchical_elbow_method.png`

**算法特点**:

优点:
- 不需要预先指定簇数量
- 可以发现任意形状的簇（取决于链接方法）
- 提供完整的聚类层次结构
- 结果可解释性强（树状图）

缺点:
- 时间复杂度 $O(n^2 \log n)$ 到 $O(n^3)$，不适合大规模数据
- 一旦合并无法撤销（贪心算法）
- 对噪声和异常值敏感
- 需要选择合适的链接方法和距离度量

**典型应用**:
- 生物信息学（基因/物种分类）
- 文档聚类和主题发现
- 图像分割
- 社交网络分析
- 客户细分

---

### K-Means聚类
**核心思想**: 基于划分的聚类算法，通过迭代优化将数据分配到K个簇，最小化簇内平方误差（SSE）。

**目标函数**: 
$$J = \sum_{k=1}^{K} \sum_{x \in C_k} \|x - \mu_k\|^2$$

**算法步骤**:
```
1. 初始化: 选择K个初始中心
2. 分配: 每个样本→最近的中心
3. 更新: 重新计算簇中心（均值）
4. 重复: 直到收敛
```

**初始化方法**:

| 方法 | 描述 | 特点 |
|------|------|------|
| Random | 随机选择K个点 | 简单但不稳定 |
| K-Means++ | 选择相距较远的初始点 | 收敛快，质量好（推荐）|

**K-Means++算法**:
```
1. 随机选第一个中心
2. 对每个点x，计算D(x)=到最近中心的距离
3. 以概率 D(x)²/ΣD(x)² 选下一个中心
4. 重复至选出K个中心
```

**评估指标**:
- **SSE/Inertia**: $\sum\sum \|x-\mu_k\|^2$，越小越好
- **轮廓系数**: $s(i) = \frac{b(i)-a(i)}{\max(a(i),b(i))}$，[-1,1]，越接近1越好
  * a(i): 簇内平均距离
  * b(i): 到最近其他簇的平均距离

**选择最佳K**:
1. 肘部法则：SSE曲线的"肘部"
2. 轮廓系数法：最大轮廓系数对应的K

**复杂度**: O(nkt)（n样本，k簇数，t迭代次数）

**优缺点**:

优点:
- 简单高效，适合大规模数据
- 收敛快（通常几十次迭代）
- 空间复杂度低 O(n)

缺点:
- 需预先指定K值
- 只能发现凸形簇
- 对初始化和异常值敏感
- 可能陷入局部最优

**vs 层次聚类**:

| 特性 | K-Means | 层次聚类 |
|------|---------|----------|
| 类型 | 划分式 | 层次式 |
| 复杂度 | O(nkt) | O(n²logn)~O(n³) |
| 规模 | 大规模 | 小中规模 |
| 簇形状 | 球形 | 任意 |
| K值 | 需预先指定 | 事后选择 |

**典型应用**:
- 客户细分
- 图像压缩/分割
- 文档聚类
- 异常检测

---

### 奇异值分解 (SVD - Singular Value Decomposition)
**核心思想**: SVD是线性代数中的重要矩阵分解方法，将任意m×n矩阵分解为三个矩阵的乘积，揭示矩阵的内在结构和最重要的特征。

**数学定义**: 对于任意矩阵 $A_{m \times n}$，存在分解：

$$A = U \Sigma V^T$$

其中:
- $U$: $m \times m$ 正交矩阵，列向量是 $AA^T$ 的特征向量（左奇异向量）
- $\Sigma$: $m \times n$ 对角矩阵，对角元素是奇异值 $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_r > 0$
- $V$: $n \times n$ 正交矩阵，列向量是 $A^T A$ 的特征向量（右奇异向量）

**关键性质**:

1. **正交性**:
   - $U^T U = I_{m}$（单位矩阵）
   - $V^T V = I_{n}$（单位矩阵）

2. **奇异值排序**:
   - $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_r > 0$
   - $r = rank(A)$（矩阵的秩）

3. **能量集中**: 前几个奇异值通常包含了矩阵的主要信息

**截断SVD（低秩近似）**:

$$A \approx A_k = U_k \Sigma_k V_k^T$$

保留前k个最大的奇异值，得到A的最佳k-秩近似（Frobenius范数意义下）：

$$\min_{rank(B)=k} \|A - B\|_F = \|A - A_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}$$

**算法步骤**:

```
1. 计算 A^T A 或 AA^T
2. 求特征值和特征向量
3. 奇异值 σᵢ = √λᵢ（λᵢ是特征值）
4. 构造 U, Σ, V^T
5. （可选）截断：保留前k个成分
```

**评估指标**:

1. **重建误差**（Frobenius范数）:
   $$\text{Error} = \|A - A_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}$$

2. **方差解释比例**:
   $$\text{Explained Variance}_i = \frac{\sigma_i^2}{\sum_j \sigma_j^2}$$

3. **压缩比**:
   $$\text{Compression Ratio} = \frac{m \times n}{m \times k + k + k \times n}$$

4. **PSNR**（峰值信噪比，用于图像）:
   $$\text{PSNR} = 10 \log_{10} \frac{\text{MAX}^2}{\text{MSE}}$$

**运行示例**:
```bash
python svd/svd_decomposition.py
```

**输出内容**:

- **示例1**: 基础SVD分解
  * 4×3矩阵的完整分解
  * 可视化U, Σ, V^T矩阵
  * 验证重建误差
  * 保存为 `svd_basic_decomposition.png`

- **示例2**: 截断SVD与低秩近似
  * 50×30矩阵的不同截断等级（k=1,3,5,10,15,20,30）
  * 重建误差 vs k曲线
  * 压缩比 vs k曲线
  * 奇异值谱（对数尺度）
  * 保存为 `svd_truncated_approximation.png`

- **示例3**: 图像压缩应用
  * 100×100合成图像
  * 不同压缩等级（k=1,5,10,20,50）的视觉效果
  * 压缩质量分析（误差、PSNR、压缩比）
  * 保存为 `svd_image_compression.png` 和 `svd_compression_analysis.png`

- **示例4**: 数据降维与可视化
  * 200样本×50特征的高维数据
  * 降维到10维主成分
  * 奇异值谱、方差解释比例、累积方差
  * 2D/3D可视化（前3个主成分）
  * 保存为 `svd_dimensionality_reduction.png`

**算法特点**:

优点:
- 理论保证：最优低秩近似（Frobenius范数）
- 揭示数据内在结构和主要模式
- 数值稳定性好
- 应用广泛（降维、压缩、去噪等）
- 可以处理任意矩阵（不要求方阵）

缺点:
- 时间复杂度高：O(min(mn², m²n))
- 空间复杂度：O(mn)
- 不适合大规模稀疏矩阵（可用随机SVD）
- 对缺失数据敏感
- 需要整个矩阵存储在内存中

**复杂度分析**:

| 操作 | 时间复杂度 | 空间复杂度 |
|------|-----------|-----------|
| 完整SVD | O(min(mn², m²n)) | O(mn) |
| 截断SVD (k成分) | O(mnk) | O(mk + kn) |
| 重建 | O(mnk) | O(mn) |

**与PCA的关系**:

PCA本质上是SVD的一个应用：
- PCA: 对中心化数据的协方差矩阵进行特征分解
- SVD: 直接对数据矩阵进行奇异值分解
- 结果等价：PCA的主成分 = SVD的右奇异向量

**SVD vs 特征分解**:

| 特性 | SVD | 特征分解 |
|------|-----|---------|
| 适用矩阵 | 任意m×n矩阵 | 仅方阵 |
| 数值稳定性 | 好 | 依赖矩阵条件 |
| 正交性 | 保证U,V正交 | 不一定正交 |
| 应用范围 | 更广 | 受限 |

**典型应用**:

1. **降维与可视化**:
   - 主成分分析（PCA）
   - 保留最重要的特征
   - 数据可视化

2. **图像处理**:
   - 图像压缩（JPEG算法基础）
   - 图像去噪
   - 图像修复

3. **推荐系统**:
   - 协同过滤
   - 矩阵补全
   - 潜在因子模型

4. **自然语言处理**:
   - 潜在语义分析（LSA）
   - 文档-词矩阵分解
   - 词向量降维

5. **信号处理**:
   - 信号去噪
   - 特征提取
   - 压缩感知

**实现亮点**:
- ✅ 完整SVD和截断SVD
- ✅ 低秩近似和重建
- ✅ 压缩比计算
- ✅ 多种评估指标（误差、方差解释比、PSNR）
- ✅ 图像压缩应用
- ✅ 数据降维可视化（2D/3D）
- ✅ 详细的统计分析

---

### 主成分分析 (PCA - Principal Component Analysis)
**核心思想**: 通过线性变换将数据投影到方差最大的方向（主成分），实现降维和特征提取。

**基本流程**:
```
1. 中心化: X_c = X - mean(X)
2. 协方差矩阵: C = (1/(n-1)) * X_c^T * X_c
3. 特征分解: C = V Λ V^T
4. 投影: X_reduced = X_c * V_k
5. 重建: X_reconstructed = X_reduced * V_k^T + mean(X)
```

**两种实现方法**:

1. **特征分解方法**:
   - 计算协方差矩阵 $C = \frac{1}{n-1} X_c^T X_c$
   - 特征分解得到特征值 $\lambda_i$ 和特征向量 $v_i$
   - 主成分 = 特征向量

2. **SVD方法**（推荐）:
   - 对中心化数据 $X_c = U \Sigma V^T$
   - 主成分 = V的列向量
   - 解释方差 = $\sigma_i^2 / (n-1)$
   - 数值更稳定

**评估指标**:

1. **解释方差比例**:
   $$\text{Explained Variance Ratio}_i = \frac{\lambda_i}{\sum_j \lambda_j}$$

2. **累积方差**（选择主成分数量）:
   $$\text{Cumulative Variance} = \sum_{i=1}^{k} \frac{\lambda_i}{\sum_j \lambda_j}$$
   - 通常保留95%或99%的方差

3. **重建误差**:
   $$\text{Error} = \|X - X_{reconstructed}\|_F$$

**核PCA** (Kernel PCA):

对非线性数据，使用核技巧映射到高维空间：

1. **核矩阵**: $K_{ij} = k(x_i, x_j)$
   - RBF核: $k(x,y) = \exp(-\gamma \|x-y\|^2)$
   - 多项式核: $k(x,y) = (\gamma x \cdot y + c)^d$

2. **中心化核矩阵**: $K_c = K - 1_n K - K 1_n + 1_n K 1_n$

3. **特征分解**: $K_c = \alpha \Lambda \alpha^T$

4. **投影**: 新样本投影到核空间

**选择主成分数量**:

1. **肘部法则**: 累积方差曲线的拐点
2. **方差阈值**: 保留95%方差所需的主成分数
3. **Kaiser准则**: 特征值 > 平均特征值

**复杂度**:
- 协方差+特征分解: O(p²n + p³)
- SVD方法: O(min(np², n²p))
- 投影: O(npk)

**PCA vs SVD关系**:
- PCA的主成分 = SVD的右奇异向量 V
- PCA的解释方差 = SVD的奇异值平方/(n-1)
- SVD是PCA的一种高效实现方式

**典型应用**:
- 数据可视化（降维到2D/3D）
- 特征提取和压缩
- 图像压缩（Eigenfaces）
- 噪声过滤
- 去除特征相关性

---

### EM算法 (Expectation-Maximization Algorithm)
**核心思想**: EM算法是一种迭代优化算法，用于含有隐变量的概率模型的参数估计。通过E步（计算隐变量后验）和M步（更新参数）交替进行，最大化观测数据的对数似然。

**算法框架**:

目标：最大化对数似然 $\log P(X|\theta) = \log \sum_Z P(X, Z|\theta)$

由于求和在对数内部难以优化，引入隐变量后验分布：

**E步（Expectation）**:
$$Q(\theta|\theta^{(t)}) = E_{Z|X,\theta^{(t)}}[\log P(X,Z|\theta)]$$
$$= \sum_Z P(Z|X,\theta^{(t)}) \log P(X,Z|\theta)$$

**M步（Maximization）**:
$$\theta^{(t+1)} = \arg\max_\theta Q(\theta|\theta^{(t)})$$

**收敛性保证**:
- 对数似然单调不减：$\log P(X|\theta^{(t+1)}) \geq \log P(X|\theta^{(t)})$
- 收敛到局部最优解

**四种EM算法变体**:

| 算法 | 特点 | 适用场景 |
|------|------|----------|
| 标准EM | M步求精确最大值 | 有闭式解的模型 |
| 广义EM (GEM) | M步只需Q函数增加 | 无闭式解但可优化 |
| GMM-EM | 高斯混合模型专用 | 连续数据聚类 |
| 变分EM (VBEM) | 贝叶斯推断框架 | 需要不确定性量化 |

**高斯混合模型 (GMM)**:

模型：$P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)$

E步（责任度）:
$$\gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_j \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)}$$

M步（参数更新）:
- 混合系数: $\pi_k = \frac{1}{N}\sum_n \gamma_{nk}$
- 均值: $\mu_k = \frac{\sum_n \gamma_{nk} x_n}{\sum_n \gamma_{nk}}$
- 协方差: $\Sigma_k = \frac{\sum_n \gamma_{nk}(x_n-\mu_k)(x_n-\mu_k)^T}{\sum_n \gamma_{nk}}$

**模型选择**:
- BIC = $-2\log L + k\log n$ (更保守)
- AIC = $-2\log L + 2k$ (较宽松)

**变分EM (VBEM)**:

将参数也视为随机变量，进行贝叶斯推断：

目标：最大化ELBO（Evidence Lower Bound）
$$\mathcal{L}(q) = E_q[\log P(X, Z, \theta)] - E_q[\log q(Z, \theta)]$$

平均场近似：$q(\theta, Z) = q(\theta)q(Z)$

优势：
- 自动正则化（先验防止过拟合）
- 不确定性量化（后验分布）
- 自动模型选择（ARD机制）

**复杂度**:
- E步: O(NKp²)（K个成分，p维特征）
- M步: O(NKp²)
- 每次迭代: O(NKp²)

**EM算法特点**:

优点:
- 理论保证单调收敛
- 适用于各种含隐变量的模型
- 实现相对简单
- 概率解释清晰

缺点:
- 只保证局部最优
- 对初始值敏感
- 可能收敛较慢
- 需要选择模型复杂度

**典型应用**:
- 聚类分析（GMM软聚类）
- 密度估计
- 异常检测
- 缺失数据填补
- HMM参数学习（Baum-Welch）
- 图像分割

---

### 14. 主成分分析 (PCA - Principal Component Analysis)

**核心思想**: PCA是一种经典的无监督学习算法，用于数据降维和特征提取。通过线性变换将原始数据投影到新的坐标系统中，使得数据在新坐标轴（主成分）上的方差最大化，从而保留数据的主要信息。

**数学定义**: 对于数据矩阵 $X_{n \times p}$（n个样本，p个特征）：

1. **数据中心化**: $X_c = X - \bar{X}$，其中 $\bar{X}$ 是样本均值

2. **协方差矩阵**: $C = \frac{1}{n-1} X_c^T X_c$

3. **特征分解**: $C = V \Lambda V^T$
   - $V$: 特征向量矩阵（主成分方向）
   - $\Lambda$: 对角矩阵，对角元素为特征值（方差大小）

4. **降维投影**: $X_{reduced} = X_c V_k$
   - $V_k$: 前k个最大特征值对应的特征向量

**关键性质**:

1. **方差最大化**: 主成分是使投影方差最大的方向
   $$\max_w \text{Var}(Xw) \quad \text{s.t.} \quad \|w\|=1$$

2. **正交性**: 主成分之间相互正交（不相关）
   $$v_i^T v_j = \delta_{ij}$$

3. **能量集中**: 前几个主成分通常包含数据的主要信息
   $$\text{累积方差比例} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{p} \lambda_i}$$

4. **最佳重建**: k个主成分给出最佳k维线性近似（最小重建误差）

**PCA实现方法**:

| 方法 | 原理 | 特点 | 适用场景 |
|------|------|------|----------|
| 特征分解 | 协方差矩阵的特征分解 | 直观理解 | 小数据集 |
| SVD | 直接对数据矩阵SVD | 数值更稳定（推荐）| 一般情况 |
| 增量PCA | 批量更新统计量 | 内存友好 | 大数据集 |
| 核PCA | 核技巧映射到高维 | 捕获非线性特征 | 非线性数据 |

**算法步骤**（SVD方法）:

```
1. 数据中心化: X_c = X - mean(X)
2. 奇异值分解: X_c = U Σ V^T
3. 主成分: V的列向量
4. 解释方差: σ²/(n-1)
5. 投影: X_reduced = X_c * V_k
6. 重建: X_reconstructed = X_reduced * V_k^T + mean(X)
```

**评估指标**:

1. **解释方差比例**:
   $$\text{Explained Variance Ratio}_i = \frac{\lambda_i}{\sum_j \lambda_j}$$

2. **累积解释方差**:
   $$\text{Cumulative Variance}_{1:k} = \sum_{i=1}^{k} \frac{\lambda_i}{\sum_j \lambda_j}$$

3. **重建误差**:
   $$\text{Reconstruction Error} = \|X - X_{reconstructed}\|_F$$

**选择主成分数量**:

1. **肘部法则**（Elbow Method）:
   - 绘制累积方差 vs 主成分数量曲线
   - 找到曲线的"肘部"（边际收益递减的拐点）

2. **方差阈值法**:
   - 保留累积方差达到指定阈值的主成分（如95%）
   - 公式：$\text{argmin}_k \left\{ \frac{\sum_{i=1}^{k} \lambda_i}{\sum_i \lambda_i} \geq \theta \right\}$

3. **Kaiser准则**:
   - 保留特征值大于平均特征值的主成分

**核PCA** (Kernel PCA):

对于非线性数据，使用核技巧将数据映射到高维空间：

1. **核矩阵**: $K_{ij} = k(x_i, x_j)$，常用核函数：
   - 线性核: $k(x,y) = x \cdot y$
   - RBF核: $k(x,y) = \exp(-\gamma \|x-y\|^2)$
   - 多项式核: $k(x,y) = (\gamma x \cdot y + c)^d$

2. **中心化核矩阵**:
   $$K_c = K - 1_n K - K 1_n + 1_n K 1_n$$

3. **特征分解**: $K_c = \alpha \Lambda \alpha^T$

4. **投影**: $\phi(x) \rightarrow K_{test} \alpha$

**运行示例**:
```bash
python pca/pca_algorithm.py
```

**输出内容**:

- **示例1**: 基础PCA降维演示
  * 2D数据降维到1D
  * 可视化主成分方向
  * 展示投影过程和重建误差
  * 保存为 `pca_basic_demo.png`

- **示例2**: 高维数据降维与可视化
  * 50维数据降维分析
  * 方差解释比例（各成分和累积）
  * 肘部法则选择最佳主成分数
  * 奇异值谱分析
  * 2D和3D投影可视化
  * 保存为 `pca_dimensionality_reduction.png`

- **示例3**: SVD vs 特征分解方法对比
  * 1000×100数据集性能测试
  * 计算时间比较
  * 结果一致性验证
  * 主成分方向相关性分析
  * 保存为 `pca_svd_vs_eigen.png`

- **示例4**: 核PCA处理非线性数据
  * 同心圆数据集（非线性可分）
  * 标准PCA vs 核PCA（RBF/多项式）
  * 展示核方法提取非线性特征的能力
  * 保存为 `pca_kernel_comparison.png`

- **示例5**: 图像压缩应用
  * 64×64合成图像
  * 不同压缩比（k=1,2,4,8,16,32）
  * 质量评估：MSE、PSNR、方差保留
  * 压缩性能分析曲线
  * 保存为 `pca_image_compression.png` 和 `pca_compression_analysis.png`

**算法特点**:

优点:
- 降低数据维度，减少计算复杂度
- 去除噪声和冗余特征
- 数据可视化（降维到2D/3D）
- 理论保证：最大方差保留
- 数值稳定性好（SVD方法）
- 可解释性强

缺点:
- 仅适用于线性关系（标准PCA）
- 假设主成分方向对应高方差
- 对异常值敏感
- 需要数据标准化/中心化
- 主成分可能难以解释
- 时间复杂度：O(min(np², n²p))

**复杂度分析**:

| 操作 | 时间复杂度 | 空间复杂度 |
|------|-----------|-----------|
| 协方差矩阵 | O(np²) | O(p²) |
| 特征分解 | O(p³) | O(p²) |
| SVD | O(min(np², n²p)) | O(np) |
| 投影（单样本）| O(pk) | O(k) |
| 重建 | O(npk) | O(np) |

**PCA vs SVD**:

| 特性 | PCA | SVD |
|------|-----|-----|
| 本质 | 协方差矩阵特征分解 | 数据矩阵奇异值分解 |
| 关系 | PCA主成分 = SVD右奇异向量 | SVD是PCA的一种实现 |
| 数值稳定性 | 一般 | 更好 |
| 计算复杂度 | O(p³) | O(min(np², n²p)) |
| 推荐 | 教学理解 | 实际应用 |

**典型应用**:

1. **数据降维与可视化**:
   - 高维数据投影到2D/3D
   - 探索性数据分析
   - 特征可视化

2. **特征工程**:
   - 特征提取和压缩
   - 去除相关特征
   - 生成正交特征

3. **图像处理**:
   - 图像压缩
   - 人脸识别（Eigenfaces）
   - 图像去噪

4. **信号处理**:
   - 信号去噪
   - 特征提取
   - 模式识别

5. **推荐系统**:
   - 协同过滤
   - 用户-物品矩阵降维
   - 潜在因子提取

6. **生物信息学**:
   - 基因表达数据分析
   - 细胞类型识别
   - 降维可视化

**实现亮点**:
- ✅ 标准PCA（SVD方法和特征分解方法）
- ✅ 增量PCA（适合大数据集）
- ✅ 核PCA（处理非线性数据）
- ✅ 白化处理（Whitening）
- ✅ 方差解释比例分析
- ✅ 肘部法则可视化
- ✅ 重建误差计算
- ✅ 图像压缩应用
- ✅ 性能对比分析
- ✅ 完整的2D/3D可视化

---

### 15. EM算法系列 (Expectation-Maximization Algorithm)

EM算法是一种迭代优化算法，用于含有隐变量的概率模型的参数估计。它在每次迭代中交替执行两个步骤：E步（期望步）计算隐变量的后验概率，M步（最大化步）更新模型参数以最大化期望对数似然。

#### 15.1 标准EM算法 (Standard EM Algorithm)

**核心思想**: 通过引入隐变量的后验分布，将难以直接优化的对数似然转化为易于优化的期望对数似然。

**数学框架**:

给定观测数据 $X = \{x_1, ..., x_n\}$ 和隐变量 $Z = \{z_1, ..., z_n\}$，目标是最大化对数似然：
$$\log P(X|\theta) = \log \sum_Z P(X, Z|\theta)$$

由于求和在对数内部，直接优化困难。EM算法通过以下迭代过程求解：

**E步（Expectation）**: 计算完全数据对数似然的期望
$$Q(\theta|\theta^{(t)}) = E_{Z|X,\theta^{(t)}}[\log P(X,Z|\theta)]$$
$$= \sum_Z P(Z|X,\theta^{(t)}) \log P(X,Z|\theta)$$

**M步（Maximization）**: 最大化Q函数
$$\theta^{(t+1)} = \arg\max_\theta Q(\theta|\theta^{(t)})$$

**收敛性保证**: 
- EM算法保证对数似然单调不减：$\log P(X|\theta^{(t+1)}) \geq \log P(X|\theta^{(t)})$
- 收敛到局部最优解（可能不是全局最优）

**经典应用示例**:

1. **硬币投掷问题**:
   - 观测：每次实验的正面次数
   - 隐变量：每次实验使用的硬币类型
   - 参数：每枚硬币的正面概率

2. **混合伯努利模型**:
   - 观测：二值数据点
   - 隐变量：每个数据点所属的成分
   - 参数：混合系数和各成分的伯努利参数

**运行示例**:
```bash
python em/em_algorithm.py
```

**输出内容**:

- **示例1**: 硬币投掷问题
  * 5次实验，每次10次投掷
  * 估计两枚硬币的正面概率
  * 展示E步（计算后验概率）和M步（更新参数）
  * 收敛曲线和参数估计误差
  * 保存为 `em_coin_flip.png`

- **示例2**: 混合伯努利模型
  * 200个样本，2个成分
  * 聚类结果可视化
  * 参数估计和聚类准确率
  * 保存为 `em_mixture_bernoulli.png`

**算法特点**:

优点:
- 简单易实现
- 理论保证单调收敛
- 适用于各种含隐变量的模型
- 每次迭代都有明确的概率解释

缺点:
- 只保证收敛到局部最优
- 对初始值敏感
- 收敛速度可能较慢
- 需要完整数据集（不支持在线学习）

---

#### 15.2 广义EM算法 (Generalized EM / GEM)

**核心思想**: 当M步无法求得闭式解时，只需保证Q函数增加即可，不必求精确最大值。

**算法定义**:

标准EM要求：$\theta^{(t+1)} = \arg\max_\theta Q(\theta|\theta^{(t)})$

广义EM只要求：$Q(\theta^{(t+1)}|\theta^{(t)}) \geq Q(\theta^{(t)}|\theta^{(t)})$

**常见GEM变体**:

1. **梯度上升GEM**:
   - M步：$\theta^{(t+1)} = \theta^{(t)} + \alpha \nabla_\theta Q(\theta|\theta^{(t)})|_{\theta=\theta^{(t)}}$
   - 适用：无闭式解但可求导的情况

2. **坐标上升GEM**:
   - M步：逐个更新参数坐标
   - $\theta_i^{(t+1)} = \arg\max_{\theta_i} Q(\theta_1^{(t+1)}, ..., \theta_{i-1}^{(t+1)}, \theta_i, \theta_{i+1}^{(t)}, ...)$
   - 适用：参数可分解更新的情况

3. **EM梯度算法**（ECM）:
   - 将M步分解为多个条件最大化步骤
   - 每步固定部分参数，优化其他参数

**收敛性**:
- GEM同样保证对数似然单调不减
- 收敛速度可能慢于标准EM
- 但适用范围更广

**运行示例**:
```bash
python em/generalized_em.py
```

**输出内容**:

- **示例1**: GEM vs 标准EM对比
  * 混合伯努利模型
  * 比较收敛速度和最终似然值
  * 展示梯度上升M步的效果
  * 保存为 `gem_vs_em.png`

- **示例2**: 坐标上升GEM算法
  * 一维高斯混合模型，400个样本
  * 逐个更新均值、方差、混合系数
  * 22次迭代收敛
  * 展示各参数的收敛轨迹
  * 保存为 `gem_coordinate_ascent.png`

**算法特点**:

优点:
- 适用性更广（无需闭式解）
- 灵活性高（可选择不同优化策略）
- 保留EM的收敛保证

缺点:
- 收敛速度通常慢于标准EM
- 需要选择步长或停止准则
- 实现相对复杂

---

#### 15.3 高斯混合模型EM算法 (GMM-EM)

**核心思想**: 假设数据由K个多元高斯分布混合生成，使用EM算法估计混合系数、均值和协方差矩阵。

**模型定义**:

$$P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)$$

其中：
- $\pi_k$: 第k个成分的混合系数，$\sum_k \pi_k = 1$
- $\mu_k$: 第k个成分的均值向量
- $\Sigma_k$: 第k个成分的协方差矩阵

**EM算法推导**:

**E步**: 计算后验概率（责任度）
$$\gamma_{nk} = P(z_n=k|x_n, \theta) = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)}$$

**M步**: 更新参数

1. **混合系数**:
   $$\pi_k = \frac{N_k}{N}, \quad N_k = \sum_{n=1}^{N} \gamma_{nk}$$

2. **均值**:
   $$\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} x_n$$

3. **协方差**:
   $$\Sigma_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} (x_n - \mu_k)(x_n - \mu_k)^T$$

**协方差矩阵类型**:

| 类型 | 约束 | 参数数量 | 特点 |
|------|------|----------|------|
| Full | 无约束 | O(Kp²) | 最灵活，可拟合任意形状 |
| Tied | 共享协方差 | O(p²) | 所有成分形状相同 |
| Diag | 对角矩阵 | O(Kp) | 特征独立，轴对齐椭圆 |
| Spherical | 球形 | O(K) | 各向同性，圆形/球形 |

**模型选择**:

1. **BIC准则**（贝叶斯信息准则）:
   $$\text{BIC} = -2\log L + k\log n$$
   - 更保守，倾向于选择较小的模型

2. **AIC准则**（赤池信息准则）:
   $$\text{AIC} = -2\log L + 2k$$
   - 较为宽松，可能选择较大的模型

**运行示例**:
```bash
python em/gmm_em.py
```

**输出内容**:

- **示例1**: 二维GMM拟合
  * 300个样本，3个成分
  * 可视化聚类结果和高斯分布等高线
  * 展示后验概率（软聚类）
  * BIC和AIC评分
  * 保存为 `gmm_2d.png`

- **示例2**: 模型选择（BIC/AIC）
  * 测试K=1到7的成分数
  * BIC曲线和AIC曲线
  * 自动识别最优成分数（K=3）
  * 保存为 `gmm_model_selection.png`

- **示例3**: 不同协方差类型对比
  * 比较Full、Tied、Diag、Spherical
  * 可视化不同协方差约束的效果
  * 展示BIC评分对比
  * 保存为 `gmm_covariance_types.png`

**算法特点**:

优点:
- 软聚类（提供概率分配）
- 可以建模任意形状的簇（Full协方差）
- 有完善的模型选择准则
- 可以处理聚类不均衡

缺点:
- 对初始化敏感
- 可能收敛到局部最优
- 对异常值敏感
- 高维数据协方差估计不稳定

**复杂度分析**:

| 操作 | 时间复杂度 | 空间复杂度 |
|------|-----------|-----------|
| E步 | O(NKp²) | O(NK) |
| M步 | O(NKp²) | O(Kp²) |
| 每次迭代 | O(NKp²) | O(NK + Kp²) |

---

#### 15.4 变分EM算法 (Variational EM / VBEM)

**核心思想**: 将EM算法扩展到贝叶斯框架，对参数和隐变量同时进行后验推断，而不仅仅是点估计。

**贝叶斯视角**:

标准EM：$\theta^* = \arg\max_\theta P(X|\theta)$（点估计）

变分EM：$q^*(\theta, Z) = \arg\min_q \text{KL}(q(\theta, Z) || P(\theta, Z|X))$（分布估计）

**变分下界（ELBO）**:

$$\log P(X) \geq \mathcal{L}(q) = E_q[\log P(X, Z, \theta)] - E_q[\log q(Z, \theta)]$$

这个下界被称为Evidence Lower Bound (ELBO)。

**变分EM迭代**:

假设后验近似为：$q(\theta, Z) = q(\theta)q(Z)$（平均场近似）

**VE步**: 固定 $q(\theta)$，优化 $q(Z)$
$$q(Z) \propto \exp(E_{q(\theta)}[\log P(X, Z, \theta)])$$

**VM步**: 固定 $q(Z)$，优化 $q(\theta)$
$$q(\theta) \propto \exp(E_{q(Z)}[\log P(X, Z, \theta)] + \log P(\theta))$$

**自动相关性确定（ARD）**:

变分贝叶斯GMM可以自动确定有效成分数：
- 为混合系数设置Dirichlet先验
- 训练后，无效成分的权重自动趋向于0
- 实现自动模型选择

**运行示例**:
```bash
python em/variational_em.py
```

**输出内容**:

- **示例1**: 变分EM vs 标准EM
  * 300个样本，2个成分
  * 比较对数似然 vs ELBO
  * 展示贝叶斯方法的正则化效果
  * 保存为 `vbem_vs_em.png`

- **示例2**: 自动模型选择（ARD机制）
  * 真实成分数：3
  * 过度拟合测试：K=6
  * 自动识别有效成分数为3
  * 展示权重的自动修剪
  * 保存为 `vbem_ard.png`

- **示例3**: 贝叶斯 vs 频率派
  * 小数据集（50个样本）对比
  * 展示贝叶斯方法在小数据上的优势
  * 不确定性可视化
  * 保存为 `bayesian_vs_frequentist.png`

**算法特点**:

优点:
- 自动正则化（先验防止过拟合）
- 不确定性量化（得到后验分布）
- 自动模型选择（ARD机制）
- 小数据集表现更好
- 避免奇异协方差矩阵

缺点:
- 计算开销更大
- 需要选择先验分布
- 平均场近似可能不准确
- 实现较为复杂

**贝叶斯 vs 频率派对比**:

| 特性 | 频率派EM | 贝叶斯变分EM |
|------|---------|------------|
| 参数估计 | 点估计 | 分布估计 |
| 目标函数 | 对数似然 | ELBO |
| 过拟合 | 容易 | 自动正则化 |
| 模型选择 | 需要BIC/AIC | ARD自动选择 |
| 不确定性 | 无 | 提供后验分布 |
| 小数据表现 | 较差 | 较好 |

**实现亮点**:
- ✅ 标准EM算法（硬币投掷、混合伯努利）
- ✅ 广义EM算法（梯度上升、坐标上升）
- ✅ 高斯混合模型（Full/Tied/Diag/Spherical协方差）
- ✅ BIC/AIC模型选择
- ✅ 变分EM算法（贝叶斯推断）
- ✅ 自动相关性确定（ARD）
- ✅ 完整的收敛分析和可视化

---

## 📈 学习计划与进度

### 监督学习算法

| 状态 | 算法名称 |
|:---:|---------|
| ✅ | **线性回归 - 梯度下降法** |
| ✅ | **感知机 - 原始形式** |
| ✅ | **感知机 - 对偶形式** |
| ✅ | **k近邻法 (k-NN) - k-d树** |
| ✅ | **朴素贝叶斯 - 极大似然估计** |
| ✅ | **朴素贝叶斯 - 贝叶斯估计** |
| ✅ | **决策树 - 分类树 (基尼指数)** |
| ✅ | **决策树 - 回归树 (MSE)** |
| ✅ | **逻辑斯谛回归 - 二项逻辑斯谛回归** |
| ✅ | **逻辑斯谛回归 - 多项逻辑斯谛回归** |
| ✅ | **最大熵模型 - 中文词性标注** |
| ✅ | **支持向量机 - 线性可分SVM (对偶形式)** |
| ✅ | **支持向量机 - 线性SVM (软间隔)** |
| ✅ | **支持向量机 - 线性SVM (随机梯度下降)** |
| ✅ | **支持向量机 - 非线性SVM (核函数方法)** |
| ✅ | **提升方法 (AdaBoost)** |
| ✅ | **EM算法 (Baum-Welch算法/HMM参数学习)** |
| ✅ | **隐马尔可夫模型 - 观测序列生成、前向、后向、Baum-Welch、Viterbi** |
| ✅ | **条件随机场 (CRF) - 前向-后向、Viterbi、BFGS训练** |

### 无监督学习算法

| 状态 | 算法名称 |
|:---:|---------|
| ✅ | **层次聚类 - 凝聚式 (Single/Complete/Average/Ward)** |
| ✅ | **K-Means聚类 - 标准算法/K-Means++初始化** |
| ✅ | **奇异值分解 (SVD) - 完整/截断SVD** |
| ✅ | **主成分分析 (PCA) - 标准/增量/核PCA** |
| ✅ | **EM算法系列 - 标准EM/广义EM/GMM-EM/变分EM** |

### 其他算法

| 状态 | 算法名称 |
|:---:|---------|
| ⬜ | 多项式回归 |
| ⬜ | 正则化 (Ridge/Lasso) |
| ⬜ | DBSCAN聚类 |
| ⬜ | 神经网络基础 |

**进度统计**: 已完成 25 / 25 个算法 (100.0%) 🎉

## 📖 参考资料

- 《机器学习方法（第2版）》- 李航
- 《机器学习》- 周志华
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [NumPy Documentation](https://numpy.org/doc/)

## 🤝 贡献

这是个人学习项目，欢迎提出建议和改进意见！

## 📧 联系方式

如有问题或建议，欢迎通过 GitHub Issues 联系我。

## 📄 许可证

MIT License

---

⭐ 如果这个项目对你有帮助，欢迎 Star！