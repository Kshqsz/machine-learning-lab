"""
éçº¿æ€§æ”¯æŒå‘é‡æœº - æ ¸å‡½æ•°æ–¹æ³•
Nonlinear Support Vector Machine - Kernel Method

ç®—æ³•åŸç†:
1. é€šè¿‡æ ¸å‡½æ•°å°†è¾“å…¥ç©ºé—´æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´
2. åœ¨é«˜ç»´ç©ºé—´ä¸­å¯»æ‰¾çº¿æ€§åˆ†ç¦»è¶…å¹³é¢
3. æ ¸æŠ€å·§: K(x_i, x_j) = Ï†(x_i)Â·Ï†(x_j)ï¼Œæ— éœ€æ˜¾å¼è®¡ç®—æ˜ å°„

æ ¸å‡½æ•°ç±»å‹:
1. çº¿æ€§æ ¸: K(x, z) = xÂ·z
2. å¤šé¡¹å¼æ ¸: K(x, z) = (Î³Â·xÂ·z + r)^d
3. é«˜æ–¯RBFæ ¸: K(x, z) = exp(-Î³||x-z||Â²)
4. Sigmoidæ ¸: K(x, z) = tanh(Î³Â·xÂ·z + r)

ä¼˜åŠ¿:
- å¯å¤„ç†éçº¿æ€§åˆ†ç±»é—®é¢˜
- æ ¸æŠ€å·§é¿å…æ˜¾å¼é«˜ç»´æ˜ å°„
- ç†è®ºåŸºç¡€å®Œå–„
- æ³›åŒ–èƒ½åŠ›å¼º

é€‚ç”¨åœºæ™¯:
- æ•°æ®éçº¿æ€§å¯åˆ†
- XORé—®é¢˜ã€åœ†å½¢åˆ†å¸ƒç­‰
- å›¾åƒè¯†åˆ«ã€æ–‡æœ¬åˆ†ç±»
- ç”Ÿç‰©ä¿¡æ¯å­¦
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOS
plt.rcParams['axes.unicode_minus'] = False


class KernelSVM:
    """æ ¸å‡½æ•°æ”¯æŒå‘é‡æœº"""
    
    def __init__(self, C=1.0, kernel='rbf', gamma='auto', degree=3, coef0=0.0):
        """
        å‚æ•°:
            C: æ­£åˆ™åŒ–å‚æ•°ï¼ˆè½¯é—´éš”æƒ©ç½šç³»æ•°ï¼‰
            kernel: æ ¸å‡½æ•°ç±»å‹ ('linear', 'poly', 'rbf', 'sigmoid')
            gamma: RBFã€å¤šé¡¹å¼ã€sigmoidæ ¸çš„ç³»æ•°ï¼Œ'auto'æ—¶ä¸º1/n_features
            degree: å¤šé¡¹å¼æ ¸çš„æ¬¡æ•°
            coef0: å¤šé¡¹å¼æ ¸å’Œsigmoidæ ¸çš„ç‹¬ç«‹é¡¹
        """
        self.C = C
        self.kernel_type = kernel
        self.gamma = gamma
        self.degree = degree
        self.coef0 = coef0
        
        # è®­ç»ƒåçš„å‚æ•°
        self.alpha = None
        self.support_vectors = None
        self.support_vector_labels = None
        self.support_vector_alpha = None
        self.b = None
        
    def _compute_kernel_matrix(self, X1, X2=None):
        """
        è®¡ç®—æ ¸çŸ©é˜µï¼ˆGramçŸ©é˜µï¼‰
        
        å‚æ•°:
            X1: ç¬¬ä¸€ç»„æ ·æœ¬ (n1, d)
            X2: ç¬¬äºŒç»„æ ·æœ¬ (n2, d)ï¼Œå¦‚æœä¸ºNoneåˆ™X2=X1
        
        è¿”å›:
            K: æ ¸çŸ©é˜µ (n1, n2)
        """
        if X2 is None:
            X2 = X1
        
        n1 = X1.shape[0]
        n2 = X2.shape[0]
        K = np.zeros((n1, n2))
        
        # è‡ªåŠ¨è®¾ç½®gamma
        if self.gamma == 'auto':
            gamma_value = 1.0 / X1.shape[1]
        else:
            gamma_value = self.gamma
        
        if self.kernel_type == 'linear':
            # çº¿æ€§æ ¸: K(x, z) = xÂ·z
            K = np.dot(X1, X2.T)
            
        elif self.kernel_type == 'poly':
            # å¤šé¡¹å¼æ ¸: K(x, z) = (Î³Â·xÂ·z + r)^d
            K = (gamma_value * np.dot(X1, X2.T) + self.coef0) ** self.degree
            
        elif self.kernel_type == 'rbf':
            # é«˜æ–¯RBFæ ¸: K(x, z) = exp(-Î³||x-z||Â²)
            for i in range(n1):
                for j in range(n2):
                    diff = X1[i] - X2[j]
                    K[i, j] = np.exp(-gamma_value * np.dot(diff, diff))
                    
        elif self.kernel_type == 'sigmoid':
            # Sigmoidæ ¸: K(x, z) = tanh(Î³Â·xÂ·z + r)
            K = np.tanh(gamma_value * np.dot(X1, X2.T) + self.coef0)
            
        else:
            raise ValueError(f"æœªçŸ¥çš„æ ¸å‡½æ•°ç±»å‹: {self.kernel_type}")
        
        return K
    
    def _objective(self, alpha, K, y):
        """
        å¯¹å¶é—®é¢˜çš„ç›®æ ‡å‡½æ•°ï¼ˆæœ€å°åŒ–å½¢å¼ï¼‰
        
        min: (1/2)Î±^T Q Î± - 1^T Î±
        å…¶ä¸­ Q_ij = y_i y_j K(x_i, x_j)
        """
        n = len(alpha)
        Q = np.outer(y, y) * K
        return 0.5 * np.dot(alpha, np.dot(Q, alpha)) - np.sum(alpha)
    
    def _objective_gradient(self, alpha, K, y):
        """ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦"""
        Q = np.outer(y, y) * K
        return np.dot(Q, alpha) - np.ones(len(alpha))
    
    def fit(self, X, y, verbose=True):
        """
        è®­ç»ƒæ ¸SVM
        
        å‚æ•°:
            X: è®­ç»ƒæ ·æœ¬ (n_samples, n_features)
            y: æ ‡ç­¾ (n_samples,) å–å€¼ä¸º+1æˆ–-1
        """
        n_samples, n_features = X.shape
        
        if verbose:
            print("="*70)
            print("éçº¿æ€§æ”¯æŒå‘é‡æœº - æ ¸å‡½æ•°æ–¹æ³•")
            print("="*70)
            print(f"è®­ç»ƒæ ·æœ¬æ•°: {n_samples}")
            print(f"ç‰¹å¾ç»´åº¦: {n_features}")
            print(f"æ ¸å‡½æ•°ç±»å‹: {self.kernel_type}")
            print(f"æƒ©ç½šå‚æ•° C: {self.C}")
            
            if self.kernel_type == 'rbf' or self.kernel_type == 'poly' or self.kernel_type == 'sigmoid':
                gamma_value = 1.0 / n_features if self.gamma == 'auto' else self.gamma
                print(f"Gamma: {gamma_value:.4f}")
            
            if self.kernel_type == 'poly':
                print(f"å¤šé¡¹å¼æ¬¡æ•°: {self.degree}")
                print(f"ç‹¬ç«‹é¡¹: {self.coef0}")
            
            print()
        
        # è®¡ç®—æ ¸çŸ©é˜µï¼ˆGramçŸ©é˜µï¼‰
        if verbose:
            print("è®¡ç®—æ ¸çŸ©é˜µ...")
        K = self._compute_kernel_matrix(X)
        
        # å¯¹å¶é—®é¢˜çš„çº¦æŸæ¡ä»¶
        # çº¦æŸ: Î£ Î±_i y_i = 0
        constraints = {'type': 'eq', 'fun': lambda alpha: np.dot(alpha, y)}
        
        # è¾¹ç•Œçº¦æŸ: 0 â‰¤ Î±_i â‰¤ C
        bounds = [(0, self.C) for _ in range(n_samples)]
        
        # åˆå§‹å€¼
        alpha0 = np.zeros(n_samples)
        
        # æ±‚è§£å¯¹å¶é—®é¢˜
        if verbose:
            print("æ±‚è§£äºŒæ¬¡è§„åˆ’é—®é¢˜...")
        
        result = minimize(
            fun=self._objective,
            x0=alpha0,
            args=(K, y),
            method='SLSQP',
            jac=self._objective_gradient,
            bounds=bounds,
            constraints=constraints,
            options={'maxiter': 1000, 'disp': False}
        )
        
        if not result.success:
            print(f"âš ï¸  ä¼˜åŒ–è­¦å‘Š: {result.message}")
        
        self.alpha = result.x
        
        # æå–æ”¯æŒå‘é‡ï¼ˆÎ± > é˜ˆå€¼ï¼‰
        sv_threshold = 1e-5
        sv_indices = self.alpha > sv_threshold
        
        self.support_vectors = X[sv_indices]
        self.support_vector_labels = y[sv_indices]
        self.support_vector_alpha = self.alpha[sv_indices]
        
        n_support_vectors = len(self.support_vector_alpha)
        
        # è®¡ç®—åç½® b
        # ä½¿ç”¨è¾¹ç•Œæ”¯æŒå‘é‡ (0 < Î± < C)
        margin_threshold = 1e-4
        margin_sv_mask = (self.support_vector_alpha > margin_threshold) & \
                        (self.support_vector_alpha < self.C - margin_threshold)
        
        if np.sum(margin_sv_mask) > 0:
            # ä½¿ç”¨è¾¹ç•Œæ”¯æŒå‘é‡è®¡ç®—b
            margin_sv = self.support_vectors[margin_sv_mask]
            margin_sv_labels = self.support_vector_labels[margin_sv_mask]
            
            # è®¡ç®—æ ¸çŸ©é˜µ
            K_margin = self._compute_kernel_matrix(margin_sv, self.support_vectors)
            
            # b = y_s - Î£ Î±_i y_i K(x_i, x_s)
            b_values = []
            for i in range(len(margin_sv)):
                b_val = margin_sv_labels[i] - np.sum(
                    self.support_vector_alpha * self.support_vector_labels * K_margin[i]
                )
                b_values.append(b_val)
            
            self.b = np.mean(b_values)
        else:
            # å¦‚æœæ²¡æœ‰è¾¹ç•Œæ”¯æŒå‘é‡ï¼Œä½¿ç”¨æ‰€æœ‰æ”¯æŒå‘é‡
            K_all = self._compute_kernel_matrix(self.support_vectors, self.support_vectors)
            b_values = []
            for i in range(n_support_vectors):
                b_val = self.support_vector_labels[i] - np.sum(
                    self.support_vector_alpha * self.support_vector_labels * K_all[i]
                )
                b_values.append(b_val)
            self.b = np.mean(b_values)
        
        if verbose:
            print("-"*70)
            print("è®­ç»ƒå®Œæˆï¼")
            print()
            
            print("æ”¯æŒå‘é‡ä¿¡æ¯:")
            print("-"*70)
            print(f"æ”¯æŒå‘é‡æ•°é‡: {n_support_vectors}")
            print(f"æ”¯æŒå‘é‡æ¯”ä¾‹: {n_support_vectors/n_samples*100:.2f}%")
            
            # åŒºåˆ†è¾¹ç•Œæ”¯æŒå‘é‡å’Œå†…éƒ¨æ”¯æŒå‘é‡
            n_margin_sv = np.sum(margin_sv_mask)
            n_inner_sv = n_support_vectors - n_margin_sv
            print(f"  - è¾¹ç•Œæ”¯æŒå‘é‡ (0 < Î± < C): {n_margin_sv}")
            print(f"  - å†…éƒ¨æ”¯æŒå‘é‡ (Î± = C): {n_inner_sv}")
            
            print()
            print(f"åç½® b: {self.b:.6f}")
            print("="*70)
    
    def decision_function(self, X):
        """
        è®¡ç®—å†³ç­–å‡½æ•°å€¼
        
        f(x) = Î£ Î±_i y_i K(x_i, x) + b
        """
        K = self._compute_kernel_matrix(X, self.support_vectors)
        return np.dot(K, self.support_vector_alpha * self.support_vector_labels) + self.b
    
    def predict(self, X):
        """é¢„æµ‹æ ·æœ¬ç±»åˆ«"""
        return np.sign(self.decision_function(X))
    
    def plot_decision_boundary(self, X, y, title="", resolution=200):
        """å¯è§†åŒ–å†³ç­–è¾¹ç•Œï¼ˆä»…æ”¯æŒ2Dæ•°æ®ï¼‰"""
        if X.shape[1] != 2:
            print("åªæ”¯æŒ2ç»´ç‰¹å¾çš„å¯è§†åŒ–")
            return
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        # åˆ›å»ºç½‘æ ¼
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(
            np.linspace(x_min, x_max, resolution),
            np.linspace(y_min, y_max, resolution)
        )
        
        # è®¡ç®—å†³ç­–å‡½æ•°å€¼
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = self.decision_function(grid_points)
        Z = Z.reshape(xx.shape)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œå’Œé—´éš”è¾¹ç•Œ
        ax.contour(xx, yy, Z, colors='black', levels=[0], linewidths=2.5, linestyles='-')
        ax.contour(xx, yy, Z, colors='gray', levels=[-1, 1], linewidths=2, linestyles='--')
        
        # ç»˜åˆ¶å†³ç­–åŒºåŸŸ
        ax.contourf(xx, yy, Z, levels=[-np.inf, 0, np.inf], 
                   colors=['lightblue', 'lightcoral'], alpha=0.3)
        
        # ç»˜åˆ¶è®­ç»ƒæ ·æœ¬
        for label, marker, color in [(1, 'o', 'red'), (-1, 's', 'blue')]:
            mask = y == label
            ax.scatter(X[mask, 0], X[mask, 1], c=color, marker=marker, s=120,
                      edgecolors='black', linewidths=1.5, label=f'ç±»åˆ« {label:+d}',
                      zorder=3)
        
        # æ ‡è®°æ”¯æŒå‘é‡
        ax.scatter(self.support_vectors[:, 0], self.support_vectors[:, 1],
                  s=300, facecolors='none', edgecolors='green', linewidths=3,
                  label='æ”¯æŒå‘é‡', zorder=4)
        
        ax.set_xlabel('xâ‚', fontsize=13)
        ax.set_ylabel('xâ‚‚', fontsize=13)
        ax.set_title(f'éçº¿æ€§SVM - {self.kernel_type.upper()}æ ¸{title}', 
                    fontsize=14, fontweight='bold')
        ax.legend(loc='best', fontsize=11)
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ–‡æœ¬è¯´æ˜
        gamma_value = 1.0 / X.shape[1] if self.gamma == 'auto' else self.gamma
        text = f'æ ¸å‡½æ•°: {self.kernel_type}\n'
        text += f'C = {self.C}\n'
        
        if self.kernel_type == 'rbf':
            text += f'Î³ = {gamma_value:.4f}\n'
        elif self.kernel_type == 'poly':
            text += f'æ¬¡æ•° d = {self.degree}\n'
            text += f'Î³ = {gamma_value:.4f}\n'
        
        text += f'æ”¯æŒå‘é‡: {len(self.support_vectors)}\n'
        text += f'b = {self.b:.4f}'
        
        ax.text(0.02, 0.98, text, transform=ax.transAxes,
               fontsize=10, verticalalignment='top',
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        kernel_name = self.kernel_type
        if self.kernel_type == 'poly':
            kernel_name = f'poly_d{self.degree}'
        elif self.kernel_type == 'rbf':
            kernel_name = f'rbf_g{gamma_value:.3f}'
        
        filename = f'svm_kernel_{kernel_name}_C{self.C}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"\nå›¾åƒå·²ä¿å­˜è‡³: {filename}")
        plt.show()


def demo_xor_problem():
    """æ¼”ç¤º1: XORé—®é¢˜ï¼ˆç»å…¸éçº¿æ€§é—®é¢˜ï¼‰"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹ 1: XORé—®é¢˜")
    print("="*70)
    print("XORé—®é¢˜æ˜¯ç»å…¸çš„éçº¿æ€§åˆ†ç±»é—®é¢˜ï¼Œçº¿æ€§SVMæ— æ³•è§£å†³")
    print()
    
    # XORæ•°æ®
    X_xor = np.array([
        [1, 1],
        [1, -1],
        [-1, 1],
        [-1, -1],
        [1.2, 1.2],
        [1.1, -1.1],
        [-1.1, 1.1],
        [-1.2, -1.2]
    ])
    
    y_xor = np.array([1, -1, -1, 1, 1, -1, -1, 1])
    
    print("è®­ç»ƒæ•°æ® (XOR):")
    print("-"*70)
    for i, (x, label) in enumerate(zip(X_xor, y_xor)):
        category = "æ­£ä¾‹" if label == 1 else "è´Ÿä¾‹"
        print(f"  æ ·æœ¬ {i+1}: x = ({x[0]:6.1f}, {x[1]:6.1f})  â†’  y = {label:+d}  ({category})")
    print()
    
    # ä½¿ç”¨RBFæ ¸
    svm = KernelSVM(C=1.0, kernel='rbf', gamma=1.0)
    svm.fit(X_xor, y_xor)
    
    # è¯„ä¼°
    y_pred = svm.predict(X_xor)
    accuracy = np.mean(y_pred == y_xor) * 100
    
    print("\nè®­ç»ƒé›†é¢„æµ‹ç»“æœ:")
    print("-"*70)
    for i, (x, y_true, y_p) in enumerate(zip(X_xor, y_xor, y_pred)):
        score = svm.decision_function(x.reshape(1, -1))[0]
        result = "âœ“" if y_p == y_true else "âœ—"
        category = "æ­£ä¾‹" if y_true == 1 else "è´Ÿä¾‹"
        print(f"  æ ·æœ¬ {i+1} ({category}): f(x) = {score:+8.4f}  â†’  é¢„æµ‹: {int(y_p):+d}  çœŸå®: {y_true:+d}  {result}")
    
    print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {accuracy:.2f}%")
    print()
    
    # å¯è§†åŒ–
    svm.plot_decision_boundary(X_xor, y_xor)


def demo_circles_problem():
    """æ¼”ç¤º2: åŒå¿ƒåœ†é—®é¢˜"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹ 2: åŒå¿ƒåœ†åˆ†ç±»é—®é¢˜")
    print("="*70)
    print("å†…åœ†ä¸ºä¸€ç±»ï¼Œå¤–ç¯ä¸ºå¦ä¸€ç±»ï¼Œéœ€è¦éçº¿æ€§å†³ç­–è¾¹ç•Œ")
    print()
    
    # ç”ŸæˆåŒå¿ƒåœ†æ•°æ®
    np.random.seed(42)
    n_samples_per_class = 20
    
    # å†…åœ†ï¼ˆæ­£ç±»ï¼‰
    r_inner = np.random.uniform(0, 1.5, n_samples_per_class)
    theta_inner = np.random.uniform(0, 2*np.pi, n_samples_per_class)
    X_inner = np.column_stack([
        r_inner * np.cos(theta_inner),
        r_inner * np.sin(theta_inner)
    ])
    y_inner = np.ones(n_samples_per_class)
    
    # å¤–ç¯ï¼ˆè´Ÿç±»ï¼‰
    r_outer = np.random.uniform(3, 4.5, n_samples_per_class)
    theta_outer = np.random.uniform(0, 2*np.pi, n_samples_per_class)
    X_outer = np.column_stack([
        r_outer * np.cos(theta_outer),
        r_outer * np.sin(theta_outer)
    ])
    y_outer = -np.ones(n_samples_per_class)
    
    # åˆå¹¶æ•°æ®
    X_circles = np.vstack([X_inner, X_outer])
    y_circles = np.concatenate([y_inner, y_outer])
    
    print(f"è®­ç»ƒæ•°æ®: {len(X_circles)}ä¸ªæ ·æœ¬")
    print(f"  å†…åœ†ï¼ˆæ­£ç±»ï¼‰: {n_samples_per_class}ä¸ª")
    print(f"  å¤–ç¯ï¼ˆè´Ÿç±»ï¼‰: {n_samples_per_class}ä¸ª")
    print()
    
    # ä½¿ç”¨RBFæ ¸
    svm = KernelSVM(C=10.0, kernel='rbf', gamma=0.5)
    svm.fit(X_circles, y_circles)
    
    # è¯„ä¼°
    y_pred = svm.predict(X_circles)
    accuracy = np.mean(y_pred == y_circles) * 100
    print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    # å¯è§†åŒ–
    svm.plot_decision_boundary(X_circles, y_circles)


def demo_polynomial_boundary():
    """æ¼”ç¤º3: å¤šé¡¹å¼å†³ç­–è¾¹ç•Œ"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹ 3: å¤šé¡¹å¼æ ¸ - æŠ›ç‰©çº¿åˆ†ç±»é—®é¢˜")
    print("="*70)
    print("æ•°æ®åˆ†å¸ƒå‘ˆæŠ›ç‰©çº¿å½¢çŠ¶ï¼Œé€‚åˆä½¿ç”¨å¤šé¡¹å¼æ ¸")
    print()
    
    # ç”ŸæˆæŠ›ç‰©çº¿æ•°æ®
    np.random.seed(42)
    n_samples = 30
    
    # æ­£ç±»ï¼šæŠ›ç‰©çº¿ä¸Šæ–¹
    X_pos = np.random.uniform(-3, 3, (n_samples, 2))
    X_pos[:, 1] = X_pos[:, 1] + 2  # å‘ä¸Šç§»åŠ¨
    X_pos = X_pos[X_pos[:, 1] > X_pos[:, 0]**2 - 2][:n_samples//2]
    y_pos = np.ones(len(X_pos))
    
    # è´Ÿç±»ï¼šæŠ›ç‰©çº¿ä¸‹æ–¹
    X_neg = np.random.uniform(-3, 3, (n_samples, 2))
    X_neg[:, 1] = X_neg[:, 1] - 2  # å‘ä¸‹ç§»åŠ¨
    X_neg = X_neg[X_neg[:, 1] < X_neg[:, 0]**2 - 4][:n_samples//2]
    y_neg = -np.ones(len(X_neg))
    
    # åˆå¹¶æ•°æ®
    X_poly = np.vstack([X_pos, X_neg])
    y_poly = np.concatenate([y_pos, y_neg])
    
    print(f"è®­ç»ƒæ•°æ®: {len(X_poly)}ä¸ªæ ·æœ¬")
    print(f"  æ­£ç±»: {len(X_pos)}ä¸ª")
    print(f"  è´Ÿç±»: {len(X_neg)}ä¸ª")
    print()
    
    # ä½¿ç”¨å¤šé¡¹å¼æ ¸
    svm = KernelSVM(C=1.0, kernel='poly', degree=2, gamma=1.0, coef0=1.0)
    svm.fit(X_poly, y_poly)
    
    # è¯„ä¼°
    y_pred = svm.predict(X_poly)
    accuracy = np.mean(y_pred == y_poly) * 100
    print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    # å¯è§†åŒ–
    svm.plot_decision_boundary(X_poly, y_poly)


def demo_kernel_comparison():
    """æ¼”ç¤º4: ä¸åŒæ ¸å‡½æ•°çš„å¯¹æ¯”"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹ 4: æ ¸å‡½æ•°å¯¹æ¯”")
    print("="*70)
    print("åœ¨åŒä¸€æ•°æ®é›†ä¸Šæ¯”è¾ƒä¸åŒæ ¸å‡½æ•°çš„æ•ˆæœ")
    print()
    
    # ç”Ÿæˆæ··åˆæ•°æ®
    np.random.seed(42)
    
    # ç±»1: ä¸¤ä¸ªç°‡
    X1_cluster1 = np.random.randn(15, 2) * 0.5 + np.array([2, 2])
    X1_cluster2 = np.random.randn(15, 2) * 0.5 + np.array([-2, -2])
    X_class1 = np.vstack([X1_cluster1, X1_cluster2])
    y_class1 = np.ones(30)
    
    # ç±»2: ä¸­é—´åŒºåŸŸ
    X_class2 = np.random.randn(30, 2) * 0.8
    y_class2 = -np.ones(30)
    
    X_mix = np.vstack([X_class1, X_class2])
    y_mix = np.concatenate([y_class1, y_class2])
    
    print(f"è®­ç»ƒæ•°æ®: {len(X_mix)}ä¸ªæ ·æœ¬ï¼ˆæ­£ç±»30ï¼Œè´Ÿç±»30ï¼‰")
    print()
    
    kernels = [
        ('linear', {}, 'çº¿æ€§æ ¸'),
        ('poly', {'degree': 2}, 'å¤šé¡¹å¼æ ¸(d=2)'),
        ('rbf', {'gamma': 0.5}, 'RBFæ ¸(Î³=0.5)'),
        ('rbf', {'gamma': 2.0}, 'RBFæ ¸(Î³=2.0)')
    ]
    
    for kernel_type, params, name in kernels:
        print(f"\n{'='*70}")
        print(f"æµ‹è¯• {name}")
        print('='*70)
        
        svm = KernelSVM(C=1.0, kernel=kernel_type, **params)
        svm.fit(X_mix, y_mix, verbose=False)
        
        y_pred = svm.predict(X_mix)
        accuracy = np.mean(y_pred == y_mix) * 100
        
        print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"æ”¯æŒå‘é‡æ•°: {len(svm.support_vectors)}")
        
        svm.plot_decision_boundary(X_mix, y_mix, title=f" - {name}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "ğŸ¯ "*20)
    print("éçº¿æ€§æ”¯æŒå‘é‡æœº - æ ¸å‡½æ•°æ–¹æ³•æ¼”ç¤º")
    print("ğŸ¯ "*20 + "\n")
    
    # æ¼”ç¤º1: XORé—®é¢˜
    demo_xor_problem()
    
    # æ¼”ç¤º2: åŒå¿ƒåœ†é—®é¢˜
    demo_circles_problem()
    
    # æ¼”ç¤º3: å¤šé¡¹å¼æ ¸
    demo_polynomial_boundary()
    
    # æ¼”ç¤º4: æ ¸å‡½æ•°å¯¹æ¯”
    demo_kernel_comparison()
    
    print("\n" + "="*70)
    print("æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
